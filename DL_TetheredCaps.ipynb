{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe32c4a-0950-4436-8556-1fe4c8fade8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Semesterabschlie√üende schriftliche Ausarbeitung**  \n",
    "**Fachhochschule S√ºdwestfalen**\n",
    "\n",
    "---\n",
    "\n",
    "**Modul:** Deep Learning  \n",
    "**Semester:** Wintersemester 2025/2026  \n",
    "**Thema:** *Architektur- und Trainingsstrategien f√ºr √ºberwachte und un√ºberwachte Deep-Learning-Modelle zur Tethered-Cap-Inspektion*\n",
    "\n",
    "---\n",
    "\n",
    "### **Autoren**\n",
    "**Sarah Gem√ºnden** ‚Äî Matrikelnummer: 30482243  \n",
    "**Chanyut Bonkhamsaen** ‚Äî Matrikelnummer: 30401284\n",
    "\n",
    "**Diese Arbeit wurde von beiden Autoren zu gleichen Teilen (50 % / 50 %) erarbeitet.**\n",
    "\n",
    "---\n",
    "\n",
    "**Abgabe am:** XX. Februar 2026  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00ff68-6304-4609-95ac-6004a4934b15",
   "metadata": {},
   "source": [
    "# Gliederung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42f33b-6e3f-4fb2-8b6c-d953657fa8fe",
   "metadata": {},
   "source": [
    "**1. Einleitung**  \n",
    "**2. Theoretische Grundlagen**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.1 Sicherungsringinspektion in Getr√§nkeabf√ºllanlagen  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.2 Anforderungen und Herausforderungen der Tetherd-Caps-Inspektion    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.3 Bewertungsmetriken im Kontext der Tethered-Caps-Inspektion  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.4 Grundlagen neuronaler Netze f√ºr die Bildklassifikation  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.4.1 Motivation: Warum CNNs statt Fully Connected Networks     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.4.2 Convolutional Layer als zentrales Bauteil       \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.4.3 Architekturprinzip von CNNs       \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.4.4 Regularisierung von CNNs       \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.4.5 Trainingsablauf     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.5 Transfer Learning mit ResNet  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.5.1 Grundidee des Transfer Learning  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.5.2 Vorgehensweisen  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.5.3 ResNet: Motivation und Kernidee  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.5.4 Aufbau einer ResNet-Architektur  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.6 Grundlagen AutoEncoder  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.6.1 Grundidee  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.6.2 Vorgehensweisen  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.6.3 Autoencoder - Motivation und Kernidee  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.6.4 Aufbau einer Autoencoder-Architektur   \n",
    "**3. Datenimport und Datenvorbereitung**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.1 technisches Setup und Paketimporte  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.2 Datenimport und erste explorative Datenanalyse  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.3 Datensplits   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.1 Standard Split (70/15/15)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.2 Variant-Based Split (Leave-One-Variant-Out) - Variante: cc_red  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.3 Variant-Based Split (Leave-One-Variant-Out) - Variante: voesl_zitrone   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.4 Bildvorverarbeitung   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.5 Datasets und DataLoader   \n",
    "**4. √úberwachte Bildklassifikation zur Inspektion von Tethered Caps**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.1 Modellarchitekturen  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.1 CNN von Grund auf (parametrisierbare Architektur)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.2 ResNet als vortrainierte Architektur (Transfer Learning)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.2 Hilfsfunktionen f√ºr Training, Evaluatoin und Fehleranalyse    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.3 Trainings- und Evaluationspipeline  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.4 Experimente mit dem CNN von Grund auf  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4.1 Baseline-Modell   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4.2 Tiefes, schmales Modell vs. Baseline  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3.3 Einfluss der Kernelgr√∂√üe  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3.4 Einfluss des Klassifikationskopfs  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3.5 Zusammenfassung Experimente mit dem CNN von Grund auf   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.5 Experimente mit ResNet    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.6 Modellinterpretation mit TorchCam  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4.7 Inverenzzeit und Praxistauglichkeit  \n",
    "**5. Un√ºberwachter Ansatz mittels Encoder-Netzwerken**  \n",
    "**6. Ergebnisse und Bewertung**  \n",
    "**7. Zusammenfassung und Ausblick**  \n",
    "**8. Quellenverzeichnis**  \n",
    "**9. Eigenst√§ndigkeitserkl√§rung**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57585f1-8428-4253-a6c9-df8eeb7c6b76",
   "metadata": {},
   "source": [
    "# 1. Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa5b3c-10c5-4b21-8b15-6b6119155091",
   "metadata": {},
   "source": [
    "* Hintergrund: Abf√ºllanlagen in der Getr√§nkeindustrie\n",
    "* div. Inspektionstechnik in einer Abf√ºllanlage\n",
    "* Qualit√§tskontrolle im Verschlie√üprozess\n",
    "* Sicherungsringinspektion\n",
    "* Herausforderung durch Tethtered Caps\n",
    "* Motivation f√ºr den Einsatz von Deep Learning\n",
    "* Zielsetzung der Arbeit\n",
    "* Vorgehensweise Beschreiben\n",
    "* Haupterkenntnisse\n",
    "\n",
    "Ziel:\n",
    "‚Ä¢\tAccuracy: 97,0‚ÄØ%\n",
    "‚Ä¢\tFehlerkennungsrate: <‚ÄØ0,1‚ÄØ% (False-Positive-Rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7ed1e-9036-4154-8058-a1144090cdab",
   "metadata": {},
   "source": [
    "# 2. Theoretische Grundlagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa096d-0b89-4d20-a91a-bbeabdaea9fe",
   "metadata": {},
   "source": [
    "Dieses Kapitel vermittelt die theoretischen Grundlagen, die f√ºr das Verst√§ndnis der im weiteren Verlauf vorgestellten Bildklassifikations- und Deep-Learning-Methoden erforderlich sind. Zun√§chst wird der industrielle Anwendungskontext der Sicherungsringinspektion in Getr√§nkeabf√ºllanlagen erl√§utert sowie die damit verbundenen Anforderungen und Herausforderungen beschrieben. Darauf aufbauend werden zentrale Bewertungsmetriken definiert, bevor die grundlegenden Konzepte neuronaler Netze und Convolutional Neural Networks (CNNs) vorgestellt werden. Abschlie√üend werden Transfer-Learning-Ans√§tze, insbesondere ResNet-Architekturen, sowie Autoencoder als erg√§nzende Methode zur Merkmalsextraktion behandelt. Das Kapitel schafft damit die theoretische Basis f√ºr die nachfolgenden Implementierungs- und Experimentalkapitel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed255b48-3996-44df-944d-7cd92b1474f4",
   "metadata": {},
   "source": [
    "## 2.1 Sicherungsringinspektion in Getr√§nkeabf√ºllanlagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70a982-b4cf-44e7-b710-8c711d6c72e5",
   "metadata": {},
   "source": [
    "In Getr√§nkeabf√ºllanlagen ist eine sichere und dichte Verschlie√üung der Beh√§lter entscheidend f√ºr Produktqualit√§t, Haltbarkeit und Geschmack. Ein besch√§digter oder unvollst√§ndiger Sicherungsring kann zu Undichtigkeiten f√ºhren und damit die Produktsicherheit beeintr√§chtigen. Die folgende schematische Darstellung zeigt den Bereich des Sicherungsrings in Rot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d1ca1-8905-420a-a19f-0fd7f6e117fd",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Sicherungsring.png\" width=\"150\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2efeee-450e-42fc-9dba-378a7b4ca7ef",
   "metadata": {},
   "source": [
    "Zur automatisierten √úberpr√ºfung des Sicherungsrings wird bei der KHS GmbH das System Innocheck TSI (Tamper-evident Seal Inspection) eingesetzt. Das System inspiziert den Sicherungsring eines Verschlusses auf Unversehrtheit, indem es den gesamten Ring aus unterschiedlichen Blickwinkeln erfasst und auswertet. Die folgende Abbildung verdeutlicht das Prinzip der optischen Erfassung und der 360-Grad-Rundumsicht:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51daa477-d725-4d81-831b-e823d892e5b3",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/TSI.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22095477-4fb0-46a7-a641-a7b32e20e17a",
   "metadata": {},
   "source": [
    "Zwei Kameras erzeugen mithilfe einer Spiegeloptik eine 360¬∞-Rundumsicht, w√§hrend LED-Beleuchtung f√ºr gleichm√§√üige und reproduzierbare Lichtverh√§ltnisse sorgt. Auf Basis moderner Bildverarbeitungsalgorithmen wertet die Software die Farbpixel und Strukturen des Sicherungsrings aus und erkennt dabei auch kleinste Besch√§digungen. Da die Deckelgeometrie herk√∂mmlicher Deckel aus allen Richtungen gleich aussieht, ist eine zuverl√§ssige Bewertung allein anhand der Farbpixel m√∂glich.\n",
    "\n",
    "Durch dieses Verfahren erm√∂glicht das System:\n",
    "- eine zuverl√§ssige Kontrolle der Ringintegrit√§t\n",
    "- die Erkennung kleinster Defekte\n",
    "- eine vollst√§ndige Rundumsicht des Verschlusses. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639617d-2971-45ca-b85d-2da860cb69b0",
   "metadata": {},
   "source": [
    "Seit Juli 2024 gilt in der EU die Vorgabe, dass Einweg-Getr√§nkeverpackungen bis zu drei Litern mit sogenannten Tethered Caps ausgestattet sein m√ºssen. Dabei handelt es sich um Verschlusskappen, die auch nach dem √ñffnen dauerhaft mit der Flasche verbunden bleiben, um zu verhindern, dass lose Deckel in die Umwelt gelangen. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76403e-49d5-4674-9fd4-e1257af1a022",
   "metadata": {},
   "source": [
    "Durch die Einf√ºhrung der Tethered Caps erh√∂hte sich die Vielfalt an Verschlussvarianten deutlich. Die neue, bewegliche Geometrie der Verschl√ºsse f√ºhrt dazu, dass diese nicht mehr aus allen Richtungen gleich aussehen. Daher ist eine reine Inspektion anhand der Farbpixel, wie sie bei klassischen symmetrischen Schraubverschl√ºssen eingesetzt wurde, f√ºr diesen Anwendungsfall nicht mehr ausreichend.\n",
    "\n",
    "Um weiterhin eine zuverl√§ssige Qualit√§tskontrolle sicherzustellen, hat die KHS GmbH die bestehende Verschlusskontrolle Innocheck TSI um KI-basierte Verfahren erweitert. Kameras erfassen die Verschl√ºsse in hoher Aufl√∂sung, w√§hrend die Modelle die aufgenommenen Bilder in Echtzeit auswerten. Durch den Einsatz von Deep Learning k√∂nnen sich die Systeme an neue Fehlerbilder anpassen und auch komplexe oder seltene Defekte zuverl√§ssig erkennen. [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa807392-7041-45c6-89f9-0b9d34b2e4f5",
   "metadata": {},
   "source": [
    "## 2.2 Anforderungen und Herausforderungen der Tetherd-Caps-Inspektion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469e14a-9647-4a9e-9c29-77671cdaca01",
   "metadata": {},
   "source": [
    "Die Inspektion von Tethered Caps stellt hohe Anforderungen an die eingesetzten Bildverarbeitungs- und Klassifikationssysteme. Ziel ist die zuverl√§ssige Beurteilung des Sicherungsrings eines Flaschenverschlusses im laufenden Produktionsprozess. Dabei handelt es sich um eine bin√§re Klassifikationsaufgabe, bei der Flaschen in die Klassen *gut* und *schlecht* eingeteilt werden. Als *schlecht* erkannte Flaschen werden unmittelbar nach der Erkennung aus dem Produktionsprozess ausgeschleust. Eine zentrale Anforderung besteht darin, dass eindeutig erkennbar ist, dass die Flasche noch nicht ge√∂ffnet wurde, was einen vollst√§ndig intakten Sicherungsring voraussetzt. Die Auswertung der Bilddaten muss in Echtzeit erfolgen, da als *schlecht* klassifizierte Flaschen direkt im Anschluss an die Inspektion aus dem Produktionsprozess ausgeschleust werden. Das Innocheck-TSI-System kann aktuell eine Leistung von bis zu 72 000 Flaschen pro Stunde verarbeiten, was 20 Flaschen pro Sekunde entspricht. Da pro Flasche vier Bildaufnahmen erstellt werden, m√ºssen innerhalb einer Sekunde insgesamt 80 Bilder analysiert und klassifiziert werden. Dies entspricht einer erforderlichen Leistung von 80 Inferenzen pro Sekunde. Diese Echtzeitanforderung stellt eine wesentliche Randbedingung f√ºr den Entwurf und die Auswahl geeigneter Deep-Learning-Modelle dar. Neben der zeitlichen Anforderung gelten hohe Qualit√§tsanforderungen an die Klassifikation. F√ºr den industriellen Einsatz werden unter anderem eine Accuracy von mindestens 97 % sowie eine Fehlerkennungsrate unter 0,1 % gefordert. Letztere bezieht sich auf f√§lschlich als schlecht klassifizierte gut-Flaschen (False-Positive-Rate), da diese unmittelbar wirtschaftliche Verluste verursachen. Die zugrunde liegenden Metriken sowie die Konfusionsmatrix werden in Kapitel 2.3 detailliert erl√§utert. In der Praxis ist die Inspektion von Tethered Caps mit einer Reihe von Herausforderungen verbunden. Flaschen k√∂nnen auf dem Transportband leicht wackeln. Zudem d√ºrfen Tropfen oder Kondensat auf der Oberfl√§che die Klassifikationsentscheidung nicht beeinflussen. Weitere erschwerende Faktoren sind variable Beleuchtungsverh√§ltnisse, eine hohe Vielfalt an Deckelvarianten und Farben sowie die komplexe Geometrie der Tethered Caps. Letztere f√ºhrt dazu, dass rein pixelbasierte Bildverarbeitungsverfahren, wie sie bei klassischen symmetrischen Schraubverschl√ºssen eingesetzt wurden, f√ºr diesen Anwendungsfall nur eingeschr√§nkt geeignet sind.\n",
    "\n",
    "Im Rahmen der Sicherungsringinspektion gibt es folgende typische Fehlerbilder:\n",
    "- Risse  \n",
    "- untergeschobene Sicherungsringe (auch als ‚ÄûSmile‚Äú bezeichnet)  \n",
    "- eine zu gro√üe L√ºcke zwischen Sicherungsring und Deckel  \n",
    "- fehlende oder gebrochene Stege  \n",
    "- Deformationen  \n",
    "\n",
    "Die folgende Abbildung zeigt exemplarisch Beispiele dieser typischen Fehlerbilder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9bd915-048c-4453-bbc1-905181507339",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Fehlerbilder.jpg\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df6a24-3f71-4ad5-91f4-4c8f1bfe05a7",
   "metadata": {},
   "source": [
    "Die Ursachen f√ºr diese Defekte k√∂nnen vielf√§ltig sein und liegen unter anderem darin, dass der Verschluss zu fest oder nicht weit genug aufgedreht wurde oder beim Aufdrehen h√§ngen geblieben ist. Dadurch entsteht eine gro√üe Bandbreite visuell unterschiedlicher Fehlerbilder, was die zuverl√§ssige automatisierte Erkennung zus√§tzlich erschwert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa8356-a90a-4b5c-b753-df9fd365f31d",
   "metadata": {},
   "source": [
    "## 2.3 Bewertungsmetriken im Kontext der Tethered-Cap-Inspektion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ede4df-3651-40fe-85a5-e676db5bfe7e",
   "metadata": {},
   "source": [
    "Zur Bewertung von Klassifikationsmodellen wird bei bin√§ren Klassifikationsaufgaben, wie der vorliegenden Sicherungsringinspektion, in der Regel die Konfusionsmatrix verwendet. Sie stellt die vorhergesagten Klassen den tats√§chlichen Klassen gegen√ºber und erm√∂glicht eine differenzierte Analyse der Klassifikationsergebnisse anhand der vier Grundf√§lle True Positive, False Positive, True Negative und False Negative.\n",
    "\n",
    "Die folgende Abbildung zeigt schematisch den Aufbau einer Konfusionsmatrix sowie die daraus ableitbaren Bewertungsmetriken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa3e78-7b64-4316-8e04-0ef88df34d5d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Konfusionsmatrix.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf115d85-2ae7-41db-85d7-7642680544fe",
   "metadata": {},
   "source": [
    "Im vorliegenden Anwendungsfall beziehen sich die Bezeichnungen *positiv* und *negativ* auf den Zustand des Flaschenverschlusses. Flaschen mit einem fehlerhaften oder besch√§digten Verschluss werden dabei der *positiven* Klasse zugeordnet, w√§hrend Flaschen mit einem intakten Verschluss der *negativen* Klasse entsprechen. Diese Zuordnung ist im industriellen Inspektionskontext √ºblich, da der Fokus der Klassifikation auf der Erkennung von Defekten liegt.\n",
    "\n",
    "Auf Basis der Konfusionsmatrix lassen sich verschiedene Metriken berechnen. F√ºr den vorliegenden industriellen Anwendungsfall sind insbesondere die folgenden beiden Kennzahlen von Bedeutung:\n",
    "\n",
    "**Accuracy** beschreibt den Anteil aller korrekt klassifizierten Flaschen an der Gesamtzahl der gepr√ºften Flaschen. Sie liefert einen allgemeinen Eindruck √ºber die Leistungsf√§higkeit des Modells, ber√ºcksichtigt jedoch nicht, welche Art von Fehlklassifikation vorliegt.  \n",
    "Die **False-Positive-Rate** beschreibt den Anteil der gut-Flaschen, die f√§lschlich als schlecht klassifiziert und somit aus dem Produktionsprozess ausgeschleust werden. Diese Kenngr√∂√üe ist im Inspektionskontext von besonderer Relevanz, da jede f√§lschlich ausgeschleuste einwandfreie Flasche unmittelbar wirtschaftliche Verluste verursacht.\n",
    "\n",
    "Im Gegensatz zu Metriken wie Accuracy oder Recall kann die False-Positive-Rate nicht direkt aus der Konfusionsmatrix abgelesen werden. Sie ergibt sich vielmehr aus dem Verh√§ltnis der False Positives zur Gesamtzahl der tats√§chlich negativen Beispiele und kann als Gegenma√ü zur Specificity berechnet werden. Die folgende Formel zeigt die Herleitung der False-Positive-Rate auf Basis der in der Konfusionsmatrix dargestellten Gr√∂√üen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e03a6-131d-4b30-87bf-3df6ba97e294",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/FPR_Formel.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe92ceb-b3c4-4085-98e2-ec1d482d8685",
   "metadata": {},
   "source": [
    "Durch die kombinierte Betrachtung dieser Metriken kann sowohl die allgemeine Klassifikationsleistung als auch die praktische Einsatzf√§higkeit des Systems im Produktionsumfeld bewertet werden, wobei die zugrunde liegenden Definitionen der Konfusionsmatrix und der Bewertungsmetriken auf etablierten statistischen Grundlagen basieren [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5db6c8-45cf-46db-8598-30280f2e7162",
   "metadata": {},
   "source": [
    "## 2.4 Grundlagen neuronaler Netze f√ºr die Bildklassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db006ab8-c4a8-476d-82e8-d9e63aa2691f",
   "metadata": {},
   "source": [
    "Dieses Kapitel vermittelt die grundlegenden Konzepte neuronaler Netze f√ºr die Bildklassifikation mit besonderem Fokus auf Convolutional Neural Networks (CNNs). Ziel ist es, die zentralen Bausteine, Designprinzipien und Trainingsmechanismen vorzustellen, die f√ºr das Verst√§ndnis der im weiteren Verlauf eingesetzten Modelle erforderlich sind. Die Darstellung konzentriert sich dabei bewusst auf die f√ºr die praktische Anwendung relevanten Aspekte und dient als theoretische Grundlage f√ºr den anschlie√üenden Implementierungs- und Experimentierteil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9544e-4865-4c13-a008-13be67fb67e7",
   "metadata": {},
   "source": [
    "### 2.4.1 Motivation: Warum CNNs statt Fully Connected Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f7636-068c-4701-995b-d02826ea0edb",
   "metadata": {},
   "source": [
    "Bei der Bildklassifikation stellt sich zun√§chst die Frage, warum nicht einfach vollst√§ndig verbundene neuronale Netze (Fully Connected Networks) eingesetzt werden. Solche Netze sind f√ºr tabellarische Daten gut geeignet, sto√üen bei Bilddaten jedoch schnell an ihre Grenzen. Der Hauptgrund ist die starke Zunahme der Parameteranzahl: Bereits mittelgro√üe Bilder f√ºhren nach dem Flattening zu einer sehr gro√üen Anzahl an Gewichten, was das Training ineffizient macht und Overfitting beg√ºnstigt. [5]\n",
    "\n",
    "Ein einfaches Beispiel verdeutlicht dieses Problem: Ein Graustufenbild mit 100 √ó 100 Pixeln besitzt 10 000 Eingabewerte. Eine vollst√§ndig verbundene Schicht mit derselben Anzahl an Neuronen erfordert bereits 100 Millionen Gewichte. Zus√§tzlich geht beim Flattening die r√§umliche Struktur des Bildes verloren, obwohl gerade die Beziehung benachbarter Pixel f√ºr die Bildinterpretation entscheidend ist. [5][6]\n",
    "\n",
    "Convolutional Neural Networks (CNNs) wurden entwickelt, um diese Nachteile zu vermeiden. Sie analysieren Bilder mithilfe von Faltungsschichten, die nur lokale Bildausschnitte betrachten und dabei dieselben Filter √ºber das gesamte Bild anwenden. Dadurch k√∂nnen typische Bildmerkmale wie Kanten oder Texturen unabh√§ngig von ihrer Position erkannt werden, w√§hrend die Anzahl der zu lernenden Parameter stark reduziert wird. [6]\n",
    "\n",
    "CNNs machen Bildklassifikation damit praktikabel, da sie die Eingabedimension effektiv reduzieren, kleine Verschiebungen im Bild tolerieren und gezielt die Korrelation benachbarter Pixel ausnutzen. Aus diesen Gr√ºnden haben sie sich als Standardarchitektur f√ºr Aufgaben der Bildklassifikation etabliert. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3af087-397a-494c-a806-8ed5db9b9eeb",
   "metadata": {},
   "source": [
    "### 2.4.2 Convolutional Layer als zentrales Bauteil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fabe0e-76dc-460e-98bc-b5b65295eff9",
   "metadata": {},
   "source": [
    "Convolutional Layers (auch Faltungsschichten) sind der zentrale Baustein von Convolutional Neural Networks (CNNs). Im Unterschied zu vollvermaschten neuronalen Netzen sind Neuronen in einer Faltungsschicht nicht mit allen Pixeln des Eingabebildes verbunden, sondern nur mit einem lokalen Wahrnehmungsfeld. In tieferen Convolutional Layers beziehen sich Neuronen entsprechend auf lokale Bereiche der vorherigen Schicht, sodass das Netzwerk schrittweise von einfachen zu komplexeren Bildmerkmalen gelangt. [5]\n",
    "\n",
    "Dabei bleiben die Eingabedaten zweidimensional angeordnet, w√§hrend bei vollst√§ndig verbundenen Netzen eine Verflachung erforderlich ist. Die Faltungsschicht realisiert die Merkmalsextraktion mithilfe von Filtern (Kernels), die als kleine Gewichtsmatrizen schrittweise √ºber das Bild verschoben werden. [5]\n",
    "\n",
    "Der grundlegende Aufbau und das Zusammenspiel von Eingabeschicht, Convolutional Layers und Filtern sind in der nachfolgenden Abbildung aus dem Buch *Praxiseinstieg Machine Learning* [5] dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4ab14-903a-45ec-8e0d-2d1052f622f7",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/ConvolutionalLayer.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323b0d7-ee13-4167-ae5a-4d1f9ab40788",
   "metadata": {},
   "source": [
    "In der Praxis kommen h√§ufig kleine Kernelgr√∂√üen wie 3√ó3 Pixel zum Einsatz, die lokale Bildausschnitte analysieren. Durch die Verwendung identischer Filter √ºber das gesamte Eingabebild k√∂nnen charakteristische Muster wie Kanten, Formen oder Texturen unabh√§ngig von ihrer Position erkannt werden. Das zugrunde liegende Weight Sharing reduziert die Anzahl der zu lernenden Parameter und tr√§gt zu einer effizienten Verarbeitung bei. [6]\n",
    "\n",
    "Ein Kernel ist dabei eine kleine Gewichtsmatrix, typischerweise der Gr√∂√üe 3√ó3, die lokal auf das Eingabebild angewendet wird. Abh√§ngig von der spezifischen Anordnung der Gewichte reagieren unterschiedliche Kernel besonders stark auf bestimmte Intensit√§ts√§nderungen, beispielsweise horizontale oder vertikale Kanten. Die folgende Abbildung aus den Vorlesungsunterlagen des Moduls Machine Learning [7] verdeutlicht exemplarisch, wie verschiedene 3√ó3-Kernel auf dieselbe Eingabe angewendet werden und dadurch unterschiedliche strukturelle Bildmerkmale extrahieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3b24e-af81-4d4d-986c-fdfe644fb624",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Kernel.png\" width=\"450\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368333bd-40d5-4fae-8552-614951fa3173",
   "metadata": {},
   "source": [
    "Der im oberen Teil der Abbildung gezeigte Kernel mit positiven Gewichten in der oberen Zeile und negativen Gewichten in der unteren Zeile hebt vor allem horizontale Kanten hervor, da er starke Helligkeitsunterschiede zwischen oberen und unteren Bildbereichen verst√§rkt. Der untere Kernel besitzt dagegen positive und negative Gewichte entlang der horizontalen Achse und reagiert daher besonders auf vertikale Kanten, da hier Intensit√§ts√§nderungen zwischen linken und rechten Bildbereichen betont werden. In beiden F√§llen entsteht ein hoher Aktivierungswert dort, wo ein starker Kontrast in der jeweiligen Richtung vorliegt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5c3c8-4ebe-4aec-b852-a4a96da47b90",
   "metadata": {},
   "source": [
    "Eine Feature Map entsteht durch die Anwendung eines einzelnen Filters auf die Eingabe und hebt jene Bildbereiche hervor, die den Filter besonders stark aktivieren. Ein Convolutional Layer verwendet in der Regel mehrere Filter und erzeugt entsprechend eine Feature Map pro Filter, wobei alle Neuronen innerhalb einer Feature Map dieselben Parameter teilen. Die Filter werden dabei w√§hrend des Trainings automatisch gelernt. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69914619-2ab1-4669-bd28-96e0c271ff8c",
   "metadata": {},
   "source": [
    "Beim Anwenden von Faltungen bestimmen Padding und Stride ma√ügeblich die r√§umliche Gr√∂√üe der entstehenden Feature Maps. Folgende Abbildung entnommen aus dem Buch *Praxiseinstieg Machine Learning* [5] veranschaulicht die beiden Prinzipien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2d9df-0c6c-4c0b-8b62-a857056b8bcd",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/PaddingStride.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a925af5-d5af-4577-89a8-ae2c497c452c",
   "metadata": {},
   "source": [
    "**Padding** bezeichnet das Auff√ºllen der Eingabe mit zus√§tzlichen Nullen an den Randbereichen. Ziel ist es, zu verhindern, dass die Ausgabedarstellung nach jeder Faltung zunehmend kleiner wird. Beim sogenannten Zero Padding bleiben H√∂he und Breite der Ausgabe bei geeigneter Wahl des Paddings konstant. In der Abbildung sind die durch das Padding hinzugef√ºgten Nullwerte grau hinterlegt; der lila markierte Bereich verdeutlicht, dass der Kernel dadurch auch an den Bildr√§ndern vollst√§ndig angewendet werden kann. [5]  \n",
    "Der **Stride** beschreibt den horizontalen und vertikalen Abstand, mit dem der Kernel √ºber die Eingabe verschoben wird. Ein gr√∂√üerer Stride f√ºhrt dazu, dass sich die Wahrnehmungsfelder weniger √ºberlappen und die Ausgabedarstellung entsprechend kleiner wird, was die Rechenkomplexit√§t reduziert. In der Abbildung illustrieren die roten und blauen Markierungen, wie der Kernel mit einer bestimmten Schrittweite √ºber das Eingabegitter bewegt wird. [5]  \n",
    "Padding und Stride beeinflussen damit ma√ügeblich die r√§umliche Aufl√∂sung der Feature Maps und stellen zentrale Stellschrauben beim Entwurf von Convolutional Neural Networks dar. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01ce42-b31b-498e-bc1a-66208ef1b3e0",
   "metadata": {},
   "source": [
    "### 2.4.3 Architekturprinzipien von CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a88e6-018b-4168-9d90-ee5658900e06",
   "metadata": {},
   "source": [
    "In typischen CNN-Architekturen werden mehrere Convolutional Layers sequenziell aufeinandergestapelt. Folgende Abbildung [8] veranschaulicht  den schematischen Aufbau einer solchen CNN-Architektur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33861b-88c2-4989-98a2-40df2f97c39d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/CNN_Architektur.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2660b11-a88e-46a6-acb4-da9e91920f2d",
   "metadata": {},
   "source": [
    "Wie in der Abbildung zu sehen, verwendet jeder Convolutional Layer dabei mehrere Filter (Kernel), sodass aus einer Eingabe mehrere Feature Maps entstehen. Die Anzahl dieser Feature Maps entspricht der Anzahl der eingesetzten Filter und definiert die Tiefe der Schicht. W√§hrend die r√§umliche Aufl√∂sung der Feature Maps im Verlauf des Netzes meist abnimmt, steigt deren Tiefe typischerweise an, da zunehmend komplexere und abstraktere Bildmerkmale extrahiert werden. Ein wesentlicher Punkt ist, dass ein Filter in einem tieferen Convolutional Layer nicht nur auf eine einzelne Feature Map, sondern auf alle Feature Maps der vorherigen Schicht angewendet wird. Die resultierende Feature Map entsteht somit aus einer gewichteten Kombination s√§mtlicher Eingangskan√§le und erlaubt es dem Netzwerk, Merkmale h√∂herer Ordnung zu lernen, die mehrere zuvor erkannte Strukturen miteinander verkn√ºpfen. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266deaf5-ad65-43a0-aa9e-36cf43e4e3fa",
   "metadata": {},
   "source": [
    "√úblicherweise folgt auf jeden Convolutional Layer unmittelbar eine nichtlineare Aktivierungsfunktion, in modernen CNN-Architekturen meist die ReLU-Funktion. Erst durch diese Nichtlinearit√§t ist das Netzwerk in der Lage, komplexe Zusammenh√§nge und nichtlineare Entscheidungsgrenzen zu modellieren. Nach ein oder mehreren solchen Kombinationen aus Faltung und Aktivierung wird h√§ufig ein Pooling Layer eingef√ºgt, bevor der n√§chste Block aus Convolutional Layers beginnt. Durch diese wiederholte Abfolge wird die r√§umliche Aufl√∂sung der Feature Maps im Verlauf des Netzes schrittweise reduziert, w√§hrend deren Tiefe typischerweise zunimmt, da zunehmend abstraktere und komplexere Bildmerkmale extrahiert werden. [5]\n",
    "\n",
    "Die ReLU-Funktion (Rectified Linear Unit) hat sich dabei als De-facto-Standard etabliert. Sie setzt negative Eingaben auf null, w√§hrend positive Werte unver√§ndert weitergegeben werden. Aufgrund ihrer einfachen mathematischen Struktur ist ReLU recheneffizient und unterst√ºtzt eine schnellere Konvergenz des Gradientenverfahrens, kann jedoch bei ung√ºnstiger Wahl der Lernrate zum Auftreten sogenannter ‚Äûtoter Neuronen‚Äú f√ºhren. Unabh√§ngig von der konkreten Aktivierungsfunktion wird diese elementweise auf jedes Element der Feature Maps angewendet, wobei es in modernen CNN-Architekturen g√§ngige Praxis ist, nach jeder Faltung oder nach Pooling-Operationen eine ReLU-basierte Aktivierung einzusetzen. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02926fa-27d2-4d67-8aa0-3797447cd700",
   "metadata": {},
   "source": [
    "Pooling Layers dienen dazu, die r√§umliche Gr√∂√üe der Feature Maps zu reduzieren, indem aus lokalen Bildbereichen Unterstichproben gezogen werden. Dadurch verringern sich sowohl die Rechenlast als auch der Speicherbedarf des Netzes, w√§hrend gleichzeitig das Risiko von Overfitting sinkt. Folgende Abbildung [5] veranschaulicht exemplarisch die Funktionsweise eines Max-Pooling-Layers, bei dem aus jedem lokalen Wahrnehmungsfeld lediglich der gr√∂√üte Aktivierungswert an die n√§chste Schicht weitergegeben wird. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c914193-ad9c-40de-ae01-e2a5e066362f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Pooling.png\" width=\"600\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f80f9-047d-487b-b784-ec008c40c424",
   "metadata": {},
   "source": [
    "√Ñhnlich wie bei Convolutional Layers besitzt jedes Neuron eines Pooling Layers ein lokales Wahrnehmungsfeld mit definierter Kernel-Gr√∂√üe, Schrittweite und Padding, wie in der Abbildung dargestellt. Im Gegensatz zur Faltung enthalten Pooling-Neuronen jedoch keine lernbaren Gewichte, sondern aggregieren die Eingabewerte ausschlie√ülich √ºber eine feste Funktion. Bei dem in der obigen Abbildung gezeigten Max-Pooling werden alle √ºbrigen Werte innerhalb eines Wahrnehmungsfeldes verworfen. [5]\n",
    "\n",
    "Neben der Reduktion des Rechenaufwands erzeugt Max-Pooling eine gewisse Invarianz gegen√ºber kleinen Verschiebungen im Bild. Durch die wiederholte Anwendung von Pooling-Schichten kann zudem eine begrenzte Robustheit gegen√ºber Translationen, Rotationen und Skalierungen erreicht werden, was insbesondere f√ºr Klassifikationsaufgaben vorteilhaft ist. [5]\n",
    "\n",
    "Demgegen√ºber steht der Nachteil der starken Informationsreduktion: Bereits bei einem 2√ó2-Kernel mit Schrittweite 2 wird die r√§umliche Aufl√∂sung in beide Richtungen halbiert, sodass insgesamt 75 % der Eingabewerte verloren gehen, was als stark destruktiv betrachtet werden kann. [5]\n",
    "\n",
    "Eine alternative Pooling-Variante ist das Average-Pooling, bei dem anstelle des Maximums der Mittelwert der Werte innerhalb eines Wahrnehmungsfeldes berechnet wird. Obwohl Average-Pooling fr√ºher weit verbreitet war, kommen in modernen CNN-Architekturen √ºberwiegend Max-Pooling-Layers zum Einsatz, da sie in vielen Anwendungsf√§llen eine bessere Leistung erzielen. [5]  \n",
    "\n",
    "Eine spezielle Form des Average-Poolings ist das Global Average Pooling, bei dem √ºber die gesamte r√§umliche Ausdehnung jeder Feature Map gemittelt wird. Dadurch entsteht pro Feature Map genau ein Skalarwert, sodass auf eine explizite Flatten-Operation und gro√üe Fully-Connected-Schichten verzichtet werden kann, was die Anzahl der Parameter weiter reduziert und Overfitting entgegenwirkt. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b72e35-04dc-4a64-aac6-575b27578c26",
   "metadata": {},
   "source": [
    "Am oberen Ende eines CNNs befindet sich der Klassifikationskopf. Die zuletzt erzeugten Feature Maps werden hierf√ºr zun√§chst geflattet, sodass sie als Vektor an ein klassisches Feed-Forward-Netz aus vollst√§ndig verbundenen Schichten √ºbergeben werden k√∂nnen. Diese Schichten kombinieren die zuvor extrahierten Merkmale global und f√ºhren die eigentliche Klassifikation durch. [5]  \n",
    "Die finale Ausgabeschicht besitzt typischerweise so viele Neuronen wie Zielklassen und verwendet eine geeignete Aktivierungsfunktion, beispielsweise Softmax, um gesch√§tzte Klassenwahrscheinlichkeiten auszugeben. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eef10f-33d4-4056-834a-8820e29c5122",
   "metadata": {},
   "source": [
    "### 2.4.4 Regularisierung in CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884f347-deeb-467c-9166-da49dd7cda11",
   "metadata": {},
   "source": [
    "Bei der Verarbeitung von Bilddaten besitzen Convolutional Neural Networks h√§ufig eine sehr hohe Modellkapazit√§t. Insbesondere bei begrenzter Datenmenge besteht daher die Gefahr des Overfittings, bei dem das Modell die Trainingsdaten sehr gut approximiert, jedoch schlecht auf unbekannte Daten generalisiert. Regularisierungstechniken zielen darauf ab, die Generalisierungsf√§higkeit des Netzes zu verbessern, indem sie √úberanpassung reduzieren und das Lernen robuster gestalten. In CNNs kommen hierf√ºr insbesondere Data Augmentation, Dropout und Batchnormalisierung zum Einsatz. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c746a84-555c-4dad-920b-cd86dee92a5e",
   "metadata": {},
   "source": [
    "**Data Augmentation**  \n",
    "\n",
    "Eine besonders effektive Regularisierungsmethode f√ºr Bilddaten ist Data Augmentation. Dabei wird die Gr√∂√üe des Trainingsdatensatzes k√ºnstlich erh√∂ht, indem aus vorhandenen Bildern realistische Varianten erzeugt werden. Typische Transformationen umfassen leichte Verschiebungen, Rotationen, Skalierungen, Spiegelungen oder √Ñnderungen von Helligkeit und Kontrast. Die erzeugten Bilder sollten dabei so realistisch sein, dass sie nicht eindeutig von Originaldaten unterscheidbar sind. [5]\n",
    "\n",
    "Durch Data Augmentation wird das Modell gezwungen, tolerant gegen√ºber Variationen in Position, Orientierung, Gr√∂√üe und Beleuchtung der Objekte zu reagieren, wodurch Overfitting wirksam reduziert wird. Zus√§tzlich kann Data Augmentation bei unausbalancierten Datens√§tzen eingesetzt werden, um seltene Klassen gezielt zu verst√§rken, etwa durch synthetische Oversampling-Verfahren. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ace8a0-c546-4484-bd2e-03984fe6929f",
   "metadata": {},
   "source": [
    "**Dropout**  \n",
    "\n",
    "Eine weitere h√§ufig eingesetzte Regularisierungstechnik ist Dropout. Dabei werden w√§hrend des Trainings in jedem Schritt einzelne Neuronen mit einer festen Wahrscheinlichkeit \n",
    "ùëù tempor√§r deaktiviert. Die betroffenen Neuronen tragen in diesem Trainingsschritt weder zur Vorw√§rts- noch zur R√ºckw√§rtspropagation bei, k√∂nnen jedoch im n√§chsten Schritt wieder aktiv sein. Nach Abschluss des Trainings wird Dropout deaktiviert und alle Neuronen werden vollst√§ndig genutzt. [5]\n",
    "\n",
    "Die Dropout-Rate \n",
    "ùëù liegt typischerweise zwischen 10 % und 50 %. Durch das zuf√§llige Weglassen von Neuronen wird verhindert, dass sich das Modell zu stark auf einzelne Aktivierungen verl√§sst, was die Robustheit erh√∂ht und Overfitting reduziert. In vielen Deep-Learning-Architekturen f√ºhrt Dropout zu einer leichten, aber stabilen Verbesserung der Generalisierungsleistung. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542852f-dd90-4899-8630-a13287d4b951",
   "metadata": {},
   "source": [
    "**Batchnormalisierung**\n",
    "\n",
    "Batchnormalisierung wurde urspr√ºnglich eingef√ºhrt, um Probleme wie schwindende oder explodierende Gradienten zu reduzieren. Dazu wird vor der Aktivierungsfunktion jeder verborgenen Schicht eine Normalisierung der Eingaben durchgef√ºhrt, bei der Mittelwert und Standardabweichung auf Basis des aktuellen Mini-Batches gesch√§tzt werden. Die normalisierten Werte werden anschlie√üend √ºber lernbare Skalierungs- und Verschiebungsparameter angepasst. [5]\n",
    "\n",
    "Neben der Stabilisierung des Trainings wirkt Batchnormalisierung auch als impliziter Regularisierer und kann die Notwendigkeit zus√§tzlicher Regularisierungstechniken verringern. Zwar erh√∂ht sie durch zus√§tzliche Parameter und Normalisierungsoperationen den Rechenaufwand pro Trainingsschritt leicht, jedoch erm√∂glicht sie h√§ufig eine stabilere Optimierung und h√∂here Lernraten. Dadurch konvergieren Modelle in der Praxis oftmals schneller und ben√∂tigen weniger Trainingsdurchl√§ufe. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f5f2e-41c9-459f-89fe-64d2d9680d01",
   "metadata": {},
   "source": [
    "Insgesamt erg√§nzen sich Data Augmentation, Dropout und Batchnormalisierung in ihrer Wirkung. W√§hrend Data Augmentation direkt auf Datenebene ansetzt, beeinflussen Dropout und Batchnormalisierung das Lernverhalten des Modells und tragen gemeinsam zu einer verbesserten Generalisierung bei. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2a1a4-e2f2-4259-92d2-98ec75671a50",
   "metadata": {},
   "source": [
    "### 2.4.5 Trainingsablauf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783a976-3380-4ffb-9520-88ebd3c1150c",
   "metadata": {},
   "source": [
    "Das Training eines Convolutional Neural Networks folgt einem klar strukturierten Ablauf, der mehrere aufeinander aufbauende Schritte umfasst. Zun√§chst werden die Bilddaten vorbereitet und in geeigneter Form bereitgestellt. Vor dem eigentlichen Training wird das neuronale Netzwerk als Modell instanziiert. Dabei werden die zuvor definierte Netzwerkarchitektur sowie alle zugeh√∂rigen lernbaren Parameter initialisiert. Erst durch diese Instanziierung entsteht ein konkretes Modellobjekt, das mit einer Verlustfunktion und einem Optimierungsverfahren verkn√ºpft und trainiert werden kann. Im weiteren Verlauf erfolgt das eigentliche Training des Netzwerks, bei dem Vorhersagen berechnet, Verluste bestimmt und die Netzwerkgewichte optimiert werden, wobei zwischen Trainings-, Validierungs- und Testphasen unterschieden wird. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528588a6-03f7-43df-b818-48887be39e31",
   "metadata": {},
   "source": [
    "**Datenvorverarbeitung, Dataset und DataLoader**\n",
    "\n",
    "Vor dem Training werden die Bilddaten √ºblicherweise vorverarbeitet, indem alle Bilder auf eine einheitliche Gr√∂√üe skaliert, anhand von Mittelwert und Standardabweichung normalisiert und in PyTorch-Tensoren konvertiert werden. Diese Schritte stellen sicher, dass die Eingaben den Erwartungen der Netzwerkarchitektur entsprechen und stabil verarbeitet werden k√∂nnen. [6]\n",
    "\n",
    "In Deep-Learning-Anwendungen werden Bilddaten in der Regel batchweise verarbeitet, da CPUs und GPUs f√ºr parallele Berechnungen auf ganzen Batches optimiert sind. PyTorch stellt hierf√ºr die Klasse DataLoader bereit, die ein Dataset kapselt und Mini-Batches von Bildern bereitstellt. Aufgaben wie das Durchmischen der Daten oder die parallele Datenbereitstellung √ºber mehrere Worker werden dabei automatisch √ºbernommen. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d187ce0-aa70-41e2-b626-df77d6cac432",
   "metadata": {},
   "source": [
    "**Verlustfunktion und Optimierer**\n",
    "\n",
    "Nach Definition der Netzwerkarchitektur wird bewertet, wie gut das Modell die zugrunde liegende Aufgabe erf√ºllt. Hierzu wird eine Verlustfunktion verwendet, die einen skalaren Fehlerwert berechnet, welcher w√§hrend des Trainings minimiert wird. Da Optimierungsalgorithmen typischerweise einen solchen Skalar erwarten, ist die Wahl einer geeigneten Verlustfunktion essenziell. [6]\n",
    "\n",
    "F√ºr Klassifikationsaufgaben kommt √ºblicherweise die Cross-Entropy-Loss zum Einsatz. Sie misst die Abweichung zwischen den vorhergesagten Klassenwahrscheinlichkeiten und der tats√§chlichen Zielklasse. Je st√§rker die Vorhersage von der wahren Klasse abweicht, desto h√∂her f√§llt der Verlust aus, w√§hrend korrekte und sichere Vorhersagen zu einem niedrigen Verlust f√ºhren. [6]\n",
    "\n",
    "Zur Minimierung des Verlusts wird ein Optimierungsalgorithmus eingesetzt, der die lernbaren Parameter des Netzwerks auf Basis der berechneten Gradienten iterativ anpasst. In der Praxis existieren verschiedene Optimierer, die sich insbesondere in der Art der Lernratenanpassung und Regularisierung unterscheiden. Eine weit verbreitete Klasse stellen adaptive Verfahren wie Adam dar, die Momentensch√§tzungen der Gradienten nutzen und dadurch ein stabiles und effizientes Training erm√∂glichen. In dieser Arbeit wird die Variante AdamW verwendet, bei der die Gewichtsregularisierung (Weight Decay) explizit von der Gradientenoptimierung entkoppelt ist. Dies f√ºhrt zu einer effektiveren Regularisierung und wird in vielen Anwendungsf√§llen mit einer verbesserten Generalisierungsf√§higkeit des Modells in Verbindung gebracht. [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab69b3-0115-4cd5-b9ea-dccc3fb671bd",
   "metadata": {},
   "source": [
    "**Ablauf eines Trainingsschritts**\n",
    "\n",
    "Der eigentliche Trainingsschritt innerhalb einer Epoche folgt einem festen Schema. Zun√§chst werden die zuvor berechneten Gradienten zur√ºckgesetzt, um eine Akkumulation √ºber mehrere Schritte zu vermeiden. Anschlie√üend erfolgt der Vorw√§rtsdurchlauf, bei dem das Modell aus den Eingabedaten Vorhersagen berechnet. Auf Basis dieser Vorhersagen wird der Verlust bestimmt, bevor im R√ºckw√§rtsdurchlauf mittels Backpropagation die Gradienten berechnet werden. Abschlie√üend werden die Modellparameter durch einen Optimierungsschritt aktualisiert. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896bc8a-c5ef-4d72-9184-7a660a18f9c1",
   "metadata": {},
   "source": [
    "**Trainings-, Validierungs- und Testphase**\n",
    "\n",
    "Jede Epoche gliedert sich in eine Trainings- und eine Validierungsphase. W√§hrend der Trainingsphase werden Vorhersagen berechnet, der Verlust bestimmt und die Netzwerkgewichte angepasst. In der Validierungsphase hingegen werden ausschlie√ülich Vorhersagen erzeugt und bewertet, ohne dass eine Gewichtsaktualisierung erfolgt. Zus√§tzlich wird eine Testphase verwendet, in der das final ausgew√§hlte Modell auf zuvor ungesehenen Daten evaluiert wird, um dessen Generalisierungsf√§higkeit objektiv zu beurteilen. [6]\n",
    "\n",
    "F√ºr Training, Validierung und Test werden unterschiedliche Modellmodi verwendet. Im Trainingsmodus sind Regularisierungstechniken wie Dropout aktiv, und es werden Gradienten berechnet. Im Validierungs- und Testmodus hingegen ist Dropout deaktiviert, und es findet keine Backpropagation statt, um eine unverzerrte Bewertung der Modellleistung zu gew√§hrleisten. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e9e94-cdc1-4c01-abb4-a60b36c4c3fd",
   "metadata": {},
   "source": [
    "## 2.5 Transfer Learning mit ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a85e4-0313-4d8c-87be-bb398c329e79",
   "metadata": {},
   "source": [
    "Nachdem in Kapitel 2.4 die grundlegenden Konzepte neuronaler Netze f√ºr die Bildklassifikation sowie der Aufbau und das Training von Convolutional Neural Networks erl√§utert wurden, wird in diesem Kapitel ein weiterf√ºhrender Architekturansatz vorgestellt. Im Fokus stehen Transfer Learning und die ResNet-Architektur als Weiterentwicklung klassischer CNNs. ResNet adressiert zentrale Herausforderungen beim Training sehr tiefer Netze und bildet die Grundlage vieler vortrainierter Modelle f√ºr Bildverarbeitungsaufgaben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273fc10-3627-4acf-b8a9-62d0e1c4a663",
   "metadata": {},
   "source": [
    "### 2.5.1 Grundidee des Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972a4dd-e570-49c7-a350-1b18858cd346",
   "metadata": {},
   "source": [
    "Transfer Learning beschreibt die Wiederverwendung eines bereits trainierten neuronalen Netzes f√ºr eine neue, inhaltlich √§hnliche Aufgabe, ohne das Modell vollst√§ndig von Grund auf neu zu trainieren. Ziel ist es, vorhandenes Wissen aus einem gro√üen Quell-Datensatz auf einen kleineren oder spezifischeren Ziel-Datensatz zu √ºbertragen, um Trainingszeit und Datenbedarf zu reduzieren. [6]\n",
    "\n",
    "In Convolutional Neural Networks sind die gelernten Merkmale hierarchisch organisiert. Fr√ºhere Schichten erfassen grundlegende visuelle Strukturen wie Kanten, Linien oder einfache Texturen, w√§hrend tiefere Schichten zunehmend komplexe und aufgabenspezifische Merkmale modellieren. Diese allgemeinen Merkmale lassen sich in vielen Bildverarbeitungsaufgaben wiederverwenden. [6]\n",
    "\n",
    "Vortrainierte Modelle werden typischerweise auf gro√üen, allgemein gehaltenen Datens√§tzen wie ImageNet trainiert, der 1000 Objektklassen umfasst. Dabei entstehen Filtergewichte, die auf die Erkennung vielf√§ltiger visueller Muster optimiert sind und sich gut auf verwandte Aufgaben √ºbertragen lassen. [6]\n",
    "\n",
    "Beim Transfer Learning wird in der Regel die Architektur des vortrainierten Modells √ºbernommen. Lediglich der Klassifikationskopf wird an die neue Aufgabe angepasst, etwa durch √Ñnderung der Anzahl der Ausgabeneuronen. Die gelernten Parameter der fr√ºhen Netzwerkschichten werden h√§ufig eingefroren, w√§hrend nur die letzten Schichten weiter trainiert oder feinjustiert werden. Dieses Vorgehen reduziert das Risiko von √úberanpassung bei begrenzten Datens√§tzen und erm√∂glicht eine effiziente Nutzung des vortrainierten Wissens. Abh√§ngig von Datensatzgr√∂√üe und Aufgaben√§hnlichkeit kommen unterschiedliche Fine-Tuning-Strategien zum Einsatz, die im weiteren Verlauf erl√§utert werden. [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c528554-60b5-4007-a126-5918e2088ce2",
   "metadata": {},
   "source": [
    "### 2.5.2 Vorgehensweisen im Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711342a-c3b0-490a-92ab-efe61d1b8248",
   "metadata": {},
   "source": [
    "Beim Transfer Learning haben sich mehrere Vorgehensweisen etabliert, die sich vor allem im Umfang der trainierbaren Parameter unterscheiden. Die Wahl der Strategie h√§ngt ma√ügeblich von der Gr√∂√üe des Ziel-Datensatzes sowie von der √Ñhnlichkeit zwischen Quell- und Zielaufgabe ab. [6]\n",
    "\n",
    "Eine verbreitete Methode ist die Nutzung eines vortrainierten Modells als reiner Feature-Extraktor. Dabei werden die Gewichte der gesamten vortrainierten Architektur eingefroren, und lediglich der neu hinzugef√ºgte Klassifikationskopf wird trainiert. Diese Vorgehensweise ist besonders geeignet, wenn nur wenige Trainingsdaten zur Verf√ºgung stehen oder wenn die Zielaufgabe der urspr√ºnglichen Trainingsaufgabe stark √§hnelt. [6]\n",
    "\n",
    "Eine weiterf√ºhrende Strategie ist das Fine-Tuning, bei dem zus√§tzlich zu dem Klassifikationskopf auch ausgew√§hlte tiefere Schichten des Netzwerks weiter trainiert werden. H√§ufig werden dabei zun√§chst nur die letzten Schichten freigegeben, w√§hrend fr√ºhe Schichten weiterhin eingefroren bleiben. Auf diese Weise kann das Modell auf aufgabenspezifische Merkmale angepasst werden, ohne die generalisierenden Eigenschaften der fr√ºh gelernten Filter zu verlieren. [6]\n",
    "\n",
    "Als Faustregel gilt, dass bei kleinen Datens√§tzen und hoher Aufgaben√§hnlichkeit ein weitgehendes Einfrieren der Architektur sinnvoll ist, w√§hrend bei gr√∂√üeren Datens√§tzen oder st√§rker abweichenden Zielaufgaben ein partielles oder vollst√§ndiges Fine-Tuning bessere Ergebnisse liefern kann. In der Praxis werden diese Strategien h√§ufig schrittweise kombiniert, indem zun√§chst nur der Klassifikationskopf trainiert und anschlie√üend ausgew√§hlte Netzwerkschichten freigegeben werden. [6]  \n",
    "\n",
    "Im weiteren Verlauf dieser Arbeit werden diese Vorgehensweisen exemplarisch anhand einer ResNet-18-Architektur untersucht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d27e02-077b-4e76-bfe2-862b9cfee42e",
   "metadata": {},
   "source": [
    "### 2.5.3 ResNet: Motivation und Kernidee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a950ee-3a45-4ef5-8881-d89479b992cd",
   "metadata": {},
   "source": [
    "Sehr tiefe neuronale Netze verf√ºgen grunds√§tzlich √ºber eine hohe Modellkapazit√§t, lassen sich in der Praxis jedoch h√§ufig nur schwer trainieren. Mit zunehmender Tiefe treten Probleme wie verschwindende Gradienten auf, wodurch fr√ºhere Schichten kaum noch angepasst werden. In der Folge stagniert oder verschlechtert sich die Modellleistung, obwohl zus√§tzliche Schichten hinzugef√ºgt werden. [5]\n",
    "\n",
    "ResNet (Residual Network) adressiert dieses Problem durch sogenannte Skip-Verbindungen, deren Prinip in folgender Abbildung [5] dargestellt ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b15f72-c708-4582-a6ad-280e73f31ed0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/ResNet_SkipVerbindungen.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed74f3-fe2f-4eed-a14a-d898351d4993",
   "metadata": {},
   "source": [
    "Durch die Skip-Verbindungen wird die Eingabe einer Schicht nicht ausschlie√ülich durch die nachfolgenden Schichten verarbeitet, sondern zus√§tzlich direkt an eine weiter oben liegende Schicht weitergeleitet und dort zur Ausgabe addiert. Das Netzwerk muss somit nicht mehr die vollst√§ndige Abbildung von Eingabe zu Ausgabe lernen, sondern lediglich eine Korrektur zur Eingabe. Diese Idee wird als Residual Learning bezeichnet. Anstatt eine komplett neue Transformation zu erlernen, konzentriert sich das Modell darauf, nur die Abweichung zwischen Eingabe und gew√ºnschter Ausgabe zu modellieren. Liegt die optimale Abbildung nahe an einer unver√§nderten Weitergabe der Eingabe, f√§llt das Lernen dadurch deutlich leichter. [5]\n",
    "\n",
    "Ein weiterer zentraler Vorteil der Skip-Verbindungen ist die verbesserte Weitergabe der Gradienten w√§hrend des Trainings. Durch die direkten Verbindungen k√∂nnen Gradienten auch bei sehr tiefen Netzen zuverl√§ssig zu fr√ºheren Schichten zur√ºckflie√üen. Dadurch bleiben diese Schichten lernf√§hig, selbst wenn einzelne nachfolgende Schichten nur langsam oder zeitweise gar nicht lernen. Ein ResNet besteht folglich aus einem Stapel sogenannter Residual Units. Jede Residual Unit umfasst mehrere Faltungsschichten sowie eine Skip-Verbindung, die die Eingabe der Unit direkt mit ihrer Ausgabe verkn√ºpft. Diese Architektur erm√∂glicht es, sehr tiefe neuronale Netze zu trainieren, ohne dass sich die typischen Trainingsprobleme klassischer tiefer Architekturen einstellen. [5]  \n",
    "\n",
    "Die folgende Abbildung [5] veranschaulicht dieses Prinzip schematisch. Auf der linken Seite ist ein klassisches tiefes neuronales Netz ohne Skip-Verbindungen dargestellt. Mit zunehmender Tiefe kann es hier zu Problemen bei der Gradientenweitergabe kommen, sodass einzelne Schichten nicht mehr effektiv lernen. Auf der rechten Seite ist hingegen die Struktur eines Deep-Residual-Netzes gezeigt. Durch die Verwendung von Skip-Verbindungen werden mehrere Residual Units gebildet, die es dem Gradienten erlauben, direkt durch das Netzwerk zu flie√üen. Dadurch bleiben auch sehr tiefe Netze trainierbar und die genannten Trainingsprobleme werden deutlich reduziert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3577e41-6faa-4420-8682-4352d627885e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/ResNet_ResidualUnits.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7917a72-cbe0-475a-8c8e-de5a2d13da6f",
   "metadata": {},
   "source": [
    "### 2.5.4 Aufbau einer ResNet-Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9df62-d4ea-4c3b-9ebb-ec8c83c8a08f",
   "metadata": {},
   "source": [
    "Die ResNet-Architektur basiert auf dem wiederholten Einsatz sogenannter *Residual Units*, die aus mehreren Faltungsschichten und einer Skip-Verbindung bestehen. Eine Residual Unit verkn√ºpft die Eingabe direkt mit der Ausgabe des Blocks, sodass das Netzwerk nicht die vollst√§ndige Transformation, sondern lediglich eine Korrektur zur Eingabe lernen muss. Diese Struktur stellt das zentrale architektonische Element von Residual Networks dar. [5]  \n",
    "\n",
    "Die folgende Abbildung [5] zeigt den schematischen Aufbau einer ResNet-Architektur sowie die Struktur einer einzelnen Residual Unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415b0b8-8975-4ce8-a6a1-83fe30655f63",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/ResNet_Architektur.png\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c3798-3957-4371-a4ae-b8094fea255c",
   "metadata": {},
   "source": [
    "Eine typische ResNet-Architektur beginnt mit einer initialen Faltungsschicht mit gro√üem Kernel, gefolgt von einer Pooling-Schicht zur Reduktion der r√§umlichen Aufl√∂sung. Anschlie√üend folgt ein tiefer Stapel von Residual Units, die in mehreren Stufen organisiert sind. Innerhalb einer Stufe besitzen alle Residual Units die gleiche Anzahl an Feature Maps. Zwischen den Stufen wird die r√§umliche Aufl√∂sung reduziert, w√§hrend sich die Anzahl der Feature Maps erh√∂ht. [5]\n",
    "\n",
    "Jede Residual Unit besteht in der Regel aus zwei aufeinanderfolgenden Faltungsschichten mit Kernelgr√∂√üe 3√ó3, kombiniert mit Batch-Normalisierung und ReLU-Aktivierung. Falls sich die Dimensionen von Eingabe und Ausgabe unterscheiden, wird die Skip-Verbindung √ºber eine zus√§tzliche 1√ó1-Faltung angepasst, um eine elementweise Addition zu erm√∂glichen. [5]\n",
    "\n",
    "Am Ende der Architektur wird h√§ufig ein Global Average Pooling eingesetzt, das die r√§umlichen Feature Maps auf einen Vektor reduziert. Darauf folgt ein vollst√§ndig verbundener Klassifikationskopf, dessen Ausgabedimension an die jeweilige Klassifikationsaufgabe angepasst wird. [5]\n",
    "\n",
    "Die grundlegende Struktur ist bei allen ResNet-Varianten identisch. Unterschiede zwischen den einzelnen Modellen ergeben sich ausschlie√ülich aus der Anzahl der verwendeten Residual Units und damit aus der Tiefe des Netzwerks. G√§ngige Varianten sind unter anderem ResNet-18, ResNet-34, ResNet-50, ResNet-101 und ResNet-152, die sich in der Anzahl der Schichten und der Modellkomplexit√§t unterscheiden.  [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d121dba-0632-46b4-9641-f2eae4a96697",
   "metadata": {},
   "source": [
    "## 2.6 Grundlagen AutoEncoder\n",
    "\n",
    "W√§hrend Transfer-Learning-Ans√§tze mit vortrainierten Architekturen wie ResNet leistungsf√§hige Werkzeuge f√ºr √ºberwachte Bildklassifikationsaufgaben darstellen, setzen sie die Verf√ºgbarkeit klar definierter und repr√§sentativer Klassenlabels voraus. In Szenarien, in denen fehlerhafte Beispiele selten, stark variabel oder nur eingeschr√§nkt verf√ºgbar sind, gewinnen alternative Lernparadigmen an Bedeutung. Vor diesem Hintergrund werden im folgenden Abschnitt die Grundlagen von Autoencodern vorgestellt, die einen un√ºberwachten Zugang zur Merkmalsextraktion und Anomalieerkennung erm√∂glichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfa0bc-7f08-498d-b423-69e64192bc31",
   "metadata": {},
   "source": [
    "### 2.6.1 Grundidee\n",
    "\n",
    "Ein Autoencoder ist ein k√ºnstliches neuronales Netzwerk, dessen Ziel es ist, eine Eingabe m√∂glichst verlustfrei auf sich selbst abzubilden. Im Gegensatz zu klassischen Klassifikations- oder Regressionsmodellen wird kein explizites Zielsignal in Form von Klassenlabels vorgegeben. Stattdessen lernt das Modell eine komprimierte Repr√§sentation der Eingabedaten, aus der die urspr√ºngliche Eingabe wieder rekonstruiert werden kann.\n",
    "\n",
    "Die zentrale Annahme eines Autoencoders ist, dass sich die wesentlichen Strukturen der Daten in einem niederdimensionalen Merkmalsraum erfassen lassen. Durch diese Kompression ist das Modell gezwungen, nur die relevanten Merkmale der Daten zu speichern und irrelevante oder zuf√§llige Variationen zu vernachl√§ssigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1df46c-f107-454d-b531-edf1947b1041",
   "metadata": {},
   "source": [
    "### 2.6.2 Vorgehensweisen\n",
    "\n",
    "Das Training eines Autoencoders erfolgt in der Regel un√ºberwacht. Die Eingabedaten dienen gleichzeitig als Zielwerte, wodurch sich das Lernproblem formal als Minimierung eines Rekonstruktionsfehlers formulieren l√§sst. Typische Vorgehensweisen sind:\n",
    "* Rekonstruktionsbasiertes Lernen: Minimierung der Differenz zwischen Eingabe und Ausgabe (z. B. mittels MSE oder L1-Loss).\n",
    "* Dimensionalit√§tsreduktion: Erlernen kompakter latenter Repr√§sentationen, √§hnlich der Hauptkomponentenanalyse (PCA), jedoch nichtlinear.\n",
    "* Regularisierung: Einschr√§nkung der Modellkapazit√§t (z. B. √ºber Bottleneck-Gr√∂√üe, Sparsity oder Rauscheinjektion), um triviale Identit√§tsabbildungen zu vermeiden.\n",
    "* Dom√§nenspezifische Varianten: Einsatz von Convolutional Autoencodern f√ºr Bilddaten, da diese lokale Strukturen und Translationen besser erfassen.\n",
    "* Je nach Ausgestaltung k√∂nnen Autoencoder unterschiedliche Lernziele verfolgen, etwa robuste Rekonstruktion, Rauschunterdr√ºckung oder die Modellierung eines Normalzustands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470c328-aa14-40bd-983e-5a6f50fe3aff",
   "metadata": {},
   "source": [
    "### 2.6.3 Autoencoder ‚Äì Motivation und Kernidee\n",
    "\n",
    "Die Motivation f√ºr den Einsatz von Autoencodern liegt in ihrer F√§higkeit, Datenstrukturen ohne explizite Annotationen zu erlernen. Besonders in Anwendungsf√§llen, in denen gelabelte Daten schwer verf√ºgbar sind oder Defekte nur selten auftreten, bietet dieser Ansatz einen entscheidenden Vorteil gegen√ºber √ºberwachten Lernverfahren.\n",
    "\n",
    "Die Kernidee besteht darin, dass ein Autoencoder nur solche Muster gut rekonstruieren kann, die er w√§hrend des Trainings h√§ufig gesehen hat. Wird das Modell ausschlie√ülich mit fehlerfreien Beispielen trainiert, entsteht ein implizites Modell des Normalzustands. Weichen neue Eingaben signifikant von diesem Zustand ab, steigt der Rekonstruktionsfehler deutlich an. Dieser Effekt kann gezielt zur Anomalieerkennung genutzt werden.\n",
    "\n",
    "Damit verlagert sich das Entscheidungsproblem von einer expliziten Klassifikation hin zu einer Bewertung der Abweichung von gelernten Normalstrukturen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac17721-2050-40c7-84b3-070f440a2d1d",
   "metadata": {},
   "source": [
    "### 2.6.4 Aufbau einer Autoencoder-Architektur\n",
    "\n",
    "Eine klassische Autoencoder-Architektur besteht aus zwei Hauptkomponenten:\n",
    "\n",
    "Encoder:\n",
    "Der Encoder transformiert die hochdimensionale Eingabe in eine kompakte, latente Repr√§sentation. Diese Transformation erfolgt √ºber mehrere neuronale Schichten, die schrittweise die Dimensionalit√§t reduzieren und abstrakte Merkmale extrahieren.\n",
    "\n",
    "Latenter Raum (Bottleneck):\n",
    "Der Bottleneck stellt die niedrigdimensionale Kodierung der Eingabe dar. Seine Gr√∂√üe und Struktur bestimmen ma√ügeblich, wie viel Information gespeichert werden kann und wirken als zentrale Regularisierung des Modells.\n",
    "\n",
    "Decoder:\n",
    "Der Decoder rekonstruiert aus der latenten Repr√§sentation wieder eine Ausgabe im urspr√ºnglichen Datenraum. Er ist typischerweise spiegelbildlich zum Encoder aufgebaut.\n",
    "\n",
    "F√ºr Bilddaten kommen h√§ufig Convolutional Autoencoder zum Einsatz, bei denen Faltungs- und Transponierte-Faltungsoperationen verwendet werden. Diese Architekturen sind besonders geeignet, um lokale Bildstrukturen, Kanten und Texturen effizient zu modellieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628991b-7426-4d63-8c45-f2dd8c2c9a6d",
   "metadata": {},
   "source": [
    "## 2.7 Zusammenfassung Theorie\n",
    "Zusammenfassend wurden in diesem Kapitel die wesentlichen theoretischen Grundlagen f√ºr die automatische visuelle Inspektion mittels Deep Learning erarbeitet. Dabei wurden sowohl √ºberwachte Ans√§tze der Bildklassifikation, einschlie√ülich Transfer Learning mit vortrainierten Architekturen wie ResNet, als auch un√ºberwachte Verfahren auf Basis von Autoencodern betrachtet. Die dargestellten Konzepte verdeutlichen, wie neuronale Netze Bildmerkmale hierarchisch extrahieren und f√ºr unterschiedliche Zielsetzungen nutzen k√∂nnen: entweder zur expliziten Klassifikation vordefinierter Klassen oder zur Modellierung eines Normalzustands, aus dem sich Anomalien ableiten lassen. Damit schafft dieses Kapitel die methodische Basis, um in den folgenden Kapiteln die beschriebenen Verfahren praktisch anzuwenden und hinsichtlich ihrer Eignung f√ºr die Inspektion von Tethered Caps zu bewerten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb90b6-64ec-4ac5-9808-81dd70b43fff",
   "metadata": {},
   "source": [
    "# 3. Datenimport und Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf1062-ea53-4fe4-8c66-d82a24ac2361",
   "metadata": {},
   "source": [
    "In diesem Kapitel wird der vollst√§ndige Prozess des Datenimports und der Datenvorverarbeitung beschrieben. Dazu z√§hlen das technische Setup, erste explorative Analysen, die Definition verschiedener Datensplits sowie die Vorbereitung der Bilddaten f√ºr das Training der Modelle. Ziel ist es, eine konsistente und reproduzierbare Datenbasis f√ºr die nachfolgenden Experimente zu schaffen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b554c-c455-4a52-916c-d03db91f667f",
   "metadata": {},
   "source": [
    "## 3.1 technisches Setup und Paketimporte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb76e49-ad5c-410f-b289-e5dc4c924624",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden die f√ºr die Umsetzung des Projekts erforderlichen Python-Pakete importiert. Zur Sicherstellung der Reproduzierbarkeit wird die exakte Paket- und Versionskonfiguration nicht im Notebook selbst, sondern zentral in einer separaten `requirements.txt` dokumentiert. Zus√§tzlich werden feste Zufallsstarts (Seeds) f√ºr Python, NumPy und PyTorch gesetzt, um ein deterministisches Trainings- und Evaluationsverhalten zu gew√§hrleisten. Abschlie√üend erfolgt die automatische Auswahl des Rechenger√§ts, sodass sofern verf√ºgbar eine GPU genutzt wird, andernfalls die Berechnung auf der CPU erfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005bfc25-de5f-46d4-82a0-f292b26f03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchinfo ist auf dem Cluster nicht vorinstalliert\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82507b-9fea-4f1a-b9f6-1ff97caee9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchcam ist auf dem Cluster nicht vorinstalliert\n",
    "!pip install torchcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be2a02-d080-4ea3-9b7e-8a4b18f34efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardbibliotheken\n",
    "import os                          # Arbeiten mit Dateipfaden und Ordnerstrukturen\n",
    "import math                        # Mathematische Hilfsfunktionen\n",
    "import random                      # Zufallsfunktionen (z. B. Shuffling, Seeds)\n",
    "import hashlib                     # Erzeugen stabiler Hashes (z. B. f√ºr Caching)\n",
    "from collections import Counter    # Z√§hlen von Elementen (z. B. Klassenh√§ufigkeiten)\n",
    "import time                        # Zeitmessung (z. B. Laufzeit pro Epoche, Inferenz)\n",
    "\n",
    "# Numerische Berechnungen\n",
    "import numpy as np                 # Numerische Operationen (Arrays, Statistiken)\n",
    "\n",
    "# Bildverarbeitung & Visualisierung\n",
    "from PIL import Image              # √ñffnen und Verarbeiten von Bilddateien\n",
    "import matplotlib.pyplot as plt    # Darstellung von Bildern, Kurven und Diagrammen\n",
    "\n",
    "# Klassisches Machine Learning / Datenaufteilung\n",
    "from sklearn.model_selection import train_test_split  # Aufteilung in Train / Val / Test\n",
    "\n",
    "# Deep Learning ‚Äì PyTorch (Core)\n",
    "import torch                       # Zentrales Deep-Learning-Framework\n",
    "import torch.nn as nn              # Neuronale Netzwerk-Bausteine (Layer, Losses)\n",
    "from torch.utils.data import Dataset, DataLoader  # Dataset- und DataLoader-Basisklassen\n",
    "\n",
    "# Deep Learning ‚Äì TorchVision\n",
    "from torchvision import transforms # Bildtransformationen (Augmentation, Normalisierung)\n",
    "import torchvision.transforms.functional as TF  # Funktionale Bildoperationen (z. B. Padding)\n",
    "from torchvision import models     # Vorgefertigte Modelle (z. B. ResNet)\n",
    "\n",
    "# Modellanalyse & Debugging\n",
    "from torchinfo import summary      # Modell√ºbersicht (Layer, Parameter, Shapes)\n",
    "\n",
    "# Explainable AI / Modellinterpretierbarkeit\n",
    "from torchcam.methods import GradCAM   # Grad-CAM zur Visualisierung relevanter Bildbereiche\n",
    "from torchcam.utils import overlay_mask # √úberlagerung der Heatmap auf das Originalbild\n",
    "\n",
    "# Hilfsfunktionen f√ºr Bilddarstellung\n",
    "from torchvision.transforms.functional import to_pil_image   # Konvertierung von Tensoren (z. B. [C,H,W]) zur√ºck in PIL-Bilder (f√ºr GradCAM)\n",
    "\n",
    "# Notebook-Ausgabe & Training-Utilities\n",
    "from IPython.display import display, Markdown  # Formatierte Textausgaben im Notebook\n",
    "from tqdm.notebook import tqdm     # Fortschrittsbalken f√ºr Trainings- und Evaluationsschleifen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ffd69-9e33-4bdb-b8d6-9fae5d74ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch (CPU)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# PyTorch (GPU)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Deterministisches Verhalten erzwingen\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87dbab-9d41-4f04-a2e8-4ee9f1cd5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur automatischen Auswahl der GPU mit dem meisten freien Speicher\n",
    "# sinnvoll bei Nutzung eines Clusters mit mehreren GPUs, um Ressourcen optimal zu verwenden\n",
    "def pick_most_free_gpu():\n",
    "    # Anzahl verf√ºgbarer CUDA-GPUs ermitteln\n",
    "    n = torch.cuda.device_count()\n",
    "    \n",
    "    # Sicherstellen, dass mindestens eine GPU vorhanden ist\n",
    "    assert n > 0, \"Keine CUDA-GPU gefunden\"\n",
    "    \n",
    "    # Liste zum Speichern von (GPU-Index, freier Speicher in Bytes)\n",
    "    free = []\n",
    "    \n",
    "    # F√ºr jede GPU den aktuell freien Speicher abfragen\n",
    "    for i in range(n):\n",
    "        f, t = torch.cuda.mem_get_info(i)  # f = freier Speicher in Bytes, t = Gesamtspeicher\n",
    "        free.append((i, f))                # Tupel aus Index und freiem Speicher speichern\n",
    "    \n",
    "    # GPU mit dem meisten freien Speicher ausw√§hlen\n",
    "    best = max(free, key=lambda x: x[1])[0]\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952fde3-cb0a-46ad-91a5-4ed4e6fdd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste GPU anhand des freien Speichers bestimmen\n",
    "best_gpu = pick_most_free_gpu()\n",
    "\n",
    "# Diese GPU als aktuelles CUDA-Device setzen\n",
    "torch.cuda.set_device(best_gpu)\n",
    "\n",
    "# torch.device-Objekt f√ºr Training/Inference erzeugen\n",
    "device = torch.device(f\"cuda:{best_gpu}\")\n",
    "\n",
    "# Zur Kontrolle Name der verwendeten GPU ausgeben\n",
    "print(\"Nutze Device:\", device, torch.cuda.get_device_name(best_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370a0dd-c007-4a69-9f50-3dca2d6af0af",
   "metadata": {},
   "source": [
    "## 3.2 Datenimport und erste explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ca8cb-91f2-4712-b134-aa37135397c7",
   "metadata": {},
   "source": [
    "Die Bilddaten wurden uns von der Inspektionstechnik der KHS GmbH f√ºr Forschungszwecke zur Verf√ºgung gestellt. Die Aufnahmen stammen aus realen Pr√ºfprozessen und wurden nicht im Rahmen dieser Arbeit selbst erzeugt.\n",
    "\n",
    "Der Datensatz ist in mehreren Ebenen strukturiert. Auf der obersten Ebene befinden sich Ordner, die unterschiedliche Verschlussvarianten repr√§sentieren, beispielsweise aquapanna_grey, cc_red, fanta_orange oder sprite_green. Jeder dieser Ordner enth√§lt zwei weitere Unterordner, welche die Klassenzuordnung der Bilder abbilden: *good* und *bad*.\n",
    "\n",
    "Innerhalb dieser Ordner liegen die eigentlichen Bilddateien. F√ºr jedes untersuchte Exemplar existieren vier Aufnahmen, die aus verschiedenen Kameraperspektiven erzeugt wurden. Die Dateinamen Cam1Side_xxxxxxxxxx, Cam1Top_xxxxxxxxxx, Cam2Side_xxxxxxxxxx und Cam2Top_xxxxxxxxxx kennzeichnen jeweils Kamera, Perspektive und eine zehnstellige fortlaufende Nummer. Durch die Kombination dieser vier Perspektiven wird gew√§hrleistet, dass der Verschluss aus allen relevanten Blickwinkeln erfasst wird und damit eine vollst√§ndige 360-Grad-Betrachtung m√∂glich ist.\n",
    "\n",
    "Da es sich um reale Produktionsdaten handelt, kann es vorkommen, dass f√ºr dieselbe Flasche nicht alle vier Bilder in derselben Klasse liegen. So kann beispielsweise eine Perspektive einen Fehler zeigen und deshalb als bad vorliegen, w√§hrend die √ºbrigen Perspektiven derselben Flasche als good klassifiziert sind. In der realen Produktionslinie f√ºhrt bereits ein einzelnes fehlerhaftes Bild dazu, dass die entsprechende Flasche ausgeschleust wird. F√ºr das Training und die Auswertung der Deep-Learning-Modelle werden die Bilder in der vorliegenden Struktur verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322ec5d-92fa-4475-ae90-9b35f791e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Ordner, in dem die bereitgestellten Bilddaten abgelegt sind\n",
    "pfad = r\"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bd8ee-d5ad-4be4-868f-443e42d5da82",
   "metadata": {},
   "source": [
    "In der klassischen Bildklassifikation liegt ein Datensatz √ºblicherweise in einer einfachen Ordnerstruktur vor, bei der ein Hauptordner jeweils einen Unterordner pro Klasse enth√§lt (z. B. good und bad). In diesem Anwendungsfall ist die Struktur komplexer, da die Bilder zun√§chst nach Verschlussvarianten organisiert sind und erst innerhalb dieser Ordner eine Unterteilung in *good* und *bad* erfolgt. Um die Daten dennoch flexibel verarbeiten und bei Bedarf problemlos weitere Varianten erg√§nzen zu k√∂nnen, werden im folgenden Schritt alle Bildpfade rekursiv durchsucht und in einer Liste gespeichert. Gleichzeitig wird die Anzahl der Bilder pro Klasse ermittelt, um einen ersten √úberblick √ºber die Klassenverteilung zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d2332-504a-4f05-a2cd-1f514ad122a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter-Objekt erzeugen\n",
    "# speichert, wie viele Bilder es insgesamt in den Klassen \"good\" & \"bad\" gibt\n",
    "class_count = Counter()\n",
    "# Dictionary, dass f√ºr jede Verschlussart die Anzahl der good- und bad-Beispiele speichert\n",
    "variant_stats = {}\n",
    "# speichert eine Liste mit allen vollst√§ndigen Dateipfaden\n",
    "all_image_paths = []                 \n",
    "\n",
    "# os.walk(pfad) --> l√§uft rekursiv durch alle Unterordner unterhalb von pfad\n",
    "# root --> aktueller Ordnerpfad\n",
    "# dirs --> Unterordner in diesem Ordner\n",
    "# files --> Dateien in diesem Ordner\n",
    "for root, dirs, files in os.walk(pfad):\n",
    "    folder = os.path.basename(root).lower() #folder --> Ordnername ohne kompletten Pfad\n",
    "\n",
    "    # Wir suchen Ordner \"good\" oder \"bad\"\n",
    "    if folder in [\"good\", \"bad\"]:\n",
    "        label = folder\n",
    "\n",
    "        # Verschlussart (-->variant) ist der √ºbergeordnete Ordnername\n",
    "        variant = os.path.basename(os.path.dirname(root)).lower()\n",
    "\n",
    "        # Falls die Verschlussart noch nicht im Dictionary vorhanden ist wird diese initialisiert\n",
    "        if variant not in variant_stats:\n",
    "            variant_stats[variant] = {\"good\": 0, \"bad\": 0}\n",
    "\n",
    "        # alle Bilddateien im Ordner durchgehen\n",
    "        for f in files:\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "\n",
    "                # Gesamtstatistik berechnen\n",
    "                class_count[label] += 1\n",
    "\n",
    "                # Statistik f√ºr diese Verschlussart berechnen\n",
    "                variant_stats[variant][label] += 1\n",
    "\n",
    "                # vollst√§ndigen Pfad speichern\n",
    "                full_path = os.path.join(root, f)\n",
    "                all_image_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493e808-82d4-428c-891c-d24673bd6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Gesamtstatistik\n",
    "print(\"Klassenverteilung (gesamt):\\n\")\n",
    "print(f\"Gesamtanzahl Bilder: {len(all_image_paths)}\\n\")\n",
    "print(class_count, \"\\n\")\n",
    "\n",
    "# Anzahl der Verschlussvarianten ausgeben\n",
    "num_variants = len(variant_stats)\n",
    "print(f\"Anzahl Verschlussvarianten: {num_variants}\\n\")\n",
    "\n",
    "# Dictionary erstellen, was die Statistiken zu den verschiedenen Verschlussvarianten geb√ºndelt sammelt\n",
    "variant_summary = {}\n",
    "\n",
    "for variant, stats in variant_stats.items():\n",
    "    good = stats[\"good\"]\n",
    "    bad = stats[\"bad\"]\n",
    "    total = good + bad\n",
    "    bad_rate = (bad / total) * 100 if total > 0 else 0\n",
    "\n",
    "    variant_summary[variant] = {\n",
    "        \"good\": good,\n",
    "        \"bad\": bad,\n",
    "        \"total\": total,\n",
    "        \"bad_rate\": bad_rate\n",
    "    }\n",
    "\n",
    "# Ausgabe Statistik je Verschlussart \n",
    "print(\"Verteilung je Verschlussvariante:\\n\")\n",
    "for variant, s in sorted(variant_summary.items()):\n",
    "    print(\n",
    "        f\"{variant:20s}  \"\n",
    "        f\"good: {s['good']:5d}   \"\n",
    "        f\"bad: {s['bad']:5d}   \"\n",
    "        f\"gesamt: {s['total']:5d}   \"\n",
    "        f\"bad-Rate: {s['bad_rate']:5.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9dc67-c05e-425a-ba9b-966b88a49d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten vorbereiten zur Visualisierung der Anzahl von good/bad pro Verschlussvariante\n",
    "variants = list(variant_stats.keys())\n",
    "good_counts = [variant_stats[v][\"good\"] for v in variants]\n",
    "bad_counts  = [variant_stats[v][\"bad\"] for v in variants]\n",
    "\n",
    "x = np.arange(len(variants))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(x - width/2, good_counts, width, label=\"good\", color=\"green\", alpha=0.5)\n",
    "plt.bar(x + width/2, bad_counts,  width, label=\"bad\", color=\"red\", alpha=0.5)\n",
    "plt.xticks(x, variants, rotation=50, ha=\"right\")\n",
    "plt.ylabel(\"Anzahl Bilder\")\n",
    "plt.title(\"Anzahl good/bad pro Verschlussvariante\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87979700-b175-4c8d-ab13-dfdf16614d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der bad-Rate pro Verschlussvariante\n",
    "bad_rates = [variant_summary[v][\"bad_rate\"] for v in variants]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(variants, bad_rates, color=\"orange\", alpha=0.8)\n",
    "plt.axhline(y=50, linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "plt.text(x=len(variants)-0.5,y=50.5, s=\"50% Schwelle\", ha=\"right\", va=\"bottom\", fontsize=9, alpha=0.8)\n",
    "\n",
    "plt.xticks(rotation=50, ha=\"right\")\n",
    "plt.ylabel(\"bad-Rate (%)\")\n",
    "plt.title(\"bad-Rate pro Verschlussvariante\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09500f42-380c-4054-93db-e22fd1117e23",
   "metadata": {},
   "source": [
    "Die Bilddateien wurden rekursiv eingelesen und die vollst√§ndigen Dateipfade in einer Liste (all_image_paths) gespeichert. Dies erm√∂glicht eine flexible Weiterverarbeitung der Daten, da die Bildpfade sp√§ter direkt f√ºr das Erstellen eigener Dataset-Klassen, f√ºr den Train-/Validierungs-Split sowie f√ºr Visualisierungen und Qualit√§tsanalysen genutzt werden k√∂nnen.\n",
    "\n",
    "Zus√§tzlich wurde ein erster quantitativer √úberblick √ºber den Datensatz erstellt. Insgesamt liegen 32 179 Bilder vor, davon 17 188 ‚Äûgood‚Äú und 14 991 ‚Äûbad‚Äú. Damit ist die Gesamtverteilung der Klassen relativ ausgewogen, auch wenn die Anzahl der good-Bilder leicht √ºberwiegt.\n",
    "\n",
    "Dar√ºber hinaus wurde die Verteilung je Verschlussvariante analysiert, um Unterschiede in der Datenmenge und im Verh√§ltnis von fehlerhaften zu fehlerfreien Beispielen sichtbar zu machen. Die Auswertung zeigt, dass die Anzahl der Bilder pro Verschlussvariante teils stark variiert: die geringste Gesamtanzahl weist kcl_pink mit 832 Bildern auf, w√§hrend gg_darkblue mit 3602 Bildern die am h√§ufigsten vertretene Variante ist. Ebenso unterscheiden sich die bad-Raten je Variante erheblich. Einige Varianten weisen eine sehr hohe Fehlerrate auf, beispielsweise flirt_orange mit 72.39 % oder cc_lightgray mit 63.78 %. Andere Varianten zeigen vergleichsweise niedrige Fehlerraten, etwa gg_darkblue mit 28.96 %, cc_red mit 32.59 % oder fanta_orange mit 32.77 %.\n",
    "\n",
    "Diese Unterschiede sind f√ºr die sp√§tere Modellbewertung bedeutsam, da ungleich verteilte Varianten und stark variierende bad-Raten das Lernverhalten der Modelle beeinflussen k√∂nnen. Varianten mit wenigen Beispielen oder hoher Fehlerrate k√∂nnen insbesondere zu verzerrten Ergebnissen oder abweichender Modellperformance f√ºhren.\n",
    "\n",
    "Um die Bilder f√ºr die weitere Verarbeitung und das sp√§tere Training der Deep-Learning-Modelle vorzubereiten, ist es zun√§chst notwendig zu pr√ºfen, ob die gelieferten Aufnahmen einheitliche oder unterschiedlich gro√üe Bildformate aufweisen. Abweichende Aufl√∂sungen k√∂nnen sich auf das Preprocessing auswirken und m√ºssen gegebenenfalls durch ein einheitliches Resize-Verfahren harmonisiert werden. Daher wird im n√§chsten Schritt die Verteilung der Bildgr√∂√üen analysiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ffb9f-0d1b-4c84-895f-471b92dc9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste zur Speicherung der Bildaufl√∂sungen\n",
    "# Es wird jeweils ein Tupel (Breite, H√∂he) abgelegt\n",
    "sizes = []\n",
    "\n",
    "# √úber alle Bildpfade iterieren und Bildgr√∂√üe auslesen\n",
    "for img_path in all_image_paths:\n",
    "    try:\n",
    "        # Bild √∂ffnen\n",
    "        with Image.open(img_path) as img:\n",
    "            sizes.append(img.size)   # img.size = (width, height)\n",
    "    except:\n",
    "        # Falls ein Bild nicht gelesen werden kann wird es √ºbersprungen, um die Analyse nicht abzubrechen\n",
    "        pass\n",
    "\n",
    "# Anzahl der erfolgreich analysierten Bilder\n",
    "print(f\"Anzahl analysierter Bilder: {len(sizes)}\")\n",
    "\n",
    "# Z√§hlen, wie oft jede Aufl√∂sung im Datensatz vorkommt\n",
    "size_count = Counter(sizes)\n",
    "\n",
    "# Ausgabe der vorhandenen Bildgr√∂√üen und ihrer H√§ufigkeit\n",
    "print(\"\\nVorhandene Bildgr√∂√üen (Breite √ó H√∂he):\")\n",
    "for (w, h), count in size_count.items():\n",
    "    print(f\"{w} x {h}  -> {count} Bilder\")\n",
    "\n",
    "# Separate Listen f√ºr Breiten und H√∂hen der Bilder erzeugen,\n",
    "# um statistische Kennzahlen berechnen zu k√∂nnen\n",
    "widths  = [w for w, h in sizes]\n",
    "heights = [h for w, h in sizes]\n",
    "\n",
    "# Ausgabe der minimalen und maximalen Bildabmessungen im Datensatz\n",
    "print(\"\\nStatistik:\")\n",
    "print(f\"Min Breite : {min(widths)}\")\n",
    "print(f\"Max Breite : {max(widths)}\")\n",
    "print(f\"Min H√∂he   : {min(heights)}\")\n",
    "print(f\"Max H√∂he   : {max(heights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025481a-0d92-4c79-81ee-5f8ae6fbb892",
   "metadata": {},
   "source": [
    "Die Analyse der Bildaufl√∂sungen zeigt, dass der Datensatz aus insgesamt 32 179 Bildern besteht, die in 17 unterschiedlichen Gr√∂√üen vorliegen. Die Breite der Bilder variiert dabei zwischen ca. 334 und 366 Pixeln, w√§hrend die H√∂he zwischen 108 und 131 Pixeln liegt. Obwohl die Aufl√∂sungen leicht voneinander abweichen, bewegen sie sich in einem relativ engen Gr√∂√üenbereich.\n",
    "\n",
    "F√ºr die weitere Modellierung bedeutet dies, dass ein einheitliches Resize-Verfahren erforderlich ist, um alle Bilder auf dieselbe Eingabegr√∂√üe zu bringen. Die geringe Varianz der Originalgr√∂√üen erleichtert diesen Schritt, da dabei nur minimale Verzerrungen oder Informationsverluste zu erwarten sind.\n",
    "\n",
    "Um einen ersten visuellen Eindruck vom Datensatz zu gewinnen, werden im Folgenden repr√§sentative Beispielbilder dargestellt.\n",
    "Zun√§chst werden zuf√§llig ausgew√§hlte good-Bilder gezeigt. Die Auswahl ist gleichm√§√üig √ºber alle Verschlussvarianten verteilt, wodurch sich ein abwechslungsreicher √úberblick ergibt. Dadurch wird sichtbar, wie unterschiedlich die Deckel hinsichtlich Farbe, Beleuchtung und Oberfl√§chenstruktur aussehen k√∂nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852f561-8be5-4e60-89d6-88fdcf4af017",
   "metadata": {},
   "source": [
    "Neben den fehlerfreien Beispielen ist auch ein Blick auf die Bad-Bilder wichtig, da diese die relevanten Defektarten enthalten, die ein Modell sp√§ter erkennen soll. Daher folgt im n√§chsten Schritt eine Auswahl typischer Bad-Beispiele, die ebenfalls gleichm√§√üig √ºber alle Verschlussvarianten verteilt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92b5b0-855e-4876-9f90-4c8ff6c6871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswahl von 16 gleichm√§√üig √ºber den Datensatz verteilten Good-Bildern\n",
    "good_paths = [p for p in all_image_paths if \"/good/\" in p.lower()]\n",
    "\n",
    "n = 16\n",
    "step = len(good_paths) / n\n",
    "idx = [math.floor(i * step) for i in range(n)]\n",
    "selected_paths = [good_paths[i] for i in idx]\n",
    "\n",
    "# 4x4 Grid zur Darstellung exemplarischer Good-Bilder\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 4))\n",
    "\n",
    "for ax, img_path in zip(axs.flatten(), selected_paths):\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Verschlussvariante aus dem Pfad\n",
    "    variant = img_path.split(\"/\")[-3]\n",
    "    ax.set_title(variant, fontsize=9)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Abst√§nde reduzieren f√ºr ein kompaktes Layout\n",
    "plt.subplots_adjust(\n",
    "    left=0.01, right=0.99, top=0.95, bottom=0.01,\n",
    "    wspace=0.05, hspace=0.28\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a5788-c759-4165-ae1b-431a22ec629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 gleichm√§√üig verteilte Bad-Bilder ausw√§hlen\n",
    "bad_paths = [p for p in all_image_paths if \"/bad/\" in p.lower()]\n",
    "\n",
    "n = 16\n",
    "step = len(bad_paths) / n\n",
    "idx = [math.floor(i * step) for i in range(n)]\n",
    "selected_paths = [bad_paths[i] for i in idx]\n",
    "\n",
    "# 4x4 Grid zur Darstellung exemplarischer Good-Bilder\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 4))\n",
    "\n",
    "for ax, img_path in zip(axs.flatten(), selected_paths):\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    variant = img_path.split(\"/\")[-3]\n",
    "    ax.set_title(variant, fontsize=9)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Abst√§nde reduzieren f√ºr ein kompaktes Layout\n",
    "plt.subplots_adjust(\n",
    "    left=0.01, right=0.99, top=0.95, bottom=0.01,\n",
    "    wspace=0.05, hspace=0.28\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89f8a1-92e2-4f9f-a9a7-68b3279e1fd4",
   "metadata": {},
   "source": [
    "Die Bildbeispiele verdeutlichen sowohl die gro√üe Farb- und Variantenvielfalt der Verschl√ºsse als auch die Variabilit√§t innerhalb der Kameraaufnahmen (z. B. Helligkeit, Kontrast, Position des Deckels).\n",
    "Die good-Bilder zeigen zudem, dass selbst fehlerfreie Tethered-Caps-Verschl√ºsse eine deutlich komplexere Geometrie aufweisen als herk√∂mmliche Schraubverschl√ºsse. W√§hrend klassische Deckel aus allen Blickrichtungen nahezu identisch aussehen und sich daher gut √ºber reine Pixelbasismethoden pr√ºfen lie√üen, besitzen Tethered Caps bewegliche Stege und asymmetrische Formen. Dadurch entsteht eine nat√ºrliche strukturelle Variabilit√§t, die f√ºr die Inspektion eine zus√§tzliche Herausforderung darstellt.\n",
    "\n",
    "Die bad-Bilder zeigen typische Defekte wie Risse, gebrochene Stege, Verbiegungen (‚ÄûSmile‚Äú) oder geometrische Deformationen. Diese visuelle Heterogenit√§t best√§tigt, dass f√ºr die sp√§tere Klassifikation ein robustes Modell erforderlich ist, das sowohl unterschiedliche Verschlussvarianten als auch die variierenden Aufnahmebedingungen zuverl√§ssig bew√§ltigen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e49c0d-8a90-4f76-89f3-c4e8b47d7cf9",
   "metadata": {},
   "source": [
    "## 3.3 Datensplits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2842f-eb7a-4fb1-83da-9bcb72064443",
   "metadata": {},
   "source": [
    "Ziel dieses Kapitels ist die strukturierte Aufteilung des Gesamtdatensatzes in Trainings-, Validierungs- und Testdaten. Neben einem klassischen zuf√§lligen Datensplit (70 % / 15 % / 15 %) werden variantenspezifische Leave-One-Variant-Out-Splits eingesetzt, bei denen jeweils eine vollst√§ndige Verschlussvariante ausschlie√ülich zur Evaluation genutzt wird. Dieser Ansatz tr√§gt der Anforderung realer Anwendungsszenarien Rechnung, in denen ein robustes Inspektionssystem auch auf bislang unbekannte Flaschen- und Verschlussvarianten generalisieren muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8ea58-55ef-4aa7-b35e-394560b95333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels und Indizes vorbereiten\n",
    "\n",
    "# Indizes aller Bilder (entspricht der Reihenfolge in all_image_paths)\n",
    "indices = list(range(len(all_image_paths)))\n",
    "\n",
    "# Label-Liste: 0 = good, 1 = bad\n",
    "labels = []\n",
    "\n",
    "# Zuordnung der Labels basierend auf dem Pfad\n",
    "for p in all_image_paths:\n",
    "    if p in good_paths:\n",
    "        labels.append(0) # good\n",
    "    else:\n",
    "        labels.append(1) # bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb29e6-5511-4e41-9ab3-f84393a9611f",
   "metadata": {},
   "source": [
    "### 3.3.1 Standard Split (70/15/15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235538c2-cbde-406d-ba2f-90b64768438a",
   "metadata": {},
   "source": [
    "Im ersten Schritt wird auf Basis der vorbereiteten Bildindizes ein klassischer, zuf√§lliger Datensplit in Trainings-, Validierungs- und Testdaten erzeugt. Der Gesamtdatensatz wird dabei im Verh√§ltnis 70 % Training, 15 % Validation und 15 % Test aufgeteilt.\n",
    "\n",
    "Um eine faire und vergleichbare Modellbewertung zu gew√§hrleisten, wird bei allen Teilmengen auf eine konstante Klassenverteilung geachtet. Der relative Anteil von good- und bad-Bildern bleibt somit in Trainings-, Validierungs- und Testdatensatz m√∂glichst identisch.\n",
    "\n",
    "Die Aufteilung erfolgt in zwei aufeinanderfolgenden Schritten unter Verwendung einer stratifizierten Zufallsauswahl, wodurch sowohl Reproduzierbarkeit als auch eine ausgewogene Klassenverteilung sichergestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69159234-1e72-41d7-a8e1-40b3cb4a739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-stufige Aufteilung in Train / Validation / Test,\n",
    "# da train_test_split jeweils nur zwei Teilmengen erzeugt\n",
    "\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.30,      # 70 % Training, 30 % Rest\n",
    "    stratify=labels,     # √§hnliche Klassenverteilung in allen Splits\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253221b9-4d8b-4786-a81d-b4b38fea1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-Liste f√ºr den tempor√§ren Split erzeugen,\n",
    "# da stratify nur Labels der aktuell betrachteten Teilmenge akzeptiert\n",
    "temp_labels = [labels[i] for i in temp_indices]\n",
    "\n",
    "val_indices, test_indices = train_test_split(\n",
    "    temp_indices,\n",
    "    test_size=0.50,      # 50 % Validation, 50 % Test\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c98e0c-dbac-470c-9855-97e42c709715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bildpfade f√ºr Train-, Validation- und Test-Split erzeugen\n",
    "train_paths = [all_image_paths[i] for i in train_indices]\n",
    "val_paths   = [all_image_paths[i] for i in val_indices]\n",
    "test_paths  = [all_image_paths[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6fa05-3dd3-4369-9d66-3d91c9c4f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassenverteilung innerhalb eines Splits z√§hlen\n",
    "def count_labels(paths):\n",
    "    labels = []\n",
    "    for p in paths:\n",
    "        if \"/good/\" in p.lower():\n",
    "            labels.append(\"good\")\n",
    "        else:\n",
    "            labels.append(\"bad\")\n",
    "    return Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e0c55-cc9e-4333-b772-689c4a2d624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gesamt:\", len(all_image_paths))\n",
    "print(\"Train :\", len(train_paths), count_labels(train_paths))\n",
    "print(\"Val   :\", len(val_paths), count_labels(val_paths))\n",
    "print(\"Test  :\", len(test_paths),  count_labels(test_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11072a60-5972-4f8b-8740-97830efd93b5",
   "metadata": {},
   "source": [
    "Die Ausgabe best√§tigt, dass der Datensatz erfolgreich in Trainings-, Validierungs- und Testdaten aufgeteilt wurde.  \n",
    "Die angestrebte Aufteilung von 70 % Training, 15 % Validation und 15 % Test wird eingehalten.\n",
    "\n",
    "Zudem zeigt die Klassenverteilung, dass der relative Anteil von *good*- und *bad*-Bildern in allen drei Splits nahezu identisch ist.  \n",
    "Damit ist sichergestellt, dass die nachfolgenden Trainings- und Evaluationsschritte auf vergleichbaren Daten basieren und keine systematischen Verzerrungen durch eine unausgewogene Klassenverteilung entstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97abbeec-a113-4816-9b0f-5bb40c0d5f75",
   "metadata": {},
   "source": [
    "### 3.3.2 Variant-Based Split (Leave-One-Variant-Out) - Variante: cc_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8dcef-bb4c-4b1d-914e-e2e755af35e3",
   "metadata": {},
   "source": [
    "Aufbauend auf dem zuvor beschriebenen Standardsplit wird im n√§chsten Schritt ein variantenspezifischer Leave-One-Variant-Out-Split implementiert.  \n",
    "\n",
    "F√ºr diesen ersten Hold-out-Split wird die Variante **cc_red** ausgew√§hlt. F√ºr cc_red liegt insgesamt eine vergleichsweise gro√üe Anzahl an Bilddaten vor, wobei der Anteil fehlerfreier (good) Bilder deutlich √ºberwiegt. Diese Konstellation entspricht einem realistischen industriellen Anwendungsszenario, in dem fehlerfreie Produkte dominieren und Defekte vergleichsweise selten auftreten. Die Variante cc_red eignet sich daher besonders zur Bewertung, wie gut das Modell auf eine bislang unbekannte, aber praxisnahe Verschlussvariante generalisiert.\n",
    "\n",
    "Alle Bilder der Variante cc_red werden vollst√§ndig dem Testdatensatz zugeordnet und w√§hrend des Trainings nicht verwendet. Die verbleibenden Bilddaten aller anderen Verschlussvarianten dienen als Trainings- und Validierungsdaten und werden im Verh√§ltnis 85 % Training und 15 % Validation aufgeteilt. Die Aufteilung erfolgt unter Verwendung einer stratifizierten Zufallsauswahl, sodass die Klassenverteilung von good- und bad-Bildern in Trainings- und Validierungsdaten m√∂glichst konstant bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a65d7-1daa-4461-90cf-a7f23d49f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschlussvariante ausw√§hlen, die ausschlie√ülich zum testen verwendet wird\n",
    "holdout_variant_1 = \"cc_red\"\n",
    "\n",
    "# alle Bilder der Hold-out Variante --> werden zum Testen verwendet\n",
    "# diese Variante wird komplett aus dem Training entfernt\n",
    "test_paths_lovo_ccred = [p for p in all_image_paths if f\"/{holdout_variant_1}/\" in p.lower()]\n",
    "\n",
    "# alle anderen Varianten (ohne cc_red)\n",
    "trainval_paths_lovo_ccred = [p for p in all_image_paths if f\"/{holdout_variant_1}/\" not in p.lower()]\n",
    "\n",
    "# Liste mit Labels f√ºr die Trainingsdaten erzeugen \n",
    "# 0 --> good; 1 --> bad\n",
    "# f√ºr stratify by Split\n",
    "labels_trainval_ccred = []\n",
    "for p in trainval_paths_lovo_ccred:\n",
    "    if \"/good/\" in p.lower():\n",
    "        labels_trainval_ccred.append(0)  # good\n",
    "    else:\n",
    "        labels_trainval_ccred.append(1)  # bad\n",
    "\n",
    "# Indizes f√ºr die Trainingsdaten\n",
    "indices_trainval_ccred = list(range(len(trainval_paths_lovo_ccred)))\n",
    "\n",
    "# Aufteilung der Trainingsdaten in Trainings- und Validierungsdaten (85/15)\n",
    "train_idx_lovo_ccred, val_idx_lovo_ccred = train_test_split(\n",
    "    indices_trainval_ccred,\n",
    "    test_size=0.15,           # 15 % Validation\n",
    "    stratify=labels_trainval_ccred, # √§hnliche Klassenverteilung in Train/Val\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pfadlisten f√ºr Trainings- und Validierungs-Pfade erzeugen\n",
    "train_paths_lovo_ccred = [trainval_paths_lovo_ccred[i] for i in train_idx_lovo_ccred]\n",
    "val_paths_lovo_ccred   = [trainval_paths_lovo_ccred[i] for i in val_idx_lovo_ccred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a061b-0948-4a46-9899-63bed0d9eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Leave-One-Variant-Out Split\")\n",
    "print(\"Holdout Variante:\", holdout_variant_1)\n",
    "print()\n",
    "\n",
    "print(\"Gesamt:\", len(all_image_paths))\n",
    "print(\"Train :\", len(train_paths_lovo_ccred), count_labels(train_paths_lovo_ccred))\n",
    "print(\"Val   :\", len(val_paths_lovo_ccred),   count_labels(val_paths_lovo_ccred))\n",
    "print(\"Test  :\", len(test_paths_lovo_ccred),  count_labels(test_paths_lovo_ccred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827414ae-d8e4-4b94-b276-9e666fc96bdf",
   "metadata": {},
   "source": [
    "### 3.3.3 Variant-Based Split (Leave-One-Variant-Out) - Variante: voesl_zitrone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ef99f-7bb0-4854-bf02-17ce89a373a0",
   "metadata": {},
   "source": [
    "Erg√§nzend zum zuvor beschriebenen Leave-One-Variant-Out-Split mit der Variante cc_red wird im Folgenden ein zweiter variantenspezifischer Split f√ºr die Variante **voesl_zitrone** umgesetzt. Diese Variante weist ebenfalls eine hohe Anzahl an Bilddaten auf, unterscheidet sich jedoch durch eine insgesamt ausgeglichenere Klassenverteilung mit einem leicht erh√∂hten Anteil an bad-Bildern. Dadurch eignet sie sich besonders zur Analyse der Modellleistung unter ver√§nderten Klassenverh√§ltnissen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85dc665-f18a-4096-b40f-df7661559801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschlussvariante ausw√§hlen, die ausschlie√ülich zum testen verwendet wird\n",
    "holdout_variant_2 = \"voesl_zitrone\"\n",
    "\n",
    "# alle Bilder der Hold-out Variante --> werden zum Testen verwendet\n",
    "# diese Variante wird komplett aus dem Training entfernt\n",
    "test_paths_lovo_voesl_zitrone = [p for p in all_image_paths if f\"/{holdout_variant_2}/\" in p.lower()]\n",
    "\n",
    "# alle anderen Varianten (ohne voesl_zitrone)\n",
    "trainval_paths_lovo_voesl_zitrone = [p for p in all_image_paths if f\"/{holdout_variant_2}/\" not in p.lower()]\n",
    "\n",
    "# Liste mit Labels f√ºr die Trainingsdaten erzeugen \n",
    "# 0 --> good; 1 --> bad\n",
    "# f√ºr stratify by Split\n",
    "labels_trainval_voesl_zitrone = []\n",
    "for p in trainval_paths_lovo_voesl_zitrone:\n",
    "    if \"/good/\" in p.lower():\n",
    "        labels_trainval_voesl_zitrone.append(0)  # good\n",
    "    else:\n",
    "        labels_trainval_voesl_zitrone.append(1)  # bad\n",
    "\n",
    "# Indizes f√ºr die Trainingsdaten\n",
    "indices_trainval_voesl_zitrone = list(range(len(trainval_paths_lovo_voesl_zitrone)))\n",
    "\n",
    "# Aufteilung der Trainingsdaten in Trainings- und Validierungsdaten (85/15)\n",
    "train_idx_lovo_voesl_zitrone, val_idx_lovo_voesl_zitrone = train_test_split(\n",
    "    indices_trainval_voesl_zitrone,\n",
    "    test_size=0.15,           # 15 % Validation\n",
    "    stratify=labels_trainval_voesl_zitrone, # √§hnliche Klassenverteilung in Train/Val\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pfadlisten f√ºr Trainings- und Validierungs-Pfade erzeugen\n",
    "train_paths_lovo_voesl_zitrone = [trainval_paths_lovo_voesl_zitrone[i] for i in train_idx_lovo_voesl_zitrone]\n",
    "val_paths_lovo_voesl_zitrone  = [trainval_paths_lovo_voesl_zitrone[i] for i in val_idx_lovo_voesl_zitrone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12553c0d-7dd4-4923-bf95-a0e96bfeaabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Leave-One-Variant-Out Split\")\n",
    "print(\"Holdout Variante:\", holdout_variant_2)\n",
    "print()\n",
    "\n",
    "print(\"Gesamt:\", len(all_image_paths))\n",
    "print(\"Train :\", len(train_paths_lovo_voesl_zitrone), count_labels(train_paths_lovo_voesl_zitrone))\n",
    "print(\"Val   :\", len(val_paths_lovo_voesl_zitrone),   count_labels(val_paths_lovo_voesl_zitrone))\n",
    "print(\"Test  :\", len(test_paths_lovo_voesl_zitrone),  count_labels(test_paths_lovo_voesl_zitrone))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc113b1-3c76-48bd-aa16-e1e37bbf522d",
   "metadata": {},
   "source": [
    "In diesem Kapitel wurden unterschiedliche Strategien zur Aufteilung des Bilddatensatzes in Trainings-, Validierungs- und Testdaten vorgestellt. Neben einem klassischen zuf√§lligen Standardsplit wurden variantenspezifische Leave-One-Variant-Out-Splits umgesetzt, um sowohl die Modellleistung auf bekannten Daten als auch die Generalisierungsf√§higkeit gegen√ºber bislang unbekannten Verschlussvarianten zu untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760cc63-b4aa-417b-aaeb-2854e3e5f5b0",
   "metadata": {},
   "source": [
    "## 3.4 Bildvorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de49eac-dad7-4dd9-8f03-eb9afc4f9199",
   "metadata": {},
   "source": [
    "In diesem Kapitel wird die Bildvorverarbeitung beschrieben, die als Grundlage f√ºr das Training und die Evaluation der neuronalen Netze dient.  \n",
    "Ziel der Vorverarbeitung ist es, die bereitgestellten Bilddaten in ein einheitliches Format zu √ºberf√ºhren und f√ºr die nachfolgenden Modelle geeignet aufzubereiten.\n",
    "\n",
    "Zun√§chst werden zwei Hilfsfunktionen definiert, um die Bildgr√∂√üe konsistent anzupassen, ohne das Seitenverh√§ltnis zu ver√§ndern. Dazu wird das Bild proportional auf eine Zielgr√∂√üe skaliert und anschlie√üend durch Padding zu einem quadratischen Format erg√§nzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e313ea-fe13-4ba8-a3dc-5e667f892555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, target_size=256):\n",
    "    # Skaliert ein Bild proportional, sodass die l√§ngere Seite der Zielgr√∂√üe entspricht\n",
    "    # Seitenverh√§ltnis bleibt erhalten, es erfolgt keine Verzerrung\n",
    "\n",
    "    w, h = image.size  # urspr√ºngliche Bildbreite und -h√∂he\n",
    "\n",
    "    if w >= h:\n",
    "        new_w = target_size\n",
    "        new_h = int(h * target_size / w)\n",
    "    else:\n",
    "        new_h = target_size\n",
    "        new_w = int(w * target_size / h)\n",
    "\n",
    "    # Resize mit bilinearer Interpolation\n",
    "    # Beim Skalieren fallen die neuen Pixelpositionen i. d. R. nicht exakt auf das alte Pixelraster\n",
    "    # Die neuen Pixelwerte werden daher aus den vier n√§chstgelegenen Pixeln interpoliert\n",
    "    return image.resize((new_w, new_h), Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27efbf-415d-4984-b319-d621f4219b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_pad(image):\n",
    "    # Erweitert ein Bild zentriert auf ein quadratisches Format\n",
    "\n",
    "    w, h = image.size\n",
    "    max_wh = max(w, h)\n",
    "\n",
    "    # Padding so berechnen, dass das Bild zentriert bleibt\n",
    "    pad_left   = (max_wh - w) // 2\n",
    "    pad_right  = max_wh - w - pad_left\n",
    "    pad_top    = (max_wh - h) // 2\n",
    "    pad_bottom = max_wh - h - pad_top\n",
    "\n",
    "    padding = (pad_left, pad_top, pad_right, pad_bottom)\n",
    "\n",
    "    # Padding mit schwarzem Hintergrund (fill=0)\n",
    "    return TF.pad(image, padding, fill=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e8a09-8203-4026-b57a-e3c548a2e2c3",
   "metadata": {},
   "source": [
    "Die Bildvorverarbeitung wird in zwei aufeinanderfolgende Schritte unterteilt. Im ersten Schritt werden ausschlie√ülich deterministische Operationen angewendet, die f√ºr alle Bilder identisch sind und unabh√§ngig vom Trainingsprozess erfolgen. Dieser Schritt dient der Vereinheitlichung der Eingabedaten und bildet die Grundlage f√ºr alle nachfolgenden Trainings- und Evaluationsschritte.\n",
    "\n",
    "Die deterministische Vorverarbeitung (Schritt 1) wird in dieser Arbeit in zwei Varianten umgesetzt, die sich ausschlie√ülich in der geometrischen Behandlung der Eingabebilder unterscheiden.\n",
    "\n",
    "Variante 1 ( --> transform_step1) stellt die Standardvorverarbeitung dar und kombiniert eine seitenverh√§ltniserhaltende Skalierung mit anschlie√üendem Padding auf ein quadratisches Format. Das Bild wird proportional skaliert, sodass die l√§ngere Seite der Zielgr√∂√üe entspricht, w√§hrend das Seitenverh√§ltnis unver√§ndert bleibt. Anschlie√üend wird das Bild zentriert auf ein quadratisches Format erweitert, wobei die fehlenden Bereiche mit einem schwarzen Hintergrund aufgef√ºllt werden.\n",
    "\n",
    "Variante 2 (--> transform_step1_distort) verwendet eine direkte Skalierung der Bilder auf eine feste quadratische Zielaufl√∂sung von 256 √ó 256 Pixeln. Dabei werden Breite und H√∂he unabh√§ngig voneinander angepasst, sodass das urspr√ºngliche Seitenverh√§ltnis nicht erhalten bleibt und geometrische Verzerrungen entstehen k√∂nnen. Diese Variante dient der gezielten Untersuchung des Einflusses der geometrischen Bildnormalisierung auf die Modellleistung.\n",
    "\n",
    "Beide Varianten werden ausschlie√ülich im Rahmen der deterministischen Vorverarbeitung eingesetzt und anschlie√üend in Tensoren konvertiert. Die erzeugten Bildtensoren k√∂nnen gespeichert und bei wiederholten Trainingsl√§ufen wiederverwendet werden, um den Rechenaufwand zu reduzieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6904d61-2bb7-4499-9886-704a1279ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorverarbeitung Step 1 - Variante 1\n",
    "# Resize mit Seitenverh√§ltnis, Padding auf Quadrat, Konvertierung zu Tensor\n",
    "transform_step1 = transforms.Compose([\n",
    "    resize_image,\n",
    "    square_pad,\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4cf6ac-011d-4926-9b11-b9844d3bc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorverarbeitung Step 1 - Variante 2\n",
    "# Direkte Skalierung auf 256√ó256 ohne Erhalt des Seitenverh√§ltnisses (bewusste Verzerrung)\n",
    "transform_step1_distort = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # unabh√§ngig in Breite und H√∂he\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf8d1c-71d4-4dc1-841c-dc2aff89afc4",
   "metadata": {},
   "source": [
    "Im zweiten Schritt werden trainingsspezifische Operationen angewendet, die ausschlie√ülich w√§hrend des Trainings zum Einsatz kommen. Dazu z√§hlen Datenaugmentationen sowie die anschlie√üende Normalisierung der Bilddaten.\n",
    "\n",
    "Die Augmentationsparameter sind explizit definiert und k√∂nnen zwischen verschiedenen Trainingsl√§ufen angepasst werden, um deren Einfluss auf die Modellleistung gezielt zu untersuchen und geeignete Parameterkombinationen zu identifizieren. F√ºr die Normalisierung werden die etablierten ImageNet-Mittelwerte und -Standardabweichungen verwendet.\n",
    "\n",
    "Weitere Augmentationsstrategien sind grunds√§tzlich m√∂glich und werden in der Praxis teilweise eingesetzt; im Rahmen dieser Arbeit beschr√§nkt sich die Vorverarbeitung jedoch bewusst auf eine √ºberschaubare und kontrollierbare Auswahl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea784a-bb61-404c-a34e-57bf36ecde6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = (0.485, 0.456, 0.406)\n",
    "imagenet_std  = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62e628-1cb5-4bf2-89ad-a50c81d87876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter zur Datenaugmentation\n",
    "ROT_DEG = 0          # maximale Rotationsabweichung in Grad\n",
    "HFLIP_P = 0.0        # Wahrscheinlichkeit f√ºr horizontales Spiegeln\n",
    "\n",
    "BRIGHTNESS = 0       # √Ñnderung der Bildhelligkeit\n",
    "CONTRAST   = 0       # √Ñnderung des Bildkontrasts\n",
    "SATURATION = 0       # √Ñnderung der Farbs√§ttigung\n",
    "HUE        = 0       # Verschiebung des Farbtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c4297-a8d4-455a-8395-cff4fef9ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, damit die Augmentationsparameter f√ºr das Training jeweils nach Bedarf angepasst werden k√∂nnen\n",
    "def build_train_transform(\n",
    "    rot_deg=ROT_DEG,\n",
    "    hflip_p=HFLIP_P,\n",
    "    brightness=BRIGHTNESS,\n",
    "    contrast=CONTRAST,\n",
    "    saturation=SATURATION,\n",
    "    hue=HUE,\n",
    "):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomRotation(rot_deg),\n",
    "        transforms.RandomHorizontalFlip(p=hflip_p),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=brightness,\n",
    "            contrast=contrast,\n",
    "            saturation=saturation,\n",
    "            hue=hue,\n",
    "        ),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba98d1-cdb7-45d3-9128-a036d0f2b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion nutzen, um konkrete Instanz zu erzeugen\n",
    "transform_train_step2 = build_train_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6419e-e3ce-48fd-baef-6096c5e7fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test_step2 = transforms.Compose([\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27407a97-45fe-41da-986d-bb7668401655",
   "metadata": {},
   "source": [
    "Die beschriebene Bildvorverarbeitung bildet die Grundlage f√ºr die nachfolgenden Schritte der Datenverarbeitung und stellt sicher, dass alle Bilder in ein einheitliches Format √ºberf√ºhrt werden.\n",
    "\n",
    "Zur Veranschaulichung der deterministischen Vorverarbeitung werden im Folgenden beide in dieser Arbeit verwendeten Varianten anhand eines Beispielbildes gegen√ºbergestellt. Dargestellt sind jeweils das Originalbild, das Ergebnis der deterministischen Vorverarbeitung (Schritt 1) sowie der trainingsspezifische Modell-Input nach Anwendung von Datenaugmentation und Normalisierung (Schritt 2).\n",
    "\n",
    "Die Abbildung dient ausschlie√ülich der Illustration der unterschiedlichen Vorverarbeitungsstrategien und ihrer Auswirkungen auf die Bildrepr√§sentation. Sie hat keinen Einfluss auf das eigentliche Training der Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbc941-3cff-4fbb-a0a9-7281959c2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielbild ausw√§hlen\n",
    "img_path = all_image_paths[30000]\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Hilfsfunktion zur Visualisierung:\n",
    "# F√ºhrt eine Min-Max-Skalierung durch, um normalisierte Tensoren darstellbar zu machen\n",
    "# keine Denormalisierung, sondern dient ausschlie√ülich der Visualisierung\n",
    "def prepare_for_display(tensor_img):\n",
    "    x = tensor_img.clone()\n",
    "    x = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    "    return TF.to_pil_image(x)\n",
    "\n",
    "# Variante 1: Resize mit Seitenverh√§ltnis + Padding auf Quadrat\n",
    "# Schritt 1: deterministische Vorverarbeitung\n",
    "img_step1_pad = transform_step1(image)\n",
    "# Schritt 2: trainingsspezifische Vorverarbeitung\n",
    "# Augmentation und Normalisierung ‚Üí tats√§chlicher Modell-Input\n",
    "img_step2_pad = transform_train_step2(img_step1_pad)\n",
    "\n",
    "# Variante 2: Direkte Skalierung auf 256x256 (bewusste Verzerrung)\n",
    "# Schritt 1: deterministische Vorverarbeitung (ohne Erhalt des Seitenverh√§ltnisses)\n",
    "img_step1_distort = transform_step1_distort(image)\n",
    "# Schritt 2: identische trainingsspezifische Vorverarbeitung\n",
    "img_step2_distort = transform_train_step2(img_step1_distort)\n",
    "\n",
    "# Visualisierung beider Varianten\n",
    "# Oben: Resize + Padding\n",
    "# Unten: Verzerrte Skalierung (256x256)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8, 5))\n",
    "\n",
    "# Variante 1 - ohne Verzerrung\n",
    "axes[0, 0].imshow(image)\n",
    "axes[0, 0].set_title(\"Originalbild\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[0, 1].imshow(TF.to_pil_image(img_step1_pad))\n",
    "axes[0, 1].set_title(\"Variante 1:\\nResize + Padding\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "axes[0, 2].imshow(prepare_for_display(img_step2_pad))\n",
    "axes[0, 2].set_title(\"Variante 1:\\nAugmentation + Normalisierung\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "\n",
    "# Variante 2 - mit Verzerrung\n",
    "axes[1, 0].imshow(image)\n",
    "axes[1, 0].set_title(\"Originalbild\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "axes[1, 1].imshow(TF.to_pil_image(img_step1_distort))\n",
    "axes[1, 1].set_title(\"Variante 2:\\nDirektes Resize (256x256)\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "axes[1, 2].imshow(prepare_for_display(img_step2_distort))\n",
    "axes[1, 2].set_title(\"Variante 2:\\nAugmentation + Normalisierung\")\n",
    "axes[1, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22517362-1aca-405f-aaa0-dbe8735ddfb9",
   "metadata": {},
   "source": [
    "## 3.5 Datasets und DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4a725-bb6d-4b1c-8c36-ecbd06828501",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird beschrieben, wie die vorverarbeiteten Bilddaten in PyTorch-Datasets eingebunden und mithilfe von DataLoadern f√ºr Training, Validierung und Test bereitgestellt werden. Dabei wird insbesondere auf eine klare Trennung zwischen deterministischer Bildvorverarbeitung und trainingsspezifischen Transformationen geachtet. Ziel ist eine effiziente, reproduzierbare und f√ºr alle Datensplits einheitliche Datenbereitstellung.  \n",
    "\n",
    "Zun√§chst werden die Verzeichnisse definiert, in denen die Ergebnisse der deterministischen Bildvorverarbeitung abgelegt werden. F√ºr jede Variante der deterministischen Vorverarbeitung wird ein separates Verzeichnis verwendet, sodass die vorverarbeiteten Bildtensoren eindeutig der jeweiligen Vorverarbeitungsstrategie zugeordnet sind. In diesen Verzeichnissen werden die einmalig erzeugten, vorverarbeiteten Bildtensoren gespeichert und bei sp√§teren Zugriffen direkt geladen. Auf diese Weise werden wiederholte Berechnungen vermieden und eine effiziente sowie reproduzierbare Datenbereitstellung sichergestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af0b95-264a-4a25-acd2-311c622c2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zu den vorverarbeiteten Bildern (deterministische Vorverarbeitung) --> mit Padding / nicht verzerrt\n",
    "preprocessed_path = \"/home/archive/TetheredCap/preprocessed_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343d34f-b03b-499b-b026-3334720492f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zu den vorverarbeiteten Bildern (deterministische Vorverarbeitung) --> bewusst verzerrt\n",
    "preprocessed_path_distort = \"/home/archive/TetheredCap/preprocessed_images_distort\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075142b6-5c36-45e6-8783-9d0cb4c7944a",
   "metadata": {},
   "source": [
    "Im n√§chsten Schritt wird ein eigenes PyTorch-Dataset implementiert, das den Zugriff auf die Bilddaten abstrahiert und die zweistufige Bildvorverarbeitung systematisch abbildet. Ziel ist es, deterministische Vorverarbeitungsschritte effizient wiederzuverwenden, w√§hrend trainingsspezifische Transformationen flexibel und splitspezifisch angewendet werden k√∂nnen.\n",
    "\n",
    "Die Bildvorverarbeitung ist dabei in zwei Stufen unterteilt. **transform_step1** umfasst deterministische Operationen zur Vereinheitlichung der Eingabedaten. Abh√§ngig von der gew√§hlten Vorverarbeitungsvariante beinhaltet dieser Schritt entweder eine seitenverh√§ltniserhaltende Skalierung mit anschlie√üendem Padding oder eine direkte Skalierung auf eine feste quadratische Zielaufl√∂sung. Die resultierenden Tensoren werden einmalig pro Bild erzeugt und in einem variantspezifischen Verzeichnis gespeichert, sodass sie bei weiteren Zugriffen wiederverwendet werden k√∂nnen.\n",
    "\n",
    "**transform_step2** enth√§lt trainingsspezifische Transformationen wie Datenaugmentation und Normalisierung. Diese werden erst nach dem Laden der vorverarbeiteten Tensoren angewendet und k√∂nnen abh√§ngig vom jeweiligen Datensplit (Training, Validation, Test) unterschiedlich konfiguriert werden.\n",
    "\n",
    "Die Klassenzugeh√∂rigkeit wird aus der Ordnerstruktur abgeleitet (good = 0, bad = 1). Zus√§tzlich wird der Originalpfad des Bildes zur√ºckgegeben, um eine gezielte Analyse und Visualisierung einzelner Beispiele, etwa bei Fehlklassifikationen, zu erm√∂glichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b8362-5fed-45e1-ab05-8e46653145dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessedImageDataset(Dataset):\n",
    "    def __init__(self, paths, preprocessed_dir, transform_step1, transform_step2=None):\n",
    " \n",
    "        # paths: Liste der Bildpfade (Train / Validation / Test)\n",
    "        # preprocessed_dir: Ablageort f√ºr deterministisch vorverarbeitete Bildtensoren\n",
    "        # transform_step1: feste Vorverarbeitung (Resize, Padding, ToTensor)\n",
    "        # transform_step2: optionale trainingsspezifische Transformationen\n",
    "\n",
    "        self.paths = paths\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.transform_step1 = transform_step1\n",
    "        self.transform_step2 = transform_step2\n",
    "\n",
    "        # Zielverzeichnis f√ºr vorverarbeitete Tensoren anlegen\n",
    "        os.makedirs(self.preprocessed_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _get_label(self, path):\n",
    "        # Label aus der Ordnerstruktur ableiten (good = 0, bad = 1)\n",
    "        p = path.lower()\n",
    "        return 0 if \"/good/\" in p else 1\n",
    "\n",
    "    def _get_preprocessed_path(self, path):\n",
    "        # Eindeutiger Dateiname basierend auf relativem Bildpfad\n",
    "        rel_path = os.path.relpath(path)\n",
    "        h = hashlib.md5(rel_path.encode(\"utf-8\")).hexdigest()\n",
    "        return os.path.join(self.preprocessed_dir, f\"{h}.pt\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self._get_label(path)\n",
    "\n",
    "        preprocessed_file = self._get_preprocessed_path(path)\n",
    "\n",
    "        # Vorverarbeiteten Tensor laden oder einmalig erzeugen\n",
    "        if os.path.exists(preprocessed_file):\n",
    "            x = torch.load(preprocessed_file, map_location=\"cpu\")\n",
    "        else:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            x = self.transform_step1(img)\n",
    "            torch.save(x, preprocessed_file)\n",
    "\n",
    "        # Trainingsspezifische Transformationen (falls definiert)\n",
    "        if self.transform_step2 is not None:\n",
    "            x = self.transform_step2(x)\n",
    "\n",
    "        return x, label, path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8e8a5-9950-465b-8dca-abf68862071e",
   "metadata": {},
   "source": [
    "Auf Basis der definierten Dataset-Klasse werden im n√§chsten Schritt die Datasets sowie die zugeh√∂rigen DataLoader erstellt, die die Daten in Mini-Batches bereitstellen und die Iteration w√§hrend Training und Evaluation √ºbernehmen. F√ºr den Trainingssplit wird dabei die trainingsspezifische Transformationsstufe (transform_step2 inklusive Datenaugmentation) angewendet, w√§hrend f√ºr Validierung und Test ausschlie√ülich die deterministische Vorverarbeitung sowie die Normalisierung ohne Augmentation zum Einsatz kommen. Die wichtigsten Parameter, wie Batch-Gr√∂√üe, Anzahl der Worker und optionale Durchmischung der Trainingsdaten, werden zentral konfiguriert.\n",
    "\n",
    "Um die Erstellung der Datasets und DataLoader f√ºr die unterschiedlichen Datensplits zu vereinheitlichen und redundanten Code zu vermeiden, wird eine Hilfsfunktion definiert. Diese kapselt sowohl die Initialisierung der jeweiligen Datasets als auch die zugeh√∂rigen DataLoader und kann identisch f√ºr den Standard-Split sowie f√ºr die variantenspezifischen Leave-One-Variant-Out-Splits verwendet werden. Dadurch wird eine konsistente und reproduzierbare Datenbereitstellung √ºber alle Experimente hinweg sichergestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baaaac5-5959-4737-a414-a59748d95d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_paths,\n",
    "    val_paths,\n",
    "    test_paths,\n",
    "    preprocessed_dir,\n",
    "    transform_step1, \n",
    "    transform_train_step2,\n",
    "    transform_test_step2,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    shuffle_train=True,\n",
    "):\n",
    "    # Trainings-Dataset:\n",
    "    # deterministische Vorverarbeitung (transform_step1) +\n",
    "    # trainingsspezifische Transformationen inkl. Augmentation (transform_step2)\n",
    "    train_dataset = PreprocessedImageDataset(\n",
    "        paths=train_paths,\n",
    "        preprocessed_dir=preprocessed_dir,\n",
    "        transform_step1=transform_step1,\n",
    "        transform_step2=transform_train_step2,\n",
    "    )\n",
    "\n",
    "    # Validierungs-Dataset:\n",
    "    # identische deterministische Vorverarbeitung,\n",
    "    # ohne Datenaugmentation (nur Normalisierung)\n",
    "    val_dataset = PreprocessedImageDataset(\n",
    "        paths=val_paths,\n",
    "        preprocessed_dir=preprocessed_dir,\n",
    "        transform_step1=transform_step1,\n",
    "        transform_step2=transform_test_step2,\n",
    "    )\n",
    "\n",
    "    # Test-Dataset:\n",
    "    # gleiche Konfiguration wie f√ºr die Validierung,\n",
    "    test_dataset = PreprocessedImageDataset(\n",
    "        paths=test_paths,\n",
    "        preprocessed_dir=preprocessed_dir,\n",
    "        transform_step1=transform_step1,\n",
    "        transform_step2=transform_test_step2,\n",
    "    )\n",
    "\n",
    "    # DataLoader f√ºr das Training:\n",
    "    # optionale Durchmischung der Daten und paralleles Laden √ºber mehrere Worker\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # DataLoader f√ºr Validierung:\n",
    "    # keine Durchmischung, da keine Gradientenberechnung erfolgt\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # DataLoader f√ºr Test:\n",
    "    # identisches Vorgehen wie bei der Validierung\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7424f-3a22-4211-a0fc-f917f54db4ac",
   "metadata": {},
   "source": [
    "Die zuvor definierte Hilfsfunktion wird im Folgenden auf die unterschiedlichen Datensplits und Vorverarbeitungsvarianten angewendet. Zun√§chst werden die DataLoader f√ºr den klassischen Standard-Split in zwei Varianten erzeugt, die sich in der deterministischen Bildvorverarbeitung unterscheiden. Anschlie√üend werden die DataLoader f√ºr die variantenspezifischen Leave-One-Variant-Out-Splits erstellt. Auf diese Weise stehen f√ºr alle Experimente konsistent konfigurierte Trainings-, Validierungs- und Test-Loader zur Verf√ºgung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064a313-d2c2-4624-a180-786f64495678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-Split (PAD: Resize + Padding)\n",
    "train_loader_std, val_loader_std, test_loader_std = create_dataloaders(\n",
    "    train_paths,\n",
    "    val_paths,\n",
    "    test_paths,\n",
    "    preprocessed_dir=preprocessed_path,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_train_step2=transform_train_step2,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")\n",
    "\n",
    "# Standard-Split (DISTORT: direktes Resize 256x256, bewusste Verzerrung)\n",
    "train_loader_std_distort, val_loader_std_distort, test_loader_std_distort = create_dataloaders(\n",
    "    train_paths,\n",
    "    val_paths,\n",
    "    test_paths,\n",
    "    preprocessed_dir=preprocessed_path_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_train_step2=transform_train_step2,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")\n",
    "\n",
    "# LOVO: cc_red (PAD)\n",
    "train_loader_ccred, val_loader_ccred, test_loader_ccred = create_dataloaders(\n",
    "    train_paths_lovo_ccred,\n",
    "    val_paths_lovo_ccred,\n",
    "    test_paths_lovo_ccred,\n",
    "    preprocessed_dir=preprocessed_path,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_train_step2=transform_train_step2,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")\n",
    "\n",
    "# LOVO: voesl_zitrone (PAD)\n",
    "train_loader_voesl, val_loader_voesl, test_loader_voesl = create_dataloaders(\n",
    "    train_paths_lovo_voesl_zitrone,\n",
    "    val_paths_lovo_voesl_zitrone,\n",
    "    test_paths_lovo_voesl_zitrone,\n",
    "    preprocessed_dir=preprocessed_path,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_train_step2=transform_train_step2,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a775a1f-886e-42ec-a277-ac9470613f24",
   "metadata": {},
   "source": [
    "In diesem Kapitel wurden die technischen und konzeptionellen Grundlagen f√ºr die nachfolgende Modellierung geschaffen. Beginnend mit dem technischen Setup und den ben√∂tigten Paketimporten wurden die Bilddaten zun√§chst explorativ analysiert und anschlie√üend √ºber unterschiedliche Strategien in Trainings-, Validierungs- und Testdaten aufgeteilt. Neben einem klassischen Standard-Split wurde mit dem variantenspezifischen Leave-One-Variant-Out-Ansatz gezielt die Generalisierungsf√§higkeit der Modelle unter ver√§nderten Datenverteilungen vorbereitet.\n",
    "\n",
    "Darauf aufbauend wurde eine zweistufige Bildvorverarbeitung eingef√ºhrt, die deterministische Schritte zur Vereinheitlichung der Eingabedaten von trainingsspezifischen Transformationen trennt. Die deterministische Vorverarbeitung wurde dabei in zwei Varianten umgesetzt, um unterschiedliche geometrische Bildrepr√§sentationen systematisch vergleichen zu k√∂nnen, w√§hrend trainingsspezifische Operationen flexibel und splitspezifisch angewendet werden. Die Einbindung der Daten in PyTorch-Datasets und DataLoader stellt schlie√ülich eine konsistente und reproduzierbare Datenbasis f√ºr alle folgenden Experimente sicher.\n",
    "\n",
    "Auf dieser Grundlage widmet sich das n√§chste Kapitel der eigentlichen Modellierung der √ºberwachten Bildklassifikation zur Inspektion von Tethered Caps, einschlie√ülich Architekturwahl, Trainingsstrategie und Evaluierung der Modellleistung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe890ea8-55bc-4f47-9598-f86e336c4ef3",
   "metadata": {},
   "source": [
    "# 4. √úberwachte Bildklassifikation zur Inspektion von Tethered Caps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a621e2e-7d99-419e-92a2-197848d0920c",
   "metadata": {},
   "source": [
    "In diesem Kapitel wird die praktische Umsetzung und systematische Evaluation einer √ºberwachten Bildklassifikation zur Inspektion von Tethered Caps beschrieben. Zun√§chst werden zwei parametrierbare Modellarchitekturen implementiert: ein Convolutional Neural Network (CNN) von Grund auf sowie ein Transfer-Learning-Ansatz auf Basis von ResNet. Darauf aufbauend werden einheitliche Hilfsfunktionen f√ºr Training, Evaluation und Fehleranalyse entwickelt und zu einer konsistenten Trainings- und Evaluationspipeline zusammengef√ºhrt, die f√ºr alle Modelle verwendet wird. Anschlie√üend erfolgt eine schrittweise Untersuchung verschiedener Architektur- und Trainingsparameter f√ºr das CNN von Grund auf sowie analog f√ºr das ResNet-Modell. Abschlie√üend werden Methoden zur Modellinterpretation mittels TorchCAM sowie die Inferenzzeit des besten Modells analysiert, um dessen Praxistauglichkeit im industriellen Einsatz zu bewerten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304812a-3a07-482c-be54-c28cbbb8006d",
   "metadata": {},
   "source": [
    "## 4.1 Modellarchitekturen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012802ba-442c-4755-bf66-b6b19076681f",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden ein CNN von Grund auf sowie ein ResNet-basierter Ansatz als parametrisierbare Modellarchitekturen definiert, die als Grundlage f√ºr die systematische Untersuchung verschiedener Modellvarianten dienen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b705f4-1f59-414a-96c3-94d25bb206a8",
   "metadata": {},
   "source": [
    "### 4.1.1 CNN von Grund auf (parametrisierbare Architektur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c75032-b615-4510-87ca-69626655b4e2",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird eine von Grund auf entwickelte Convolutional Neural Network-Architektur implementiert, die gezielt f√ºr experimentelle Untersuchungen konzipiert wurde. Ziel dieser Modellklasse ist nicht prim√§r die Optimierung der Klassifikationsleistung, sondern das systematische Experimentieren mit zentralen architektonischen und trainingsrelevanten Komponenten von CNNs.\n",
    "\n",
    "Um unterschiedliche Architekturentscheidungen nachvollziehbar untersuchen zu k√∂nnen, ist das Netzwerk bewusst parametrisierbar aufgebaut. Dadurch lassen sich einzelne Eigenschaften des Modells gezielt ver√§ndern, w√§hrend der restliche Aufbau unver√§ndert bleibt. Die resultierenden Effekte auf Trainingsverhalten und Modellleistung k√∂nnen so isoliert analysiert werden.\n",
    "\n",
    "Im Rahmen der Experimente k√∂nnen insbesondere folgende Aspekte variiert werden:\n",
    "\n",
    "**Netzwerktiefe und -breite (channels)**  \n",
    "Durch Anpassung der Anzahl der Convolution-Bl√∂cke (Tiefe) sowie der Anzahl der Feature-Maps pro Block (Breite) l√§sst sich untersuchen, wie sich Modellkapazit√§t und Repr√§sentationsf√§higkeit auf das Lernergebnis auswirken.\n",
    "\n",
    "**Klassifikationskopf (head)**  \n",
    "Es kann zwischen einem klassischen Fully-Connected-Head und einem kompakteren Global-Average-Pooling-Head gew√§hlt werden, um unterschiedliche Strategien der Merkmalsaggregation zu vergleichen.\n",
    "\n",
    "**Batch Normalization (use_bn)**  \n",
    "Die optionale Aktivierung von Batch Normalization erlaubt es, den Einfluss dieser Regularisierungs- und Stabilisierungstechnik auf Konvergenzverhalten und Generalisierung zu analysieren.\n",
    "\n",
    "**Kernelgr√∂√üe der Convolution-Schichten (kernel_size)**  \n",
    "Durch Variation der Kernelgr√∂√üe kann der Einfluss des rezeptiven Feldes auf die Merkmalsextraktion untersucht werden.\n",
    "\n",
    "**Regularisierung (dropout)**  \n",
    "√úber die Dropout-Rate l√§sst sich gezielt der Einfluss zus√§tzlicher Regularisierung im Klassifikationskopf untersuchen.\n",
    "\n",
    "Die Architektur dient damit als flexible experimentelle Basis, um grundlegende Designentscheidungen von Convolutional Neural Networks systematisch zu analysieren und bildet die Referenz f√ºr alle weiteren Modellvergleiche in dieser Arbeit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cfdb8-5728-4711-aa9c-e0c09b8cf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer eigenen Modellklasse um parametrierbare neuronale Netze zu erzeugen, welche von nn.Module erbt\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=2,        # Anzahl der Ausgabeklassen\n",
    "        in_channels=3,        # Anzahl der Kan√§le im Eingabebild, 3 bedeutet RGB (Rot/Gr√ºn/Blau)\n",
    "        channels=(32, 64),    # Tuple mit den Feature-Map-Anzahlen pro Conv-Block; mehr Werte --> tieferes Netz/ mehr Stufen; gr√∂√üere Zahlen --> breiteres Netz/ mehr Filter pro Stufe\n",
    "        use_bn=True,          # BatchNorm an/aus\n",
    "        head=\"fc\",            # \"fc\" --> klassischer Kopf mit Flatten & Fully Connected Layers; \"gap\" --> Global Average Pooling + Linear Layer (kompakter)\n",
    "        fc_hidden=128,        # nur relevant wenn head=\"fc\"; Angabe der Gr√∂√üe der Hidden Layer im Fully-Connected-Teil\n",
    "        dropout=0.0,          # Dropout-Rate f√ºr die Regularisierung\n",
    "        kernel_size=3,        # Kernelgr√∂√üe f√ºr alle Conv-Schichten\n",
    "        adaptive_pool_out=(4, 4),  # Zielgr√∂√üe des AdaptiveAvgPool2d im FC-Head; z.B. (4,4) oder (2,2). Nur relevant wenn head=\"fc\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sicherstellen, dass nur unterst√ºtzte Head-Varianten verwendet werden\n",
    "        assert head in (\"fc\", \"gap\")\n",
    "        # Sicherstellen, dass channels nicht leer ist (ben√∂tigt f√ºr channels[-1])\n",
    "        assert len(channels) > 0, \"channels darf nicht leer sein.\"\n",
    "        # Sicherstellen, dass eine ungerade Kernelgr√∂√üe verwendet wird (z. B. 3, 5, 7), damit padding die r√§umliche Gr√∂√üe stabil h√§lt\n",
    "        assert kernel_size % 2 == 1, \"kernel_size sollte ungerade sein (z. B. 3, 5, 7).\"\n",
    "\n",
    "        self.head = head\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Dropout optional: bei dropout=0 wirkt die Schicht als Identity (--> dann kein Effekt)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # Aufbau des CNN-Hauptteils:\n",
    "        # Die einzelnen Convolution-Bl√∂cke werden hier gesammelt\n",
    "        # und sp√§ter zu einer sequentiellen Struktur zusammengef√ºgt\n",
    "        blocks = []\n",
    "        c_in = in_channels\n",
    "\n",
    "        # Hilfsfunktion zum Aufbau eines einzelnen Convolution-Blocks, bestehend aus:\n",
    "        # Conv -> optional BatchNorm -> ReLU -> MaxPooling\n",
    "        # parametrisiert √ºber Ein- und Ausgabekan√§le sowie Kernelgr√∂√üe\n",
    "        def conv_block(c_in, c_out, kernel_size):\n",
    "            # Conv -> (optional BN) -> ReLU -> MaxPool\n",
    "            padding = kernel_size // 2  # h√§lt die r√§umliche Gr√∂√üe bei ungerader Kernelgr√∂√üe konstant\n",
    "            layers = [\n",
    "                nn.Conv2d(\n",
    "                    c_in,\n",
    "                    c_out,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=not use_bn,\n",
    "                )\n",
    "            ]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm2d(c_out))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.MaxPool2d(2))  # halbiert H√∂he und Breite\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Aufbau des CNNs Schicht f√ºr Schicht anhand der angegebenen Kanalstruktur\n",
    "        # Kernelgr√∂√üe ist √ºberall gleich\n",
    "        for i, c_out in enumerate(channels):\n",
    "            blocks.append(conv_block(c_in, c_out, kernel_size=self.kernel_size))\n",
    "            c_in = c_out\n",
    "\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        last_c = channels[-1]  # Anzahl der Kan√§le nach dem letzten Conv-Block\n",
    "\n",
    "        # Klassifikationskopf\n",
    "        if self.head == \"gap\":\n",
    "            # Global Average Pooling -> Linear\n",
    "            # reduziert [B,C,H,W] --> [B,C,1,1] --> unabh√§ngig von der Eingangsbildgr√∂√üe\n",
    "            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.classifier = nn.Linear(last_c, num_classes)\n",
    "        else:\n",
    "            # FC-Head mit fester Featuregr√∂√üe\n",
    "            # d.h. wir erzwingen eine feste r√§umliche Gr√∂√üe vor dem MLP mittels AdaptiveAvgPooling\n",
    "            self.pool = nn.AdaptiveAvgPool2d(adaptive_pool_out)\n",
    "\n",
    "            # Feature-Dimension h√§ngt nun von adaptive_pool_out ab:\n",
    "            # [B, C, H, W] --> [B, C * H * W]\n",
    "            pooled_h, pooled_w = adaptive_pool_out\n",
    "            in_features = last_c * pooled_h * pooled_w\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),                      # [B,C,H,W] --> [B, C*H*W]\n",
    "                nn.Linear(in_features, fc_hidden),  # C*H*W --> Hidden\n",
    "                nn.ReLU(inplace=True),\n",
    "                self.dropout,                      # optionales Dropout (nur wenn > 0)\n",
    "                nn.Linear(fc_hidden, num_classes),  # Hidden --> KlassenLogits\n",
    "            )\n",
    "\n",
    "    # Vorw√§rtsdurchlauf\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Pooling - abh√§ngig vom Klassifikationskopf\n",
    "        x = self.pool(x)\n",
    "\n",
    "        if self.head == \"gap\":\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.classifier(x)\n",
    "        else:\n",
    "            x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efc0d1-281c-41ad-8f90-d67b79e869e0",
   "metadata": {},
   "source": [
    "### 4.1.2 ResNet als vortrainierte Architektur (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a912da-df85-4f1f-9671-a5141219f402",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird eine ResNet-basierte Modellklasse implementiert, um ein vortrainiertes Convolutional Neural Network als zweite Modellfamilie mit dem zuvor entwickelten CNN vergleichen zu k√∂nnen. Der Schwerpunkt liegt dabei nicht auf einer vollst√§ndigen Neukonstruktion der Architektur, sondern auf typischen Transfer-Learning-Settings mit kontrollierbaren Eingriffen in Backbone und Klassifikationskopf. Dadurch lassen sich Effekte von Pretraining und Fine-Tuning systematisch untersuchen, w√§hrend Trainingslogik und Evaluation mit den vorherigen Experimenten konsistent bleiben.\n",
    "\n",
    "Im Rahmen der Experimente k√∂nnen insbesondere folgende Aspekte variiert werden:\n",
    "\n",
    "**Architekturwahl (arch)**\n",
    "√úber den Parameter arch wird festgelegt, welche ResNet-Variante genutzt wird. Im Rahmen dieser Ausarbeitung wird ausschlie√ülich ResNet-18 verwendet.\n",
    "\n",
    "**Pretraining (pretrained)**\n",
    "Mit pretrained=True werden auf ImageNet gelernte Gewichte als Feature-Extraktor genutzt, um Lernverhalten und Generalisierung gegen√ºber einem Training from scratch zu vergleichen.\n",
    "\n",
    "**Eingangskan√§le (in_channels)**\n",
    "Falls nicht mit RGB gearbeitet wird, kann die erste Convolution-Schicht an eine andere Kanalanzahl angepasst werden, ohne den restlichen Backbone zu ver√§ndern.\n",
    "\n",
    "**Transfer-Learning-Strategie (freeze_backbone, unfreeze_from)**\n",
    "Das Backbone kann initial eingefroren werden, um ausschlie√ülich den Klassifikationskopf zu trainieren. Optional k√∂nnen ab einem definierten Block (layer4, layer3, ‚Ä¶) wieder Parameter freigegeben werden, um Fine-Tuning-Varianten zu testen.\n",
    "\n",
    "**Klassifikationskopf (head, fc_hidden, dropout)**\n",
    "Der Klassifikationskopf ersetzt die originale fc-Schicht des ResNet und √ºbernimmt die Abbildung des vom Backbone erzeugten Feature-Vektors (bei ResNet-18 typischerweise 512 Dimensionen nach Global Average Pooling) auf die Zielklassen (good/bad). Damit bleiben Feature-Extraktion und Entscheidungslogik klar getrennt und gezielt variierbar.\n",
    "\n",
    "√úber den Parameter head werden zwei Varianten untersucht:\n",
    "\n",
    "* **Linear-Head** (head=\"linear\")\n",
    "Minimaler Klassifikator mit direkter Projektion des Feature-Vektors auf die Ausgabeklassen. Diese Variante f√ºgt nur wenige zus√§tzliche Parameter hinzu und eignet sich besonders, um den Einfluss von Pretraining und Fine-Tuning isoliert zu analysieren.\n",
    "\n",
    "* **MLP-Head** (head=\"mlp\")\n",
    "Erweiterter Kopf mit zus√§tzlicher Hidden-Schicht (fc_hidden) und ReLU-Aktivierung vor der finalen Klassifikation. Dadurch erh√∂ht sich die Modellkapazit√§t, was komplexere Entscheidungsgrenzen erm√∂glicht, jedoch auch das Overfitting-Risiko steigern kann.\n",
    "\n",
    "Die Regularisierung erfolgt optional √ºber Dropout im Klassifikationskopf. So l√§sst sich kontrollieren, ob Leistungsunterschiede prim√§r auf erh√∂hter Kapazit√§t oder verbesserter Generalisierung beruhen.\n",
    "\n",
    "Damit bildet die ResNet-Klasse eine flexible Transfer-Learning-Basis, die einen fairen Vergleich mit dem zuvor entwickelten CNN erm√∂glicht und typische Designentscheidungen (Pretraining, Freeze/Unfreeze, Head-Komplexit√§t) experimentell isolierbar macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67faf4c5-658e-415c-9544-a6f9da2da80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition einer eigenen Modellklasse f√ºr ResNet-basierte Bildklassifikation\n",
    "# Ziel: Vergleich mit dem zuvor entwickelten CNN bei m√∂glichst √§hnlicher Trainingslogik\n",
    "# Fokus: Transfer Learning (Pretraining, Freeze/Unfreeze) + Head-Varianten\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=2,             # Anzahl der Ausgabeklassen (good / bad)\n",
    "        in_channels=3,             # Anzahl der Kan√§le im Eingabebild (3 = RGB)\n",
    "        arch=\"resnet18\",           # verwendete ResNet-Architektur\n",
    "        pretrained=True,           # ImageNet-Pretraining aktivieren\n",
    "        freeze_backbone=False,     # True: Backbone einfrieren (nur Kopf trainieren)\n",
    "        unfreeze_from=\"layer4\",    # ab welchem Block wieder trainiert werden soll\n",
    "        head=\"linear\",             # \"linear\" = Standard-Kopf, \"mlp\" = erweiterter Kopf\n",
    "        fc_hidden=256,             # Hidden-Gr√∂√üe im MLP-Head\n",
    "        dropout=0.0,               # Dropout-Rate im Klassifikationskopf\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sicherstellen, dass nur unterst√ºtzte Varianten verwendet werden\n",
    "        assert arch in (\"resnet18\",)\n",
    "        assert head in (\"linear\", \"mlp\")\n",
    "        assert 0.0 <= dropout < 1.0\n",
    "\n",
    "        self.arch = arch\n",
    "        self.head = head\n",
    "\n",
    "        # Dropout optional: bei dropout=0 wirkt die Schicht als Identity (--> dann kein Effekt)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # Laden des ResNet-Backbones\n",
    "        # Bei pretrained=True werden auf ImageNet gelernte Feature-Extraktoren genutzt\n",
    "        if pretrained:\n",
    "            # je nach torchvision-version gibt es unterschiedliche API-Varianten\n",
    "            try:\n",
    "                model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "            except Exception:\n",
    "                model = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            model = models.resnet18(weights=None)\n",
    "\n",
    "        # Anpassung der ersten Conv-Schicht, falls nicht RGB verwendet wird\n",
    "        # Standardm√§√üig erwartet ResNet 3 Eingangskan√§le\n",
    "        if in_channels != 3:\n",
    "            old_conv = model.conv1\n",
    "\n",
    "            # neue conv1 mit gleicher Struktur wie im Original, aber mit anderem in_channels\n",
    "            model.conv1 = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=old_conv.out_channels,\n",
    "                kernel_size=old_conv.kernel_size,\n",
    "                stride=old_conv.stride,\n",
    "                padding=old_conv.padding,\n",
    "                bias=(old_conv.bias is not None),\n",
    "            )\n",
    "\n",
    "        # Dimension des Feature-Vektors direkt vor dem ResNet-Kopf\n",
    "        # Bei ResNet-18 typischerweise 512\n",
    "        backbone_out = model.fc.in_features\n",
    "\n",
    "        # Ersetzen des Klassifikationskopfes\n",
    "        # Global Average Pooling ist bei ResNet bereits integriert (avgpool)\n",
    "        if head == \"linear\":\n",
    "            # Minimaler Klassifikationskopf\n",
    "            # Optionales Dropout direkt vor der finalen Linearschicht\n",
    "            model.fc = nn.Sequential(\n",
    "                self.dropout,\n",
    "                nn.Linear(backbone_out, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            # Erweiterter MLP-Kopf zur Erh√∂hung der Modellkapazit√§t\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Linear(backbone_out, fc_hidden),\n",
    "                nn.ReLU(inplace=True),\n",
    "                self.dropout,\n",
    "                nn.Linear(fc_hidden, num_classes),\n",
    "            )\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        # Freeze-Logik f√ºr Transfer Learning\n",
    "        if freeze_backbone:\n",
    "            self.freeze_backbone()\n",
    "\n",
    "            # Optional: bestimmte Bl√∂cke wieder freigeben (Fine-Tuning)\n",
    "            if unfreeze_from is not None:\n",
    "                self.unfreeze_from_layer(unfreeze_from)\n",
    "\n",
    "    # Friert alle Backbone-Parameter ein\n",
    "    # Der Klassifikationskopf bleibt trainierbar\n",
    "    def freeze_backbone(self):\n",
    "        # alles einfrieren\n",
    "        for _, p in self.model.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Kopf wieder trainierbar machen\n",
    "        for p in self.model.fc.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Gibt ab einem bestimmten Block wieder Parameter frei\n",
    "    # Reihenfolge der ResNet-Bl√∂cke: layer1 -> layer2 -> layer3 -> layer4 -> fc\n",
    "    def unfreeze_from_layer(self, layer_name=\"layer4\"):\n",
    "        valid = (\"layer1\", \"layer2\", \"layer3\", \"layer4\", \"fc\")\n",
    "        assert layer_name in valid\n",
    "\n",
    "        order = [\"layer1\", \"layer2\", \"layer3\", \"layer4\", \"fc\"]\n",
    "        start_idx = order.index(layer_name)\n",
    "\n",
    "        # ab gew√ºnschtem Block alles wieder trainierbar machen\n",
    "        for ln in order[start_idx:]:\n",
    "            module = getattr(self.model, ln)\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    # Hilfsfunktion zur Trennung von Backbone- und Head-Parametern\n",
    "    # damit kann man z.B. unterschiedliche Learning Rates verwenden\n",
    "    def trainable_parameter_groups(self):\n",
    "        head_params = list(self.model.fc.parameters())\n",
    "\n",
    "        # alles au√üer fc geh√∂rt zum Backbone\n",
    "        backbone_params = [\n",
    "            p for n, p in self.model.named_parameters()\n",
    "            if not n.startswith(\"fc.\")\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"backbone\": [p for p in backbone_params if p.requires_grad],\n",
    "            \"head\": [p for p in head_params if p.requires_grad],\n",
    "        }\n",
    "\n",
    "    # Vorw√§rtsdurchlauf\n",
    "    # die komplette Logik liegt im torchvision-ResNet\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ce3dd-6cc1-45bc-aa63-591d2ab2ebce",
   "metadata": {},
   "source": [
    "## 4.2 Hilfsfunktionen f√ºr Training, Evaluation und Fehleranalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc6247-cb74-49df-a5cf-3ee1d75d7fee",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden zentrale Hilfsfunktionen implementiert, die im Rahmen des Trainings, der Evaluation sowie der anschlie√üenden Fehleranalyse der Modelle verwendet werden. Die Funktionen dienen der Berechnung relevanter Metriken, der Visualisierung des Trainingsverlaufs und der Modellperformance sowie der systematischen Analyse von Fehlklassifikationen.\n",
    "\n",
    "Neben klassischen Auswertungen wie Konfusionsmatrix, Accuracy, Loss und False-Positive-Rate umfasst das Kapitel auch Funktionen zur qualitativen Fehleranalyse. Diese erm√∂glichen es, Fehlklassifikationen sowohl aggregiert (z. B. nach Verschlussvarianten) als auch anhand konkreter Beispielbilder zu untersuchen. Ziel ist es, das Verhalten der Modelle besser zu verstehen und m√∂gliche systematische Fehler sichtbar zu machen.  \n",
    "\n",
    "Zus√§tzlich werden Hilfsfunktionen zur Grad-CAM-basierten Visualisierung implementiert. Diese erm√∂glichen eine qualitative Analyse der Modellentscheidungen auf Einzelbildebene, indem regionsbasierte Aktivierungen f√ºr vorhergesagte Klassen visualisiert werden. Insbesondere bei Fehlklassifikationen (False Positives und False Negatives) unterst√ºtzt dies die Identifikation m√∂glicher systematischer Fehlfokussierungen, etwa auf Hintergrund- oder Randbereiche der Bilder.\n",
    "\n",
    "Alle hier implementierten Hilfsfunktionen sind modular aufgebaut und werden wiederverwendbar in den Trainings- und Evaluationspipelines der verschiedenen Modellvarianten eingesetzt. Die in den jeweiligen Codebl√∂cken enthaltenen Kommentare erl√§utern die Funktionalit√§t im Detail, sodass im Folgenden auf zus√§tzliche textuelle Erkl√§rungen zwischen den einzelnen Implementierungen verzichtet wird.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea636144-f396-4e3a-98bb-23230a1cc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnet die Konfusionsmatrix aus den echten Labels (y_true) und den Vorhersagen (y_pred)\n",
    "# Ziel: √úbersicht, wie viele Beispiele pro Klasse korrekt bzw. falsch einsortiert wurden\n",
    "def create_confusion_matrix(y_true, y_pred, num_classes=2):\n",
    "    # leere Matrix der Gr√∂√üe [num_classes x num_classes] initialisieren\n",
    "    # Zeilen --> tats√§chliche Klasse; Spalten --> vorhergesagte Klasse\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    # f√ºr jedes Beispiel: Zelle [true_label, predicted_label] inkrementieren\n",
    "    # dadurch z√§hlt die Matrix alle korrekten und falschen Zuordnungen\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[int(t), int(p)] += 1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243779b-9f9d-480f-8591-485de2143abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfusionsmatrix plotten\n",
    "def plot_confusion_matrix(cm, class_names=(\"good\", \"bad\"), title=\"Konfusionsmatrix\"):\n",
    "    fig, ax = plt.subplots(figsize=(4.0, 4.0))\n",
    "\n",
    "    # TN, FP, FN, TP aus der Matrix extrahieren\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "    tp = cm[1, 1]\n",
    "\n",
    "    total = cm.sum()\n",
    "    acc = (tp + tn) / total if total > 0 else 0.0\n",
    "\n",
    "    denom_fpr = (fp + tn)\n",
    "    fpr = fp / denom_fpr if denom_fpr > 0 else 0.0\n",
    "\n",
    "    # Titel inkl. Metriken erweitern\n",
    "    title_extended = (\n",
    "        f\"{title}\\n\"\n",
    "        f\"Accuracy = {acc*100:.2f}%   |   FPR = {fpr*100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # maximale Zelle bestimmen --> wird f√ºr die Farbs√§ttigung der Darstellung verwendet\n",
    "    # falls alle Werte 0 sind, wird vmax auf 1 gesetzt, damit die Farbskala funktioniert\n",
    "    vmax = cm.max() if cm.max() > 0 else 1\n",
    "    im = ax.imshow(cm, cmap=\"Blues\", vmin=0, vmax=vmax)\n",
    "\n",
    "    ax.set_title(title_extended)\n",
    "    ax.set_xlabel(\"Vorhersage\")  # Spalten\n",
    "    ax.set_ylabel(\"tats√§chliche Klasse\")  # Zeilen\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_yticklabels(class_names)\n",
    "\n",
    "    # Zahlen in die Zellen schreiben (Textfarbe je nach Hintergrund)\n",
    "    threshold = vmax * 0.55  # Schwellwert f√ºr Textfarbe\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "            ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=color, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0757d-d39d-40e9-9409-23038c00bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der False-Positive-Rate (FPR) aus der Konfusionsmatrix\n",
    "# Relevanter Fehlerfall: good (0) wird f√§lschlich als bad (1) klassifiziert\n",
    "def compute_fpr_from_cm(cm):\n",
    "    # Auslesen der relevanten Zeilen:\n",
    "    # tn = true negatives (tats√§chlich 0, vorhergesagt 0)\n",
    "    # fp = false positives (tats√§chlich 0, vorhergesagt 1)\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "\n",
    "    # Nenner = alle tats√§chlich negativen Beispiele\n",
    "    denom = (fp + tn)\n",
    "    # FPR = Anteil der negativen Beispiele, die f√§lschlicherweise als positiv erkannt wurden\n",
    "    return fp / denom if denom > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622a8e4-a8bf-4dd0-9eac-473b25a51282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung des Trainingsverlaufs √ºber alle Epochen\n",
    "# Darstellung von Accuracy, Loss und False-Positive-Rate f√ºr Training und Validierung\n",
    "def plot_training_history(history, num_epochs=None):\n",
    "    display(Markdown(\"### **Performance im Trainingsverlauf**\"))\n",
    "    if num_epochs is None:\n",
    "        num_epochs = len(history[\"train_loss\"])\n",
    "\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "    axes[0].plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    axes[0].plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    axes[0].set_title(\"Accuracy\")\n",
    "    axes[0].set_xlabel(\"Epoche\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    axes[1].plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    axes[1].set_title(\"Loss\")\n",
    "    axes[1].set_xlabel(\"Epoche\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(epochs, np.array(history[\"val_fpr\"]) * 100.0, label=\"Validation FPR\")\n",
    "    axes[2].set_title(\"False Positive Rate\")\n",
    "    axes[2].set_xlabel(\"Epoche\")\n",
    "    axes[2].set_ylabel(\"False Positive Rate [%]\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208b518-caad-450f-a7f5-410586eebd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiert die Verschlussvariante aus dem Dateipfad\n",
    "# wird u.a. f√ºr die variantenbezogene Analyse und das Plotten der Fehlklassifikationen verwendet\n",
    "def extract_variant_from_path(path):\n",
    "    # Pfad vereinheitlichen (Windows / Linux)\n",
    "    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n",
    "\n",
    "    # Suche nach dem Ordner 'good' oder 'bad'\n",
    "    # die Verschlussvariante befindet sich direkt davor\n",
    "    for i in range(len(parts)):\n",
    "        if parts[i] in (\"good\", \"bad\") and i > 0:\n",
    "            return parts[i - 1]\n",
    "\n",
    "    # Fallback f√ºr unerwartete Pfadstrukturen\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b8325-9743-4042-a499-020887a43c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plottet Fehlklassifikationen nach Verschlussvariante\n",
    "# zeigt alle Varianten; auch die ohne Fehlklassifikationen\n",
    "def plot_misclassifications_by_variant(wrong_paths, all_variants):\n",
    "    # feste Referenz aller Varianten (auch solche ohne Fehler)\n",
    "    variants = list(all_variants)\n",
    "\n",
    "    # Trennung der Fehlklassifikationen nach Fehlertyp\n",
    "    # False Positives: good (0) -> bad (1)\n",
    "    good_as_bad = [p for (p, yt, yp) in wrong_paths if yt == 0 and yp == 1]\n",
    "    # False Negatives: bad (1) -> good (0)\n",
    "    bad_as_good = [p for (p, yt, yp) in wrong_paths if yt == 1 and yp == 0]\n",
    "\n",
    "    # Extraktion der Verschlussvariante aus dem Dateipfad\n",
    "    # None-Werte werden ignoriert (robuster gegen√ºber unerwarteten Pfadstrukturen)\n",
    "    fp_variants = [\n",
    "        v for v in (extract_variant_from_path(p) for p in good_as_bad)\n",
    "        if v is not None\n",
    "    ]\n",
    "    fn_variants = [\n",
    "        v for v in (extract_variant_from_path(p) for p in bad_as_good)\n",
    "        if v is not None\n",
    "    ]\n",
    "\n",
    "    # Z√§hlung der Fehlklassifikationen pro Variante\n",
    "    fp_counts = Counter(fp_variants)\n",
    "    fn_counts = Counter(fn_variants)\n",
    "\n",
    "    # Werte in fester Reihenfolge der Varianten aufbauen\n",
    "    fp_vals = [fp_counts.get(v, 0) for v in variants]\n",
    "    fn_vals = [fn_counts.get(v, 0) for v in variants]\n",
    "\n",
    "    x = np.arange(len(variants))\n",
    "    width = 0.4\n",
    "\n",
    "    display(Markdown(\"### **Fehlklassifikationen je Verschlussvariante**\"))\n",
    "\n",
    "    plt.figure(figsize=(max(12, len(variants) * 0.6), 5))\n",
    "    plt.bar(x - width / 2, fp_vals, width=width, label=\"Good ‚Üí Bad (FP)\")\n",
    "    plt.bar(x + width / 2, fn_vals, width=width, label=\"Bad ‚Üí Good (FN)\")\n",
    "\n",
    "    plt.xticks(x, variants, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Anzahl Fehlklassifikationen\")\n",
    "    plt.title(\"Fehlklassifikationen je Verschlussvariante (Test)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d63ba4-b454-4748-81cb-484b473ecf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibt bis zu k Elemente gleichm√§√üig √ºber die Liste verteilt zur√ºck\n",
    "# Ziel --> nach dem Training die Bilder der Fehlklassifizierungen analysieren\n",
    "# dazu sollen gleichm√§√üig √ºber alle Fehlklassifikationen verteilt die Bilder dargestellt werden, um eine gute √úbersicht der Fehlklassifizierungen zu erhalten\n",
    "def sample_evenly(items, k=16):\n",
    "    n = len(items)\n",
    "\n",
    "    # wenn die Liste k√ºrzer oder gleich lang ist wie k,\n",
    "    # gibt es keinen Grund zu sampeln --> die komplette Liste wird zur√ºckgegeben\n",
    "    if n <= k:\n",
    "        return items\n",
    "\n",
    "    # erzeugt k gleichm√§√üig verteilte Indizes von 0 bis n-1\n",
    "    # linspace gibt float-Wert zur√ºck; deshalb runden um in eine Ganzzahl umzuwandeln\n",
    "    idxs = np.linspace(0, n - 1, k).round().astype(int)\n",
    "\n",
    "    # R√ºckgabe der Elemente an den vorher ermittelten Positoinen\n",
    "    return [items[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0f982-8538-4fac-943a-8f7f826ce2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plottet Fehlklassifikationen als kompaktes Bild-Grid\n",
    "# Ziel: qualitative Analyse ausgew√§hlter Beispielbilder (gleichm√§√üig √ºber die Fehlerliste verteilt)\n",
    "def plot_misclassification_grid(paths, header, k=16, cols=4, figsize=(10, 4), return_sel=False):\n",
    "    display(Markdown(header))\n",
    "\n",
    "    # bis zu k Pfade ausw√§hlen (bevorzugt gleichm√§√üig verteilt, falls sample_evenly vorhanden ist)\n",
    "    if callable(globals().get(\"sample_evenly\")):\n",
    "        sel = sample_evenly(paths, k=k)\n",
    "    else:\n",
    "        sel = paths[:k]\n",
    "\n",
    "    # falls keine Beispiele vorhanden sind, Hinweis ausgeben und sauber abbrechen\n",
    "    if len(sel) == 0:\n",
    "        print(\"Keine Fehlklassifikationen in dieser Kategorie.\")\n",
    "        return [] if return_sel else None\n",
    "\n",
    "    # kurze Auflistung der gew√§hlten Beispielpfade\n",
    "    print(\"Beispielpfade (gleichm√§√üig √ºber die Fehler verteilt):\")\n",
    "    for p in sel:\n",
    "        print(p)\n",
    "\n",
    "    # Titel pro Bild: Verschlussvariante aus dem Pfad\n",
    "    if callable(globals().get(\"extract_variant_from_path\")):\n",
    "        titles = [extract_variant_from_path(p) for p in sel]\n",
    "    else:\n",
    "        titles = None\n",
    "\n",
    "    # Grid-Gr√∂√üe berechnen\n",
    "    n = len(sel)\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "\n",
    "    # Achsen robust als 2D-Array behandeln (Sonderf√§lle: 1 Zeile / 1 Spalte)\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(rows, 1)\n",
    "\n",
    "    # Bilder laden und plotten\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < n:\n",
    "            p = sel[i]\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                ax.imshow(img)\n",
    "            except Exception as e:\n",
    "                # falls ein Bild nicht geladen werden kann, wird die Fehlermeldung im Plot angezeigt\n",
    "                ax.text(0.5, 0.5, f\"Fehler\\n{e}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "            # optional Titel setzen (z.B. Verschlussvariante)\n",
    "            if titles is not None:\n",
    "                ax.set_title(titles[i], fontsize=9, pad=2)\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n",
    "    plt.show()\n",
    "\n",
    "    # ausgew√§hlte Pfade zur√ºckgeben (--> optional)\n",
    "    return sel if return_sel else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6b93d-6ee1-43f5-992b-cad79b2b1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiert Beispielbilder zu Fehlklassifikationen (w√§hrend dem Test)\n",
    "# getrennt nach Fehlertyp: Good -> Bad (False Positives) und Bad -> Good (False Negatives)\n",
    "def visualize_misclassification_examples(wrong_paths, k=16, cols=4, figsize=(10, 4)):\n",
    "    display(Markdown(\"## Details Fehlklassifizierung\"))\n",
    "\n",
    "    # Fehlertypen trennen (FP / FN)\n",
    "    good_as_bad = [p for (p, yt, yp) in wrong_paths if yt == 0 and yp == 1]  # False Positives\n",
    "    bad_as_good = [p for (p, yt, yp) in wrong_paths if yt == 1 and yp == 0]  # False Negatives\n",
    "\n",
    "    # Block 1: Good als Bad klassifiziert (False Positives)\n",
    "    plot_misclassification_grid(\n",
    "        good_as_bad,\n",
    "        header=\"### Good als Bad klassifiziert (False Positives)\",\n",
    "        k=k,\n",
    "        cols=cols,\n",
    "        figsize=figsize,\n",
    "    )\n",
    "\n",
    "    # Block 2: Bad als Good klassifiziert (False Negatives)\n",
    "    plot_misclassification_grid(\n",
    "        bad_as_good,\n",
    "        header=\"### Bad als Good klassifiziert (False Negatives)\",\n",
    "        k=k,\n",
    "        cols=cols,\n",
    "        figsize=figsize,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f0d9d-1d1a-4eef-9853-0e334322931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalisierung eines Bild-Tensors\n",
    "# macht ein zuvor normalisiertes Bild wieder \"anzeigbar\"\n",
    "# wird ausschlie√ülich f√ºr Visualisierung (z. B. Grad-CAM) genutzt\n",
    "# hat keinen Einfluss auf Training oder Inferenz\n",
    "def denormalize_tensor(x, mean, std):\n",
    "    # x kann entweder die Form\n",
    "    # [C, H, W] (ein einzelnes Bild)\n",
    "    # oder [1, C, H, W] (Batch mit genau einem Bild) haben\n",
    "    if x.dim() == 4:\n",
    "        x = x[0]  # Batch-Dimension entfernen\n",
    "\n",
    "    # mean und std in Tensoren umwandeln und\n",
    "    # auf die Bilddimensionen broadcast-f√§hig machen\n",
    "    # Form danach: [C, 1, 1]\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    # R√ºckrechnung der Normalisierung:\n",
    "    # normiertes Bild: (x - mean) / std\n",
    "    # Denormalisierung: x * std + mean\n",
    "    x = x * std + mean\n",
    "\n",
    "    # Werte auf den g√ºltigen Bereich [0, 1] begrenzen,\n",
    "    # damit die Darstellung als Bild korrekt funktioniert\n",
    "    return x.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b7714-15d8-44c3-9565-76c06b342293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM-Visualisierung f√ºr ein einzelnes Bild:\n",
    "# berechnet die Modellvorhersage\n",
    "# erzeugt eine Grad-CAM-Heatmap f√ºr einen definierten Layer\n",
    "# Denormalisierung nur f√ºr die Anzeige, nicht f√ºr das Modell\n",
    "def show_gradcam(\n",
    "    model,\n",
    "    x,                      # Eingabebild als Tensor [1, C, H, W] (bereits normalisiert)\n",
    "    target_layer,           # Layer, auf dem Grad-CAM berechnet werden soll\n",
    "    y_true=None,            # optionale Ground-Truth-Klasse (0/1), z. B. aus dem Dateipfad\n",
    "    class_idx=None,         # optionale Zielklasse f√ºr Grad-CAM (default: Vorhersage)\n",
    "    device=None,\n",
    "    class_names=(\"good\", \"bad\"),\n",
    "    mean=imagenet_mean,\n",
    "    std=imagenet_std,\n",
    "):\n",
    "    # Falls kein Device explizit √ºbergeben wurde,\n",
    "    # wird das Device direkt vom Input-Tensor √ºbernommen\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "\n",
    "    # Modell in den Evaluationsmodus versetzen\n",
    "    # (wichtig: Dropout aus, BatchNorm im Inferenzmodus)\n",
    "    model = model.to(device).eval()\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Grad-CAM ben√∂tigt Gradienten\n",
    "    # da im Notebook evtl. vorher torch.no_grad() oder inference_mode()\n",
    "    # aktiv war, erzwingen wir hier explizit einen grad-enabled Kontext.\n",
    "    with torch.enable_grad():\n",
    "\n",
    "        # x wird zu einem Leaf-Tensor gemacht, damit Gradienten sauber berechnet werden k√∂nnen\n",
    "        x_cam = x.detach().requires_grad_(True)\n",
    "\n",
    "        # Vorhersage berechnen\n",
    "        # bewusst NICHT in torch.no_grad(), da TorchCAM Hooks setzt\n",
    "        logits_pred = model(x_cam)\n",
    "        pred = int(logits_pred.argmax(dim=1).item())\n",
    "\n",
    "        # Falls keine Zielklasse explizit angegeben wurde, erkl√§ren wir standardm√§√üig die vorhergesagte Klasse\n",
    "        if class_idx is None:\n",
    "            class_idx = pred\n",
    "\n",
    "        # Grad-CAM-Extractor initialisieren -> setzt Forward- und Backward-Hooks auf target_layer\n",
    "        cam_extractor = GradCAM(model, target_layer=target_layer)\n",
    "\n",
    "        # Sicherheitshalber alte Gradienten l√∂schen, um Seiteneffekte aus vorherigen L√§ufen zu vermeiden\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Erneuter Forward-Pass; TorchCAM triggert intern den notwendigen Backward-Pass\n",
    "        logits = model(x_cam)\n",
    "        cams = cam_extractor(class_idx, logits)\n",
    "\n",
    "        # Da wir nur einen target_layer nutzen, nehmen wir das erste (und einzige) CAM\n",
    "        cam = cams[0].detach().cpu()\n",
    "\n",
    "        # Hooks nach der Nutzung entfernen, damit sie sich im Notebook nicht ansammeln\n",
    "        if hasattr(cam_extractor, \"remove_hooks\"):\n",
    "            cam_extractor.remove_hooks()\n",
    "            \n",
    "    # Fehler-Typ bestimmen (nur falls Ground Truth bekannt ist)\n",
    "    err_type = None\n",
    "    if y_true is not None:\n",
    "        y_true = int(y_true)\n",
    "        if y_true == 1 and pred == 0:\n",
    "            err_type = \"FN\"  # bad -> good\n",
    "        elif y_true == 0 and pred == 1:\n",
    "            err_type = \"FP\"  # good -> bad\n",
    "        elif y_true == 0 and pred == 0:\n",
    "            err_type = \"TN\"\n",
    "        elif y_true == 1 and pred == 1:\n",
    "            err_type = \"TP\"\n",
    "\n",
    "    # Eingabebild f√ºr die Anzeige denormalisieren\n",
    "    x_vis = denormalize_tensor(x.detach().cpu(), mean=mean, std=std)\n",
    "    img = to_pil_image(x_vis)\n",
    "\n",
    "    # Heatmap √ºber das Originalbild legen\n",
    "    overlay = overlay_mask(\n",
    "        img,\n",
    "        to_pil_image(cam, mode=\"F\"),\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "    # Titel f√ºr die Visualisierung erzeugen\n",
    "    pred_name = class_names[pred]\n",
    "    cls_name = class_names[class_idx] if class_idx in (0, 1) else str(class_idx)\n",
    "\n",
    "    if y_true is None:\n",
    "        # Fall: reine Erkl√§rung ohne bekannte Ground Truth\n",
    "        title_right = (\n",
    "            f\"Grad-CAM:\\n\"\n",
    "            f\"erkl. Klasse={cls_name}, Vorhersage={pred_name}\"\n",
    "        )\n",
    "    else:\n",
    "        # Fall: Ground Truth vorhanden (TP, FP, FN, TN)\n",
    "        true_name = class_names[y_true]\n",
    "        title_right = (\n",
    "            f\"Grad-CAM: {err_type}; tat. Klasse={true_name},\\n\"\n",
    "            f\"Vorhersage={pred_name}, erkl. Klasse={cls_name}\"\n",
    "        )\n",
    "\n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Input\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(title_right)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(overlay)\n",
    "    plt.show()\n",
    "\n",
    "    return pred, class_idx, err_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6281526-001b-49c1-a65a-029036988957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper-Funktion f√ºr Grad-CAM:\n",
    "# L√§dt ein Bild direkt vom Dateipfad, wendet die Test-Preprocessing-Schritte an\n",
    "# und ruft anschlie√üend die Grad-CAM-Analyse auf\n",
    "def show_gradcam_from_path(\n",
    "    model,\n",
    "    img_path,\n",
    "    target_layer,\n",
    "    transform_step1,\n",
    "    transform_test_step2,\n",
    "    y_true=None,\n",
    "    class_idx=None,\n",
    "    device=None,\n",
    "    class_names=(\"good\", \"bad\"),\n",
    "    mean=imagenet_mean,\n",
    "    std=imagenet_std,\n",
    "):\n",
    "    # Ground Truth bestimmen (falls nicht explizit √ºbergeben)\n",
    "    # Ableitung aus der Ordnerstruktur: .../good/... -> 0, .../bad/... -> 1\n",
    "    if y_true is None:\n",
    "        p = str(img_path).replace(\"\\\\\", \"/\")\n",
    "        if \"/good/\" in p:\n",
    "            y_true = 0\n",
    "        elif \"/bad/\" in p:\n",
    "            y_true = 1\n",
    "        else:\n",
    "            y_true = None\n",
    "\n",
    "    # Bild vom Dateipfad laden\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # exakt gleiche Vorverarbeitung wie im Test (keine Augmentation)\n",
    "    x = transform_step1(img)\n",
    "    x = transform_test_step2(x)\n",
    "\n",
    "    # Batch-Dimension erg√§nzen: [C,H,W] -> [1,C,H,W]\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    # √úbergabe an die eigentliche Grad-CAM-Funktion\n",
    "    return show_gradcam(\n",
    "        model,\n",
    "        x,\n",
    "        target_layer=target_layer,\n",
    "        y_true=y_true,\n",
    "        class_idx=class_idx,\n",
    "        device=device,\n",
    "        class_names=class_names,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e22768-72b6-47b6-bc6b-b2fd74777906",
   "metadata": {},
   "source": [
    "## 4.3 Trainings- und Evaluationspipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14fb803-8f7d-47b0-a3c7-5580e5385be3",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die Trainings- und Evaluationspipeline beschrieben.\n",
    "Die Implementierung ist in zwei zentrale Funktionen gegliedert: *run_one_epoch*, welche die Ausf√ºhrung einer einzelnen Epoche f√ºr Training, Validierung oder Test kapselt, sowie *train_model*, welche den vollst√§ndigen Trainingsprozess √ºber mehrere Epochen hinweg steuert. Dabei werden das Modelltraining, die Berechnung relevanter Metriken, die Auswahl des besten Modells auf Basis der Validierung sowie die abschlie√üende Evaluation auf dem Testdatensatz systematisch umgesetzt.\n",
    "\n",
    "F√ºr das Training wird die Kreuzentropie-Verlustfunktion in Kombination mit dem AdamW-Optimierer verwendet. Die Kreuzentropie eignet sich f√ºr das vorliegende Klassifikationsproblem, da sie Fehlklassifikationen direkt auf Basis der vorhergesagten Klassenwahrscheinlichkeiten bewertet und eine stabile Optimierung bei bin√§ren Entscheidungsaufgaben erm√∂glicht. Der AdamW-Optimierer wurde gew√§hlt, da er die Gewichtsnormierung (Weight Decay) von der Gradientenoptimierung entkoppelt und sich in der Praxis als robuste Regularisierungsmethode mit guter Generalisierungsf√§higkeit erwiesen hat.  \n",
    "\n",
    "Die Modellbewertung erfolgt pro Epoche auf dem Validierungsdatensatz anhand von Accuracy, Loss und False-Positive-Rate, wobei das jeweils beste Modell f√ºr die abschlie√üende Evaluation auf dem Testdatensatz ausgew√§hlt wird.\n",
    "\n",
    "Die beschriebene Pipeline wird einheitlich f√ºr alle √ºberwachten Trainingsszenarien verwendet. Sowohl die selbst konzipierte CNN-Architektur als auch das vortrainierte ResNet-Modell werden mit derselben Pipeline trainiert und evaluiert. Dadurch wird eine konsistente und vergleichbare Bewertung der unterschiedlichen Modellans√§tze sichergestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458aa81-8736-44f4-b283-fd6c429f8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Durchf√ºhrung genau einer Epoche (Training, Validierung oder Test)\n",
    "def run_one_epoch(model, loader, optimizer, criterion, device, train=True, show_pbar=False, epoch_idx=None, num_epochs=None):\n",
    "    # Train-/Eval-Modus setzen, um u.a. Dropout und BatchNorm zu steuern\n",
    "    model.train() if train else model.eval()\n",
    "\n",
    "    # Metriken-Tracker f√ºr die Epoche initialisieren\n",
    "    epoch_loss = 0.0  # z√§hlt den Loss √ºber die gesamte Epoche mit\n",
    "    correct = 0       # z√§hlt die Anzahl der richtig klassifizierten Bilder √ºber die gesamte Epoche mit\n",
    "    total = 0         # z√§hlt, wie viele Bilder in der Epoche verarbeitet wurden\n",
    "\n",
    "    # Listen initialisieren, um w√§hrend der Epoche die echten Labels sowie die Vorhersagen zu sammeln\n",
    "    # zus√§tzlich werden die Pfade aller Fehlklassifizierungen gespeichert, um diese sp√§ter analysieren zu k√∂nnen\n",
    "    y_true_all = []   # speichert alle echten Labels der Epoche\n",
    "    y_pred_all = []   # speichert alle Vorhersagen der Epoche\n",
    "    wrong_paths = []  # Liste von Tupeln (Dateipfad, echtes Label, Vorhersage)\n",
    "\n",
    "    iterator = loader\n",
    "\n",
    "    # Fortschrittsbalken anzeigen\n",
    "    if show_pbar:\n",
    "        if epoch_idx is not None and num_epochs is not None:\n",
    "            desc = f\"Train (Epoche {epoch_idx}/{num_epochs})\" if train else f\"Val/Test (Epoche {epoch_idx}/{num_epochs})\"\n",
    "        else:\n",
    "            desc = \"Train\" if train else \"Val/Test\"\n",
    "        iterator = tqdm(loader, desc=desc, unit=\"batch\", leave=False)\n",
    "\n",
    "    # Gradiententracking je nach Phase einstellen (Training --> an; Val/Test --> aus)\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch in iterator:\n",
    "            # DataLoader liefert: x --> Tensor (Bilddaten), y --> Label, paths --> Dateipfade als Strings\n",
    "            x, y, paths = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            if train:\n",
    "                # Gradientenspeicher leeren, damit sich die Gradienten nicht aufaddieren\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Vorw√§rtsdurchlauf\n",
    "            logits = model(x)\n",
    "\n",
    "            # Loss berechnen\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            if train:\n",
    "                # Backpropagation und Optimizer-Step w√§hrend dem Training; nicht bei Validierung & Test\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Loss aufsummieren (nach Batchgr√∂√üe gewichtet)\n",
    "            epoch_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # Klassen-Labels (Vorhersage) aus den Logits ableiten\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Anzahl der richtig klassifizierten Bilder hochz√§hlen\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "            # Daten f√ºr sp√§tere Auswertungen auf die CPU-Listen schieben\n",
    "            y_cpu = y.detach().cpu()\n",
    "            preds_cpu = preds.detach().cpu()\n",
    "            y_true_all.extend(y_cpu.numpy().tolist())\n",
    "            y_pred_all.extend(preds_cpu.numpy().tolist())\n",
    "\n",
    "            # Fehlklassifikationen sammeln (Pfad, Ist-Label, Vorhersage)\n",
    "            for pth, yt, yp in zip(paths, y_cpu.numpy(), preds_cpu.numpy()):\n",
    "                if int(yt) != int(yp):\n",
    "                    wrong_paths.append((pth, int(yt), int(yp)))\n",
    "\n",
    "            # Live-Update im Balken pro Batch (durchschnittlicher Loss & Accuracy bisher)\n",
    "            if show_pbar and total > 0:\n",
    "                avg_loss_so_far = epoch_loss / total\n",
    "                acc_so_far = correct / total\n",
    "                iterator.set_postfix({\n",
    "                    \"loss\": f\"{avg_loss_so_far:.4f}\",\n",
    "                    \"acc\": f\"{acc_so_far:.3f}\",\n",
    "                })\n",
    "\n",
    "    # Epochenmetriken normalisieren, indem durch die Anzahl der Samples geteilt wird\n",
    "    avg_loss = epoch_loss / total if total > 0 else 0.0\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # Konfusionsmatrix und False-Positive-Rate aus den gesammelten Labels berechnen\n",
    "    cm = create_confusion_matrix(y_true_all, y_pred_all, num_classes=2)\n",
    "    fpr = compute_fpr_from_cm(cm)\n",
    "\n",
    "    # R√ºckgabe: Loss, Accuracy, False-Positive-Rate, Konfusionsmatrix und Pfade der Fehlklassifizierungen\n",
    "    return avg_loss, acc, fpr, cm, wrong_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845d577-abb1-4052-8735-f759679c1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, num_epochs=15, lr=1e-3, device=None,):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dictionary mit div. Listen um den Verlauf der Metriken √ºber die verschiedenen Epochen zu speichern\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_fpr\": [],\n",
    "    }\n",
    "\n",
    "    # Variablen initialisieren, um w√§hrend dem Training das beste Modell zu speichern\n",
    "    best_state_dict = None\n",
    "    best_val_loss = float(\"inf\")  # initialisiert die Variable mit unendlich gro√üem Loss; stellt sicher, dass jede reale Loss-Zahl kleiner ist\n",
    "    best_epoch = None\n",
    "    found_acceptable_fpr = False  # wird ben√∂tigt, falls keines der Modelle unter die 5%-Schwelle bei der False-Positive-Rate kommt\n",
    "    fpr_threshold = 0.05          # Schwelle f√ºr die False-Positive-Rate (5% als Anteil)\n",
    "\n",
    "    # Fortschrittsbalken √ºber die Epochen anzeigen\n",
    "    pbar = tqdm(range(1, num_epochs + 1), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "    # √ºber alle Epochen iterieren\n",
    "    for epoch in pbar:\n",
    "        # speichert den aktuellen Zeitpunkt, damit gemessen werden kann, wie lange eine Epoche dauert\n",
    "        t0 = time.time()\n",
    "\n",
    "        # pro Epoche --> Training auf dem Trainings-Datensatz\n",
    "        train_loss, train_acc, _, _, _ = run_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            train=True,\n",
    "            show_pbar=True,\n",
    "            epoch_idx=epoch,\n",
    "            num_epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        # pro Epoche --> Validierung auf dem Validierungs-Datensatz\n",
    "        val_loss, val_acc, val_fpr, _, _ = run_one_epoch(\n",
    "            model,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            train=False,\n",
    "            show_pbar=True,\n",
    "            epoch_idx=epoch,\n",
    "            num_epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        # pro Epoche werden die Metriken in dem history-Dictionary gespeichert\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_fpr\"].append(val_fpr)\n",
    "\n",
    "        # Speichert das beste Modell anhand der vorher initialisierten Variablen\n",
    "        # Ziel: bevorzugt ein Modell mit False-Positive-Rate (Validation) < 5.0% (Schwelle), ansonsten bestes Modell nach Val-Loss\n",
    "        if val_fpr < fpr_threshold:\n",
    "            # sobald ein Modell die FPR-Schwelle schafft, werden nur noch Modelle ber√ºcksichtigt, die ebenfalls unter der Schwelle liegen\n",
    "            if (not found_acceptable_fpr) or (val_loss < best_val_loss):\n",
    "                found_acceptable_fpr = True\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            # solange kein Modell die FPR-Schwelle geschafft hat, wird das beste Modell nach Val-Loss gespeichert\n",
    "            if (not found_acceptable_fpr) and (val_loss < best_val_loss):\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        # beendet die Zeitmessung und dt enth√§lt die ben√∂tigte Zeit der gesamten Epoche\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        # Live-Anzeige der Metriken direkt im Balken\n",
    "        pbar.set_postfix({\n",
    "            \"tr_loss\": f\"{train_loss:.4f}\",\n",
    "            \"tr_acc\": f\"{train_acc:.3f}\",\n",
    "            \"val_loss\": f\"{val_loss:.4f}\",\n",
    "            \"val_acc\": f\"{val_acc:.3f}\",\n",
    "            \"val_fpr%\": f\"{val_fpr*100:.2f}\",\n",
    "            \"s/ep\": f\"{dt:.0f}\",\n",
    "        })\n",
    "\n",
    "\n",
    "    # nach Abschluss aller Trainings- / Validierungs-Epochen wird das beste Modell wiederhergestellt und anschlie√üend zum Testen verwendet\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "    # Testdurchlauf mit dem besten Modell\n",
    "    test_loss, test_acc, test_fpr, test_cm, wrong_paths_test = run_one_epoch(\n",
    "        model,\n",
    "        test_loader,\n",
    "        optimizer=None,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        train=False,\n",
    "        show_pbar=True,\n",
    "        epoch_idx=best_epoch,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    # Anzeige der Testergebnisse\n",
    "    print(f\"Bestes Modell aus Epoche: {best_epoch} (Val-Loss={best_val_loss:.4f})\")\n",
    "    print(f\"loss={test_loss:.4f}, acc={test_acc:.3f}, fpr={test_fpr*100:.2f}%\")\n",
    "\n",
    "    return model, history, test_cm, wrong_paths_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627eecf-fbd0-4d50-8a93-5ad447006da1",
   "metadata": {},
   "source": [
    "## 4.4 Experimente mit dem CNN von Grund auf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5551672-9deb-4d52-850e-45ba8da64a84",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden mehrere parametrisierte Varianten eines von Grund auf implementierten CNNs untersucht. Dabei werden architektonische Anpassungen sowie ausgew√§hlte Trainingsparameter variiert, um deren Einfluss auf das Trainingsverhalten und die Klassifikationsleistung systematisch zu analysieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb566d-d39c-4761-b71e-52f8d43daf1b",
   "metadata": {},
   "source": [
    "### 4.4.1 Baseline-Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a0a54-a4a1-4951-816e-a4cbf1db2425",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die Baseline-Architektur definiert, die als Ausgangspunkt f√ºr alle nachfolgenden Experimente mit dem von Grund auf entwickelten CNN dient. Ziel ist es, eine klar strukturierte und bewusst einfach gehaltene Referenzarchitektur festzulegen, auf deren Basis einzelne architektonische Entscheidungen systematisch variiert und bewertet werden k√∂nnen.\n",
    "\n",
    "Das Baseline-Modell folgt einem klassischen CNN-Aufbau, bestehend aus einem Feature-Extraktionsbereich (Backbone) und einem nachgelagerten Klassifikationskopf (Head). Der Backbone setzt sich aus mehreren aufeinanderfolgenden Convolution-Bl√∂cken zusammen, die jeweils aus Faltung, Batch Normalization, ReLU-Aktivierung und Max-Pooling bestehen. Dabei wird die r√§umliche Aufl√∂sung der Feature-Maps schrittweise reduziert, w√§hrend die Anzahl der Kan√§le zunimmt.\n",
    "\n",
    "Als Klassifikationskopf wird in der Baseline ein vollvermaschtes Netz verwendet. Um eine feste Feature-Dimension unabh√§ngig von der Eingangsbildgr√∂√üe zu gew√§hrleisten, kommt Adaptive Average Pooling zum Einsatz, gefolgt von einem mehrschichtigen vollvermaschten Netz. Diese klare Trennung zwischen Backbone und Head erm√∂glicht es, in sp√§teren Experimenten gezielt entweder die Merkmalextraktion oder den Klassifikationskopf zu ver√§ndern, ohne die gesamte Architektur neu entwerfen zu m√ºssen.\n",
    "\n",
    "Die folgende Abbildung dient der konzeptionellen Veranschaulichung der Baseline-Architektur. Sie zeigt den grundlegenden Aufbau des Netzes sowie die Entwicklung der Tensorformen entlang des Vorw√§rtsdurchlaufs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a7ba4-0651-4981-9300-392c96af9cb0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Architektur_Basismodell.png\" width=\"1000\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698eab6f-5f3f-4aa3-9663-bea66723e83a",
   "metadata": {},
   "source": [
    "Die beschriebene Baseline-Architektur wird im Folgenden als parametrisierbares CNN implementiert und bildet die Referenz f√ºr s√§mtliche nachfolgenden Architekturvarianten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ac226-6130-46a2-9a36-f10d42d96c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basis = BaselineCNN(\n",
    "    channels=(32, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"fc\",\n",
    "    fc_hidden=256,\n",
    "    dropout=0.0,\n",
    "    adaptive_pool_out=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78726378-16e1-4fe2-a8a0-e30e74d992da",
   "metadata": {},
   "source": [
    "Die folgende tabellarische √úbersicht fasst die implementierte Architektur nochmals zusammen und erg√§nzt die grafische Darstellung um zus√§tzliche technische Details, insbesondere die Anzahl der trainierbaren Parameter pro Schicht sowie eine Absch√§tzung des Speicherbedarfs w√§hrend des Vorw√§rts- und R√ºckw√§rtsdurchlaufs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bf042-63a3-4eea-8205-d0cc44616d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basis = model_basis.to(device)\n",
    "\n",
    "summary(\n",
    "    model_basis,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57c059-1aa1-46ea-834e-c2781752468b",
   "metadata": {},
   "source": [
    "Im n√§chsten Schritt wird das Baseline-Modell f√ºr 30 Epochen trainiert.\n",
    "Dabei erfolgt das Training auf dem Trainingsdatensatz, die Modellauswahl auf Basis der Validierungsdaten und die abschlie√üende Bewertung auf dem Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a539b7-cf36-43fb-acb6-7f58f1ca0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_basis, hist_basis, cm_basis, wrong_basis = train_model(\n",
    "    model_basis,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa5901-dc32-4b3d-b416-a88892affc24",
   "metadata": {},
   "source": [
    "Das bereits trainierte Modell sowie die zugeh√∂rigen Auswertungsartefakte (Trainingsverlauf, Fehlklassifikationen und Konfusionsmatrix) k√∂nnen anschlie√üend √ºber den folgenden Code geladen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdf5a7-776a-44cd-9ea1-2039b8c450e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell laden\n",
    "\n",
    "# Basis-Pfade f√ºr Modell und Auswertungsartefakte\n",
    "model_path = \"Modelle/BasicCNN/model_basis.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "# Modell-Gewichte laden\n",
    "model_basis.load_state_dict(torch.load(model_path, map_location=device))  # Laden der gespeicherten Gewichte\n",
    "\n",
    "model_basis.to(device)   # Modell auf GPU / CPU verschieben\n",
    "model_basis.eval()       # Eval-Modus aktivieren (deaktiviert z.B. Dropout)\n",
    "\n",
    "\n",
    "# Gespeicherte Ausewrtungen laden\n",
    "# Trainingsverlauf (Loss, Accuracy etc.)\n",
    "# weights_only=False notwendig, da kein reines State-Dict\n",
    "hist_basis = torch.load(os.path.join(eval_base_path, \"HIST/hist_basis.pth\"),weights_only=False)\n",
    "\n",
    "# Liste der Fehlklassifikationen \n",
    "wrong_basis = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_basis.pth\"),weights_only=False)\n",
    "\n",
    "# Confusion Matrix (als numpy-Array gespeichert)\n",
    "cm_basis = np.load(os.path.join(eval_base_path, \"CM/cm_basis.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong + CM erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7b8b5-1179-47cf-ad1d-4b945aec3dc8",
   "metadata": {},
   "source": [
    "Im Folgenden wird der Trainingsverlauf analysiert, indem die Entwicklung zentraler Metriken √ºber die einzelnen Epochen hinweg betrachtet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98facb69-3540-4ffb-b1c1-00da0a855dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6fe85-7c25-4e32-9cbe-02db2573dc0f",
   "metadata": {},
   "source": [
    "Die Trainingsverl√§ufe zeigen, dass das Baseline-Modell bereits nach wenigen Epochen eine hohe Klassifikationsleistung erreicht. Die Trainingsgenauigkeit steigt kontinuierlich an und n√§hert sich fr√ºhzeitig einem Wert nahe 100 %, w√§hrend auch die Validierungsgenauigkeit insgesamt auf einem sehr hohen Niveau liegt.\n",
    "\n",
    "Der Trainings-Loss nimmt √ºber die Epochen hinweg stabil ab, was auf eine kontinuierliche Optimierung der Modellparameter hindeutet. Der Validierungs-Loss folgt diesem Trend grunds√§tzlich, weist jedoch einzelne deutliche Ausrei√üer auf. Diese tempor√§ren Peaks sind vermutlich auf einzelne Fehlklassifikationen bei vergleichsweise kleiner Validierungsmenge zur√ºckzuf√ºhren und deuten nicht auf ein systematisches √úberanpassen hin, da sich die Werte in den folgenden Epochen wieder stabilisieren.\n",
    "\n",
    "Auff√§llig ist die teils stark schwankende False-Positive-Rate in der Validierung. Einzelne Epochen zeigen deutliche Ausschl√§ge, w√§hrend die FPR in den meisten Epochen sehr niedrig bleibt.  \n",
    "\n",
    "Insgesamt zeigt das Baseline-Modell ein stabiles Lernverhalten mit schneller Konvergenz und hoher Generalisierungsleistung, trotz vereinzelter Schwankungen in einzelnen Metriken.  \n",
    "\n",
    "W√§hrend die bisherigen Darstellungen das Lernverhalten w√§hrend des Trainingsprozesses zeigen, gibt die folgende Konfusionsmatrix Aufschluss √ºber die finale Modellleistung auf dem unabh√§ngigen Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb6d34-beee-4a3c-94eb-0c812d539caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ff783-013e-45f9-9401-e6d8ad0361f2",
   "metadata": {},
   "source": [
    "Das Baseline-Modell erreicht auf dem Testdatensatz eine Accuracy von 99,79 % und √ºbertrifft damit deutlich die geforderte Mindestgenauigkeit von 97 %. Auch die False-Positive-Rate liegt mit 0,19 % auf sehr niedrigem Niveau, unterschreitet jedoch die Zielvorgabe von < 0,1 % knapp nicht. Insgesamt liefert bereits das erste, bewusst einfach gehaltene Referenzmodell eine sehr starke Klassifikationsleistung und bildet damit eine belastbare Ausgangsbasis f√ºr die nachfolgenden Architekturvarianten.  \n",
    "\n",
    "Im n√§chsten Schritt werden die aufgetretenen Fehlklassifikationen differenziert nach Verschlussvariante analysiert, um m√∂gliche systematische Muster oder besonders herausfordernde Varianten zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d651864-7cdc-4b1b-9682-cc089dd6f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_basis, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fae3d2-0ee8-4bef-b22e-ae539384190b",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen treten nur bei wenigen spezifischen Verschlussvarianten auf und sind insgesamt sehr gering. Auff√§llig ist insbesondere die Variante river_cola_black mit mehreren False Positives sowie voesl_zitrone mit mehreren False Negatives. Die meisten Varianten werden hingegen fehlerfrei klassifiziert, was die insgesamt hohe Robustheit des Baseline-Modells unterstreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0315e-e8b2-4827-9d08-e6ef4833ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e3032-f31f-48f0-952a-6ae43cd89171",
   "metadata": {},
   "source": [
    "Die qualitative Analyse der Fehlklassifikationen zeigt, dass die Fehler √ºberwiegend bei wenigen spezifischen Varianten auftreten. Insbesondere bei river_cola_black sind mehrere False Positives zu beobachten. Bei dieser Variante zeigen die falsch klassifizierten Bilder ausschlie√ülich den Flaschenhals, ohne den Sicherungsring abzubilden. Die Fehlentscheidungen lassen sich daher plausibel auf Einschr√§nkungen im gew√§hlten Bildausschnitt beziehungsweise in der Bildaufnahme zur√ºckf√ºhren und weniger auf eine grunds√§tzlich fehlerhafte Modellentscheidung.\n",
    "\n",
    "Bei topsport_green treten vermehrt False Negatives auf, was darauf hindeutet, dass bestimmte visuelle Merkmale wie Kontrastverh√§ltnisse oder Reflexionen die Klassifikationsentscheidung beeinflussen k√∂nnen.\n",
    "\n",
    "Diese Beobachtungen unterstreichen die hohe Bedeutung qualitativ hochwertiger und konsistenter Trainings- und Testdaten, die den relevanten Inspektionsbereich zuverl√§ssig erfassen.\n",
    "\n",
    "Im n√§chsten Schritt werden diese Einzelf√§lle gezielt mithilfe von Grad-CAM analysiert, um zu untersuchen, welche Bildbereiche das Modell tats√§chlich zur Entscheidungsfindung heranzieht und ob der Sicherungsring in diesen F√§llen √ºberhaupt als relevantes Merkmal ber√ºcksichtigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fb343-3744-4459-b11d-db39fd9c8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_basis = model_basis.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11233f60-f9b0-4d01-859c-c64ef3ed11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_basis_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/topsport_green/bad/Cam2Side_0000000083.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_model_basis_gc1,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b3555-2bb1-459e-a8e0-36b5903a30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_basis_gc3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/sprite_green/bad/Cam2Top_0000000044.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_model_basis_gc3,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590a962-f106-4d87-833a-f3dc70dbf356",
   "metadata": {},
   "source": [
    "Die Visualisierungen zeigen, dass das Modell seine Aufmerksamkeit √ºberwiegend auf sichtbare Kanten- und Strukturmerkmale im Bereich des Verschlusses legt. In den betrachteten False-Negative-F√§llen konzentriert sich die Aktivierung jedoch nicht eindeutig auf den Sicherungsring, sondern teilweise auf angrenzende Strukturen oder kontrastreiche Bereiche. Im ersten analysierten Beispiel ist der Verschluss zudem im relevanten Bereich leicht deformiert, wobei dieser deformierte Abschnitt in der Grad-CAM nicht als besonders aktiv hervorgehoben wird. Dies liefert eine plausible Erkl√§rung f√ºr die Fehlklassifikation und deutet darauf hin, dass visuelle Artefakte, Beleuchtung oder Perspektive die Entscheidungsfindung beeinflussen k√∂nnen.\n",
    "\n",
    "Insgesamt best√§tigen die Grad-CAM-Analysen, dass das Modell grunds√§tzlich relevante Regionen ber√ºcksichtigt, jedoch in Einzelf√§llen nicht eindeutig zwischen sicherungsringrelevanten Merkmalen und √§hnlichen Strukturen differenziert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d7827-f136-4897-833f-50349a84ca52",
   "metadata": {},
   "source": [
    "### 4.4.2 Einfluss der deterministischen Bildvorverarbeitung (Padding vs. Verzerrung)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8e6d6-18f5-44b1-8b70-8da93ad06150",
   "metadata": {},
   "source": [
    "Aufbauend auf dem Training des Baseline-Modells wird in diesem Abschnitt als erster Vergleich der Einfluss der deterministischen Bildvorverarbeitung untersucht. Dabei bleiben die Modellarchitektur sowie die Anzahl der trainierbaren Parameter unver√§ndert, sodass beobachtete Leistungsunterschiede ausschlie√ülich auf die verwendete Vorverarbeitungsstrategie zur√ºckzuf√ºhren sind.\n",
    "\n",
    "Ziel dieses Tests ist es zu analysieren, ob und in welchem Umfang die geometrische Behandlung der Eingabebilder die Modellleistung beeinflusst. Da die deterministische Bildvorverarbeitung die Grundlage aller nachfolgenden Experimente bildet, wird dieser Vergleich bewusst fr√ºh im Experimentablauf durchgef√ºhrt, um die weiteren Modellvariationen auf einer konsistenten und fundierten Datenrepr√§sentation aufzubauen.  \n",
    "\n",
    "Im folgenden Code wird das Modell mit identischer Architektur instanziiert und anschlie√üend mit denselben Trainingsparametern wie das zuvor betrachtete Baseline-Modell trainiert, wobei sich ausschlie√ülich die verwendeten Trainings-, Validierungs- und Testdaten aufgrund der abweichenden deterministischen Vorverarbeitung unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1cd9e-6f65-4510-bad7-7911b4d2c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_basis_distort = BaselineCNN(\n",
    "    channels=(32, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"fc\",\n",
    "    fc_hidden=256,\n",
    "    dropout=0.0,\n",
    "    adaptive_pool_out=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18498ce9-d76b-4136-a4a0-4e1a39ba7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_basis_distort, hist_basis_distort, cm_basis_distort, wrong_basis_distort = train_model(\n",
    "    model_basis_distort,\n",
    "    train_loader_std_distort,\n",
    "    val_loader_std_distort,\n",
    "    test_loader_std_distort,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517a1e6-3b6a-4514-8ae6-03fda13db72e",
   "metadata": {},
   "source": [
    "Zur Auswertung des Experiments wird im Folgenden das trainierte Modell sowie die zugeh√∂rigen Auswertungsartefakte geladen, um die Ergebnisse reproduzierbar analysieren zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222dedc-eea6-4e3f-9427-3120f3eab617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell laden\n",
    "\n",
    "# Basis-Pfade f√ºr Modell und Auswertungsartefakte\n",
    "model_path = \"Modelle/BasicCNN/model_basis_distort.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "# Modell-Gewichte laden\n",
    "model_basis_distort.load_state_dict(torch.load(model_path, map_location=device))  # Laden der gespeicherten Gewichte\n",
    "\n",
    "model_basis_distort.to(device)   # Modell auf GPU / CPU verschieben\n",
    "model_basis_distort.eval()       # Eval-Modus aktivieren (deaktiviert z.B. Dropout)\n",
    "\n",
    "# Gespeicherte Ausewrtungen laden\n",
    "# Trainingsverlauf (Loss, Accuracy etc.)\n",
    "# weights_only=False notwendig, da kein reines State-Dict\n",
    "hist_basis_distort = torch.load(os.path.join(eval_base_path, \"HIST/hist_basis_distort.pth\"),weights_only=False)\n",
    "\n",
    "# Liste der Fehlklassifikationen \n",
    "wrong_basis_distort = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_basis_distort.pth\"),weights_only=False)\n",
    "\n",
    "# Confusion Matrix (als numpy-Array gespeichert)\n",
    "cm_basis_distort = np.load(os.path.join(eval_base_path, \"CM/cm_basis_distort.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong + CM erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46d5e8-4fa0-4b9f-861a-1046db7f13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_basis_distort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c79165-c141-4931-88db-70720d9c4db3",
   "metadata": {},
   "source": [
    "Im Vergleich zum Baseline-Modell mit Padding zeigt das Modell bei verzerrter Bildvorverarbeitung ebenfalls eine schnelle Konvergenz und erreicht insgesamt eine hohe Trainings- und Validierungsgenauigkeit. Der Trainings-Loss nimmt kontinuierlich ab, w√§hrend der Validierungs-Loss trotz einzelner Ausrei√üer insgesamt stabil auf niedrigem Niveau verbleibt.\n",
    "\n",
    "Auff√§llig sind jedoch st√§rkere Schwankungen in der Validierungs-FPR, insbesondere in den fr√ºhen Epochen. Dies deutet darauf hin, dass die geometrische Verzerrung der Bilder die Entscheidungsgrenzen des Modells sensibler beeinflusst als die Padding-Variante. Trotz der insgesamt hohen Accuracy scheint die verzerrte Darstellung die Stabilit√§t der Fehlklassifikationsrate etwas zu beeintr√§chtigen.\n",
    "\n",
    "Insgesamt best√§tigt der Verlauf, dass das Modell auch unter verzerrter Vorverarbeitung leistungsf√§hig ist, jedoch potenziell empfindlicher auf geometrische Ver√§nderungen der Eingabebilder reagiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed36f0-08f6-45f1-8231-e24539b24717",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_basis_distort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b11cc1-a0cf-49b3-8253-2a940428df9f",
   "metadata": {},
   "source": [
    "Im Testdatensatz erreicht das Modell mit verzerrter Bildvorverarbeitung eine Accuracy von 99,69 % und liegt damit nahezu auf dem Niveau des Baseline-Modells mit Padding, das 99,79 % erzielte. Die False-Positive-Rate steigt jedoch von 0,19 % auf 0,54 % an. W√§hrend die Gesamtgenauigkeit somit nur minimal abnimmt, zeigt sich eine sp√ºrbare Verschlechterung hinsichtlich der Fehlklassifikationen der Klasse good als bad.\n",
    "\n",
    "Die Unterschiede zwischen beiden Varianten sind insgesamt gering und bewegen sich auf sehr hohem Leistungsniveau. Angesichts der geringen absoluten Fehlanzahl ist zudem zu ber√ºcksichtigen, dass sich bei anderem Initialisierungs-Seed leichte Verschiebungen ergeben k√∂nnten. Dennoch zeigt die Padding-Variante in diesem Experiment eine konsistent niedrigere False-Positive-Rate und damit die stabilere Leistung im Hinblick auf das zentrale Optimierungskriterium.\n",
    "\n",
    "Da insbesondere die Minimierung von False Positives ein wesentliches Ziel der Anwendung darstellt, wird f√ºr die nachfolgenden Experimente die Resize-und-Padding-Strategie beibehalten. Die verzerrte Vorverarbeitung liefert ebenfalls sehr gute Ergebnisse, bietet jedoch keinen erkennbaren Vorteil gegen√ºber der referenzierten Padding-Variante und wird daher nicht weiterverfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090396d-9a8a-4939-9e0b-16ba2ab49517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_basis_distort, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152f415-a0a3-41a8-bc88-c969bd44f510",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen verteilen sich auch bei der verzerrten Vorverarbeitung auf mehrere Verschlussvarianten. Besonders auff√§llig ist eine erh√∂hte Anzahl an False Positives bei river_cola_black sowie einzelne Fehler bei weiteren Varianten.\n",
    "\n",
    "Auch bei der Resize-und-Padding-Strategie traten Fehlklassifikationen bei mehreren Varianten auf, sodass kein grunds√§tzlich neues Fehlermuster erkennbar ist. Allerdings zeigt sich bei der verzerrten Darstellung eine st√§rkere H√§ufung von Good‚ÜíBad-Fehlklassifikationen, was die erh√∂hte False-Positive-Rate best√§tigt.\n",
    "\n",
    "Insgesamt bleibt die absolute Fehleranzahl gering, jedoch wirkt die Padding-Variante im direkten Vergleich etwas robuster und stabiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb3f1f-92de-47bb-a9ad-51a8338bdac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_basis_distort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c692a-e996-41d6-85ca-bfe13c5b5887",
   "metadata": {},
   "source": [
    "Die Detailanalyse best√§tigt, dass ein Gro√üteil der False Positives bei river_cola_black auf Bilder zur√ºckzuf√ºhren ist, in denen lediglich der Flaschenhals sichtbar ist, w√§hrend der relevante Sicherungsring nicht im Bildausschnitt enthalten ist. Die Fehlklassifikation ist daher plausibel durch eine unvollst√§ndige Abbildung des Inspektionsbereichs erkl√§rbar.\n",
    "\n",
    "Zus√§tzlich zeigen viele der fehlklassifizierten river_cola_black-Bilder starke Kontrast- und Beleuchtungseffekte, wodurch feine Strukturen nur eingeschr√§nkt erkennbar sind. Dies deutet darauf hin, dass neben der geometrischen Verzerrung insbesondere Aufnahmebedingungen und Bildqualit√§t einen ma√ügeblichen Einfluss auf die Modellentscheidung haben.  \n",
    "\n",
    "Insgesamt zeigt die Analyse, dass die Fehlklassifikationen weniger auf grunds√§tzliche Schw√§chen der Modellarchitektur zur√ºckzuf√ºhren sind, sondern vielmehr auf Einschr√§nkungen im Bildausschnitt sowie auf ung√ºnstige Beleuchtungs- und Kontrastbedingungen. Die Ergebnisse unterstreichen damit die hohe Bedeutung einer konsistenten und auf den relevanten Inspektionsbereich ausgerichteten Datenerfassung.\n",
    "\n",
    "Das Modell selbst zeigt auch unter verzerrter Vorverarbeitung eine insgesamt sehr hohe Leistungsf√§higkeit, reagiert jedoch sensibler auf variierende Aufnahmebedingungen als die Padding-Variante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e8479-d48c-4a96-b35a-65d930f5bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_basis_distort = model_basis_distort.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda20b8c-72a0-4dc2-aeee-b757e56a2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_basis_distort_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/bad/Cam1Top_0000000687.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_model_basis_distort_gc1,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3171c-aecb-421c-98a0-af98bc9f2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_basis_distort_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/topsport_yellow/good/Cam2Side_0000000139.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_model_basis_distort_gc2,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b127fb6-c931-4863-8096-7053ea12e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_basis_distort_gc3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/gg_darkblue/good/Cam2Side_0000000228.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_model_basis_distort_gc3,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77286fa7-7de6-41ad-a26b-ec987eb1587d",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen der Fehlklassifikationen zeigen, dass das Modell seine Aufmerksamkeit h√§ufig nicht ausschlie√ülich auf den Sicherungsring richtet, sondern teilweise auf angrenzende Kanten, Kontrastbereiche oder den unteren Flaschenhals fokussiert. Insbesondere bei False Positives werden auff√§llige vertikale oder seitliche Strukturen st√§rker gewichtet als der eigentliche Inspektionsbereich.\n",
    "\n",
    "In den betrachteten False-Negative-Beispielen ist zwar eine Aktivierung im Bereich des Verschlusses erkennbar, jedoch nicht eindeutig auf den sicherungsringrelevanten Defekt konzentriert. Dies deutet darauf hin, dass das Modell zwar relevante Regionen ber√ºcksichtigt, jedoch in einzelnen F√§llen nicht ausreichend zwischen schadensrelevanten und strukturell √§hnlichen Merkmalen differenziert.\n",
    "\n",
    "Insgesamt best√§tigen die Grad-CAM-Analysen, dass die Fehlentscheidungen weniger zuf√§llig sind, sondern auf eine ver√§nderte Gewichtung visuell dominanter Strukturen zur√ºckzuf√ºhren sein k√∂nnten.  \n",
    "\n",
    "Zusammenfassend zeigt sich, dass auch die verzerrte Bildvorverarbeitung insgesamt sehr gute Ergebnisse liefert und grunds√§tzlich f√ºr die Modellierung geeignet ist. F√ºr die weiteren Analysen und Architekturvergleiche wird jedoch die Resize-und-Padding-Strategie mit unverzerrten Bildern verwendet, da sie im direkten Vergleich eine stabilere False-Positive-Rate aufweist und den relevanten Inspektionsbereich geometrisch unver√§ndert erh√§lt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73abf4-110d-4ccc-bff1-274e278f3ef8",
   "metadata": {},
   "source": [
    "### 4.4.3 Analyse einer erh√∂hten Netztiefe als Architekturvariation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff415ced-a223-4641-b55a-ab08dff6b1d7",
   "metadata": {},
   "source": [
    "Im n√§chsten Schritt wird aufbauend auf dem Baseline-Modell ein tieferes, zugleich schmaleres CNN modelliert und mit der Baseline verglichen.\n",
    "Ziel dieses Experiments ist es, den Einfluss einer erh√∂hten Netztiefe bei reduzierter Kanalbreite auf Trainingsverhalten, Modellkomplexit√§t und Klassifikationsleistung zu untersuchen.\n",
    "\n",
    "Im Unterschied zum Baseline-Modell wird die Architektur durch mehrere zus√§tzliche Convolution-Bl√∂cke erweitert, w√§hrend die Anzahl der Feature-Maps pro Block insgesamt geringer gehalten wird. Der Klassifikationskopf sowie weitere zentrale Hyperparameter (z. B. Kernelgr√∂√üe, Aktivierungsfunktionen) bleiben unver√§ndert, sodass die beobachteten Effekte prim√§r auf die ver√§nderte Tiefen-/Breitenstruktur zur√ºckzuf√ºhren sind.\n",
    "\n",
    "Die folgende Abbildung zeigt die resultierende Netzwerkarchitektur schematisch und verdeutlicht die strukturellen Unterschiede zum Baseline-Modell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abe081-1819-4ff7-839b-7b0216ec8e05",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Architektur_tiefesCNN.png\" width=\"1500\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {
    "f2a7aec3-4f98-408d-894d-19b06e0eeace.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "55c6db2b-9dc6-4bb7-980a-758e8bf3c795",
   "metadata": {},
   "source": [
    "Auf Basis der dargestellten Architektur wird das tiefe, schmale Modell im Folgenden in PyTorch implementiert.\n",
    "Die Parametrisierung orientiert sich direkt an der zuvor gezeigten Netzwerkstruktur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319cdad-3ccb-4927-9e5d-86b208d81cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"fc\",\n",
    "    fc_hidden=256,\n",
    "    dropout=0.0,\n",
    "    adaptive_pool_out=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c4998-3bed-49ad-804e-afc978e64c08",
   "metadata": {},
   "source": [
    "Die nachfolgende Tabelle fasst die Architektur des tiefen, schmalen Modells nochmals detailliert zusammen.\n",
    "Im Vergleich zur grafischen Darstellung werden hier insbesondere die Anzahl der trainierbaren Parameter sowie der gesch√§tzte Speicherbedarf w√§hrend des Trainings ausgewiesen.\n",
    "Trotz einer insgesamt √§hnlichen Parameteranzahl im Vergleich zum Baseline-Modell zeigt sich ein reduzierter Speicherbedarf, was auf die st√§rkere r√§umliche Verdichtung der Feature-Maps durch die zus√§tzliche Tiefe des Netzes zur√ºckzuf√ºhren ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a019f5-9a92-47ae-a6fb-842f84b4bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep = model_deep.to(device)\n",
    "\n",
    "summary(\n",
    "    model_deep,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997d44d-ab75-467f-be3b-83beb1020aa2",
   "metadata": {},
   "source": [
    "Das tiefe, schmale Modell weist mit rund 665.000 trainierbaren Parametern eine √§hnliche Gr√∂√üenordnung wie das Baseline-Modell auf, verteilt die Modellkapazit√§t jedoch st√§rker √ºber zus√§tzliche Convolution-Bl√∂cke. Die erh√∂hte Tiefe f√ºhrt zu einer st√§rkeren r√§umlichen Verdichtung der Feature-Maps bei gleichzeitig moderater Kanalbreite. Dadurch wird die Repr√§sentationsf√§higkeit strukturell ver√§ndert, ohne die Gesamtkomplexit√§t stark zu erh√∂hen.\n",
    "\n",
    "Im n√§chsten Schritt wird das tiefe, schmale Modell unter identischen Trainingsbedingungen wie das Baseline-Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207b850-7c46-4db5-92b7-dd9e3050db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Modells\n",
    "model_deep, hist_deep, cm_deep, wrong_deep = train_model(\n",
    "    model_deep,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535bf45-f3c7-49b2-986b-03eae5d6671d",
   "metadata": {},
   "source": [
    "Im Folgenden wird das beste Modell erneut geladen und zusammen mit den gespeicherten Auswertungsartefakten (Trainingsverlauf, Fehlklassifikationen und Konfusionsmatrix) f√ºr die weitere Analyse bereitgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e83e8-dc1f-448e-89bc-36f9d6345ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainiertes Modell laden\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep.to(device)\n",
    "model_deep.eval()\n",
    "\n",
    "hist_deep = torch.load(os.path.join(eval_base_path, \"HIST/hist_basis_deep.pth\"),weights_only=False)\n",
    "wrong_deep = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep.pth\"),weights_only=False)\n",
    "cm_deep = np.load(os.path.join(eval_base_path, \"CM/cm_deep.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eafe99-ee52-4486-b0cc-f7d8eeab4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5a550-ab85-46cb-889f-ac4b0d3c1b51",
   "metadata": {},
   "source": [
    "Im Trainingsverlauf zeigt das tiefe, schmale Modell eine sehr schnelle Konvergenz. Sowohl Trainings- als auch Validierungsgenauigkeit erreichen bereits nach wenigen Epochen Werte nahe 99 % und verlaufen anschlie√üend stabil. Der Validierungs-Loss sinkt kontinuierlich und weist im Vergleich zum Baseline-Modell deutlich geringere Ausrei√üer auf.\n",
    "\n",
    "Auff√§llig ist insbesondere die False-Positive-Rate: W√§hrend beim Baseline-Modell einzelne starke Schwankungen und Ausrei√üer zu beobachten waren, bleibt die Validierungs-FPR beim tieferen Modell √ºberwiegend auf niedrigem Niveau und stabilisiert sich im weiteren Trainingsverlauf.\n",
    "\n",
    "Insgesamt deutet der Vergleich darauf hin, dass die erh√∂hte Netz¬≠tiefe bei moderater Kanalbreite zu einer robusteren und stabileren Optimierung f√ºhrt, ohne Anzeichen von Overfitting zu zeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bf95d-3aff-40c7-be42-5cbfac1a6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f75dc-ffd6-4df7-9d0e-33a3cc56993c",
   "metadata": {},
   "source": [
    "Auf dem Testdatensatz erreicht das tiefe, schmale CNN eine Accuracy von 99,88 % bei einer False-Positive-Rate von 0,16 %. Im Vergleich zum Baseline-Modell reduziert sich die Anzahl der Fehlklassifikationen nochmals leicht. Besonders die Zahl der False Positives sinkt auf lediglich vier F√§lle, w√§hrend nur zwei defekte Verschl√ºsse f√§lschlich als ‚Äûgood‚Äú klassifiziert werden.\n",
    "\n",
    "Damit zeigt das Modell eine insgesamt sehr hohe Klassifikationsleistung bei gleichzeitig stabil niedriger Fehlerrate und best√§tigt die positive Wirkung der erh√∂hten Netz¬≠tiefe auf die Modellperformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee1b6e-5b7c-4181-a309-c4fc22bb1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_deep, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9beb1e9-1f78-41ae-9798-08e13c4cf1c4",
   "metadata": {},
   "source": [
    "Die verbleibenden Fehlklassifikationen sind stark variantenspezifisch und betreffen prim√§r river_cola_black. Eine generelle Schw√§che gegen√ºber bestimmten Farb- oder Strukturmustern ist nicht erkennbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96b499-a123-427c-9983-11b96bba3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dd146-5f06-4200-8d6b-fd4360679c5d",
   "metadata": {},
   "source": [
    "Die detaillierte Analyse zeigt, dass die False Positives ausschlie√ülich bei der Variante river_cola_black auftreten. In allen betrachteten F√§llen ist auf den Bildern prim√§r der Flaschenhals sichtbar, w√§hrend der relevante Inspektionsbereich, insbesondere der Sicherungsring, nicht abgebildet ist. Die Fehlklassifikation ist somit plausibel auf die eingeschr√§nkte Sichtbarkeit des entscheidungsrelevanten Bereichs zur√ºckzuf√ºhren und weniger auf eine grunds√§tzliche Fehlinterpretation durch das Modell.\n",
    "\n",
    "Die beiden False-Negative-F√§lle betreffen einzelne Bilder der Varianten cc_red und flirt_orange. Die dargestellten Defekte wirken visuell eher unauff√§llig. Zwar ist bei cc_red eine Deformation erkennbar, diese betrifft jedoch nicht eindeutig den Sicherungsring. Auch hier erscheint die Fehlentscheidung aus visueller Perspektive nachvollziehbar.\n",
    "\n",
    "Insgesamt deuten die verbleibenden Fehler auf grenzwertige oder unvollst√§ndig erfasste Bildsituationen hin und nicht auf systematische Schw√§chen des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2f388-a3ee-42a5-b18c-bafa1426580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_deep = model_deep.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1523c73-40aa-4656-9b0a-2a22f8ad65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/bad/Cam1Side_0000000699.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_model_deep_gc1,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a72eef-1cc6-4ba1-91ed-85d86269abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/flirt_orange/bad/Cam1Side_0000000325.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_model_deep_gc2,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf2cd4-e380-404b-bb9d-8b9f55abdf16",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen der beiden False-Negative-F√§lle zeigen, dass das Modell seine Aufmerksamkeit grunds√§tzlich im Bereich des Verschlusses konzentriert. Die aktivierten Regionen liegen im unteren bzw. mittleren Bereich des Deckels und erfassen strukturelle Merkmale der Geometrie.\n",
    "\n",
    "Allerdings ist keine eindeutig dominante Aktivierung direkt am Sicherungsring erkennbar. Stattdessen werden teilweise angrenzende Strukturen oder kontrastreiche Bereiche st√§rker gewichtet. Dies liefert eine plausible Erkl√§rung daf√ºr, warum die gezeigten, visuell eher unauff√§lligen Defekte nicht zuverl√§ssig als ‚Äûbad‚Äú klassifiziert wurden.\n",
    "\n",
    "Insgesamt best√§tigen die Grad-CAM-Analysen, dass das Modell relevante Bildregionen ber√ºcksichtigt, in Grenzf√§llen jedoch nicht klar zwischen sicherungsringrelevanten Merkmalen und √§hnlichen Strukturen differenziert.  \n",
    "\n",
    "Aufgrund der insgesamt sehr hohen Klassifikationsleistung, der geringen Fehlerrate sowie der konsistenten Fokussierung auf verschlussrelevante Bildbereiche wird das tiefe, schmale CNN im Folgenden als Referenzarchitektur f√ºr weitere Experimente verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451e33e-345f-47e8-8e5a-199ef3cb6b2a",
   "metadata": {},
   "source": [
    "### 4.4.4 Einfluss des Klassifikationskopfes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc191ef8-12fd-4d82-8053-f298206a6b54",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird der Einfluss des Klassifikationskopfes auf die Modellleistung untersucht. Als Ausgangsbasis dient das zuvor identifizierte beste Backbone, n√§mlich das tiefe, schmale CNN, das sich in den vorangegangenen Architekturvergleichen hinsichtlich Stabilit√§t und Klassifikationsleistung als √ºberlegen erwiesen hat. W√§hrend die Faltungsarchitektur unver√§ndert beibehalten wird, wird der bisher verwendete vollst√§ndig verbundene Klassifikationskopf durch alternative Kopfstrukturen ersetzt. Ziel dieses Experiments ist es, zu analysieren, inwieweit die Wahl des Klassifikationskopfes die Generalisierungsf√§higkeit, das Trainingsverhalten sowie die Fehlklassifikationscharakteristik beeinflusst, ohne die zuvor optimierte Feature-Extraktion zu ver√§ndern.  \n",
    "Die folgende Abbildung zeigt die Architektur des Modells mit alternativem Klassifikationskopf schematisch. Bis einschlie√ülich Convolution Block 5 entspricht die Struktur vollst√§ndig der zuvor betrachteten tiefen, schmalen Architektur, sodass die Feature-Extraktion unver√§ndert √ºbernommen wird.\n",
    "\n",
    "Der wesentliche Unterschied liegt im √úbergang vom Faltungsnetz zum Klassifikationskopf. Anstelle des bisherigen Flatten-Schritts mit anschlie√üendem vollst√§ndig verbundenem Schichtstapel wird hier ein Adaptive Average Pooling eingesetzt, das jede Feature-Map auf einen einzelnen skalaren Wert reduziert. Dadurch wird jede der 128 Feature-Maps auf eine kompakte, globale Repr√§sentation zusammengefasst, wodurch die r√§umliche Information vollst√§ndig aggregiert wird.\n",
    "\n",
    "Infolgedessen entf√§llt der Flatten-Operator, und der Klassifikationskopf reduziert sich auf eine deutlich kompaktere Struktur. Der resultierende 128-dimensionale Feature-Vektor wird direkt √ºber eine lineare Schicht auf die zweidimensionale Ausgabeschicht abgebildet, welche die beiden Klassen repr√§sentiert. Optional kann an dieser Stelle Dropout zur Regularisierung eingesetzt werden, ohne die zuvor gelernte Feature-Extraktion zu ver√§ndern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f106ce-8673-4def-b35b-6f5a503c3884",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/Architektur_Klassifikationskopf.png\" width=\"1300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44088f-527f-4eff-8bdb-8a380351636d",
   "metadata": {},
   "source": [
    "Auf Basis der dargestellten Architektur wird das Modell im Folgenden in PyTorch implementiert, wobei die Faltungsbl√∂cke unver√§ndert √ºbernommen und ausschlie√ülich der Klassifikationskopf entsprechend der beschriebenen GAP-Struktur angepasst wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a22012-f7d4-4564-969e-92ec1b78d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_gap = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),  \n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"gap\",                      \n",
    "    dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec2b23-5f7c-403a-b26b-6ad7df3c5c99",
   "metadata": {},
   "source": [
    "Die tabellarische Zusammenfassung der Modellarchitektur verdeutlicht unmittelbar, dass die Anzahl der trainierbaren Parameter im Vergleich zu den zuvor betrachteten Modellen deutlich reduziert ist. Ursache hierf√ºr ist der Wegfall des umfangreichen vollst√§ndig verbundenen Klassifikationskopfes, der durch die Global-Average-Pooling-Struktur ersetzt wird. Infolge dessen verringert sich nicht nur die Modellkomplexit√§t, sondern auch der ben√∂tigte Speicherbedarf w√§hrend Training und Inferenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb543fb0-d44e-43df-b336-3c8825e92835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_gap = model_deep_gap.to(device)\n",
    "\n",
    "summary(\n",
    "    model_deep_gap,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328290b0-89c7-4669-9285-34d2cf0af993",
   "metadata": {},
   "source": [
    "Das Modell mit Global-Average-Pooling-Klassifikationskopf wird im Folgenden unter identischen Trainingsbedingungen wie die zuvor betrachteten Architekturen trainiert, um einen fairen Vergleich der resultierenden Lern- und Generalisierungseigenschaften zu erm√∂glichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5b220-b50d-4a58-b476-323f7bf6ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_deep_gap, hist_deep_gap, cm_deep_gap, wrong_deep_gap = train_model(\n",
    "    model_deep_gap,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bb88d-bf61-48b9-9d9e-ca7950a0ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep_gap.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep_gap.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep_gap.to(device)\n",
    "model_deep_gap.eval()\n",
    "\n",
    "hist_deep_gap = torch.load(os.path.join(eval_base_path, \"HIST/hist_deep_gap.pth\"),weights_only=False)\n",
    "wrong_deep_gap = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep_gap.pth\"),weights_only=False)\n",
    "cm_deep_gap = np.load(os.path.join(eval_base_path, \"CM/cm_deep_gap.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163c3cb-541e-4847-94f5-b6a8a8a3495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849d9f2-fb92-4f2f-bdeb-6c0b05986835",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf zeigt eine sehr schnelle Konvergenz des Modells. Die Trainingsaccuracy steigt bereits nach wenigen Epochen auf nahezu 100 % an, w√§hrend der Trainingsloss kontinuierlich gegen Null tendiert. Dies deutet auf eine hohe Anpassungsf√§higkeit der Architektur an die Trainingsdaten hin.\n",
    "\n",
    "Auff√§llig sind jedoch starke Schwankungen in den Validierungsmetriken, insbesondere in den ersten Epochen. Sowohl Validierungsaccuracy als auch Validierungsloss zeigen vereinzelt deutliche Ausrei√üer. Diese spiegeln sich auch in der False-Positive-Rate wider, die in einzelnen Epochen stark ansteigt.\n",
    "\n",
    "Ab etwa der Mitte des Trainings stabilisieren sich die Validierungswerte jedoch deutlich. Accuracy und Loss verlaufen anschlie√üend sehr nahe an den Trainingskurven, was auf eine gute Generalisierung des Modells hindeutet.\n",
    "\n",
    "Im Vergleich zum zuvor betrachteten Modell mit vollst√§ndig verbundenem Klassifikationskopf zeigt der GAP-Kopf eine √§hnlich hohe Endleistung, jedoch eine st√§rkere Instabilit√§t in fr√ºhen Trainingsphasen. Dies kann darauf zur√ºckgef√ºhrt werden, dass durch Global Average Pooling die r√§umliche Information stark komprimiert wird und der Klassifikator unmittelbar auf global aggregierten Features basiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa2ff7-e400-4c83-b403-2fefe994977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17199848-d006-400e-bc64-dd4952ce0299",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix zeigt eine sehr hohe Klassifikationsleistung mit einer Accuracy von 99,90 %. Von 2.579 tats√§chlich guten Verschl√ºssen wurden lediglich 5 f√§lschlicherweise als ‚Äûbad‚Äú klassifiziert (False Positives). Gleichzeitig wurde kein einziger fehlerhafter Verschluss als ‚Äûgood‚Äú vorhergesagt (0 False Negatives).\n",
    "\n",
    "Die False-Positive-Rate liegt mit 0,19 % auf sehr niedrigem Niveau. Besonders hervorzuheben ist das vollst√§ndige Vermeiden von False Negatives, was im industriellen Kontext von hoher Relevanz ist, da keine fehlerhaften Teile als gut freigegeben w√ºrden.\n",
    "\n",
    "Trotz der deutlichen Reduktion der trainierbaren Parameter durch den Einsatz von Global Average Pooling bleibt die Klassifikationsleistung auf exzellentem Niveau. Dies deutet darauf hin, dass die zuvor gelernte Feature-Extraktion ausreichend diskriminative Merkmale liefert und ein komplexer Fully-Connected-Kopf f√ºr diese Aufgabe nicht erforderlich ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad208c-c5a8-402c-9016-1e327d993056",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_deep_gap, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021392ff-ada9-47fe-bf59-74f5aa015488",
   "metadata": {},
   "source": [
    "Im Testdatensatz treten insgesamt f√ºnf Fehlklassifikationen auf. Alle Fehler entfallen auf die Variante river_cola_black und sind ausschlie√ülich False Positives, also good als bad klassifiziert. False Negatives treten nicht auf.\n",
    "\n",
    "Die Fehlklassifikationen sind damit klar auf eine einzelne Variante konzentriert und nicht √ºber mehrere Varianten verteilt. Alle √ºbrigen Verschlussvarianten werden vollst√§ndig korrekt klassifiziert.\n",
    "\n",
    "Die Konzentration der Fehler auf river_cola_black deutet darauf hin, dass spezifische visuelle Eigenschaften dieser Variante, insbesondere Kontrast und Belichtung, die Modellentscheidung beeinflussen. Insgesamt best√§tigt die sehr geringe Fehleranzahl jedoch die hohe Stabilit√§t und Robustheit des Modells im Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e665a-fa82-440e-bdf3-b2b2d74b389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e563a-0b9a-4a78-a0be-9851241ea14f",
   "metadata": {},
   "source": [
    "Alle f√ºnf False Positives treten ausschlie√ülich bei der Variante river_cola_black auf. In den meisten F√§llen ist √ºberwiegend der Flaschenhals sichtbar und der relevante Bereich des Sicherungsrings nur eingeschr√§nkt erkennbar. In einem Fall ist der Verschluss zwar sichtbar, jedoch erschweren starke Kontraste und Reflexionen die klare Abgrenzung der sicherungsringrelevanten Strukturen.\n",
    "\n",
    "Die Kombination aus dunkler Farbgebung, Spiegelungen und geringem Detailkontrast f√ºhrt offenbar dazu, dass das Modell einzelne Struktur- oder Lichtartefakte als potenziellen Defekt interpretiert.\n",
    "\n",
    "False Negatives treten in dieser Konfiguration nicht auf. Insgesamt zeigen die Detailbilder, dass die verbleibenden Fehlklassifikationen auf spezielle visuelle Randbedingungen beschr√§nkt sind und keine generelle Schw√§che bei klar erkennbaren Defekten vorliegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c098727-230d-4b36-baaf-59c9bd798d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_deep_gap = model_deep_gap.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452794f-698b-4f6d-a89b-b272ad7a8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_gap_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/river_cola_black/good/Cam1Side_0000000087.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_model_deep_gap_gc1,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1f28b-f02b-4c9b-af39-e4240e350edf",
   "metadata": {},
   "source": [
    "Im vorliegenden Fehlklassifikationsbeispiel ist der Verschluss inklusive Sicherungsring vollst√§ndig sichtbar. Die Grad-CAM hebt den Bereich des Sicherungsrings deutlich als entscheidungsrelevant hervor. Gleichzeitig ist im Originalbild in diesem Bereich eine starke Reflexion erkennbar, was darauf hindeutet, dass nicht die strukturelle Integrit√§t des Rings selbst, sondern lokale Helligkeitsartefakte die Klassifikation beeinflusst haben k√∂nnten.\n",
    "\n",
    "Auff√§llig ist zudem, dass Teile des gepaddeten Randbereichs ebenfalls Aktivierungen zeigen. Dies ist insofern unerwartet, als dieser Bereich keine inhaltliche Information tr√§gt und idealerweise keinen Einfluss auf die Entscheidung haben sollte.\n",
    "\n",
    "Zusammenfassend zeigt sich, dass der Global-Average-Pooling-Klassifikationskopf im Vergleich zum vollst√§ndig verbundenen Kopf eine leicht bessere Testleistung erzielt, obwohl die Anzahl der trainierbaren Parameter deutlich reduziert wurde. Gleichzeitig sinkt die Modellkomplexit√§t, insbesondere im Klassifikationskopf, was sowohl den Speicherbedarf als auch die Trainingsstabilit√§t positiv beeinflusst.\n",
    "\n",
    "F√ºr die nachfolgenden Experimente wird daher die Architektur mit Global-Average-Pooling-Kopf als Grundlage verwendet, da sie bei geringerer Komplexit√§t mindestens gleichwertige, in diesem Fall sogar leicht bessere Ergebnisse liefert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da531d20-26af-40da-9c5a-d99fa1ede5b5",
   "metadata": {},
   "source": [
    "### 4.4.5 Aktivierung von Dropout zur Reduktion von Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d19e2-26d4-4320-8216-432b4250b749",
   "metadata": {},
   "source": [
    "In den bisherigen Experimenten zeigte das Modell kein ausgepr√§gtes Overfitting. Trainings- und Validierungsmetriken verlaufen stabil und auf vergleichbarem Niveau, und auch die Testleistung best√§tigt eine gute Generalisierungsf√§higkeit.\n",
    "\n",
    "Dennoch wird der Einfluss von Dropout als zus√§tzliche Regularisierungsma√ünahme untersucht. Ziel ist es, systematisch zu pr√ºfen, ob sich durch die zuf√§llige Deaktivierung einzelner Feature-Dimensionen im Klassifikationskopf eine weitere Stabilisierung des Trainingsverlaufs oder eine geringf√ºgige Verbesserung der Fehlklassifikationsrate erreichen l√§sst.\n",
    "\n",
    "Als Ausgangsbasis dient das zuvor identifizierte beste Modell mit Global-Average-Pooling-Klassifikationskopf. Die Faltungsarchitektur bleibt unver√§ndert, erg√§nzt wird lediglich eine Dropout-Schicht mit einer Rate von 0,25 vor der finalen linearen Klassifikationsschicht. Damit wird isoliert der Effekt von Dropout analysiert, ohne die zuvor optimierte Feature-Extraktion zu ver√§ndern.  \n",
    "\n",
    "Der folgende Code instanziiert das zuvor definierte tiefe, schmale CNN mit Global-Average-Pooling-Klassifikationskopf erneut. Die Faltungsarchitektur sowie alle zentralen Hyperparameter bleiben unver√§ndert. Einzige Anpassung ist die Aktivierung von Dropout mit einer Rate von 0,25 im Klassifikationskopf. Dadurch entspricht die Modellstruktur der vorherigen Variante, wird jedoch um eine zus√§tzliche Regularisierungskomponente erg√§nzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd94fac-2f83-4f77-a0ef-cf44cd41f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_dropout = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"gap\",\n",
    "    dropout=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56777f9d-b9d0-4935-8d5f-5b9a91baaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Modells\n",
    "model_deep_dropout, hist_deep_dropout, cm_deep_dropout, wrong_deep_dropout = train_model(\n",
    "    model_deep_dropout,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be5468-d79c-45df-83df-7f6c899b175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell und Trainingshistorie etc. laden\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep_dropout.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep_dropout.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep_dropout.to(device)\n",
    "model_deep_dropout.eval()\n",
    "\n",
    "hist_deep_dropout = torch.load(os.path.join(eval_base_path, \"HIST/hist_deep_dropout.pth\"),weights_only=False)\n",
    "wrong_deep_dropout = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep_dropout.pth\"),weights_only=False)\n",
    "cm_deep_dropout = np.load(os.path.join(eval_base_path, \"CM/cm_deep_dropout.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b37184-dec5-4901-bbd2-e28af8232e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3b794-9891-4064-b5a4-8c167c3fafac",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf zeigt weiterhin eine sehr schnelle Konvergenz. Die Trainingsaccuracy steigt bereits in den ersten Epochen auf nahezu 100 % und bleibt dort stabil. Auch der Trainingsloss f√§llt kontinuierlich gegen Null.\n",
    "\n",
    "Die Validierungsaccuracy erreicht ebenfalls fr√ºh sehr hohe Werte, zeigt jedoch in den ersten Epochen st√§rkere Schwankungen als ohne Dropout. Einzelne Ausrei√üer im Validierungs-Loss sowie in der False-Positive-Rate sind deutlich erkennbar. Ab etwa Epoche 12 stabilisieren sich die Validierungsmetriken jedoch auf einem sehr niedrigen Niveau.\n",
    "\n",
    "Insgesamt ist kein klassisches Overfitting-Verhalten erkennbar, da Trainings- und Validierungsperformance weiterhin eng beieinanderliegen. Der Einsatz von Dropout f√ºhrt in diesem Setting nicht zu einer sichtbaren Verbesserung der Generalisierung, sondern erh√∂ht insbesondere in der fr√ºhen Trainingsphase die Schwankungen der Validierungsmetriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4986e9-fc98-4327-b363-a37f83f2e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b29804-eb63-4bc3-a512-496d0a40da93",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix im Test zeigt eine Accuracy von 99,90 % bei einer False-Positive-Rate von 0,19 %. Es treten f√ºnf False Positives und keine False Negatives auf. Damit entspricht das Ergebnis exakt der zuvor betrachteten GAP-Variante ohne Dropout.\n",
    "\n",
    "Im Vergleich zur Architektur ohne Dropout ergibt sich somit keine messbare Verbesserung der Klassifikationsleistung. Weder die Anzahl der Fehlklassifikationen noch die Fehlertypen ver√§ndern sich.\n",
    "\n",
    "Der Einsatz von Dropout f√ºhrt in diesem Setting daher nicht zu einer Leistungssteigerung, best√§tigt jedoch die bereits zuvor beobachtete gute Generalisierungsf√§higkeit des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f5099-8cc3-4be2-9a51-aa666013ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_deep_dropout, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8782e-0d86-4cc8-923d-0f4b9861667a",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen im Test konzentrieren sich ausschlie√ülich auf die Variante river_cola_black. Es treten f√ºnf False Positives auf, w√§hrend keine False Negatives beobachtet werden. Alle √ºbrigen Varianten werden vollst√§ndig korrekt klassifiziert.\n",
    "\n",
    "Damit zeigt das Modell eine sehr stabile Generalisierung √ºber nahezu alle Varianten hinweg. Die Fehler sind klar auf eine einzelne, visuell anspruchsvollere Variante begrenzt und deuten nicht auf ein generelles Schw√§chenprofil der Architektur hin.\n",
    "\n",
    "Auff√§llig ist zudem, dass dieses Ergebnis exakt dem Modell ohne aktiviertes Dropout entspricht. Weder die Anzahl noch die Verteilung der Fehlklassifikationen ver√§ndern sich durch den Einsatz von Dropout. In diesem Setting l√§sst sich somit kein zus√§tzlicher Regularisierungseffekt feststellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e136820-84e1-4e39-b7a5-d2c2eaed1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2941815-c2bd-4061-88a9-49321c8aea7a",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen entsprechen weitgehend dem zuvor betrachteten Modell ohne Dropout. Die Bilder, auf denen ausschlie√ülich der Flaschenhals sichtbar ist und der relevante Bereich des Sicherungsrings nicht im Bildausschnitt enthalten ist, werden identisch fehlklassifiziert.\n",
    "\n",
    "Zus√§tzlich tritt wie bereits ohne Dropout erneut eine Fehlklassifikation bei einem Bild auf, in dem der relevante Bereich grunds√§tzlich sichtbar ist. In diesem Fall erschweren jedoch starke Reflexionen sowie ung√ºnstige Beleuchtungs und Kontrastverh√§ltnisse die klare Erkennbarkeit der Struktur des Sicherungsrings.\n",
    "\n",
    "Insgesamt zeigt sich damit, dass Dropout mit einer Rate von 0,25 weder die Art noch das Muster der Fehlklassifikationen substantiell ver√§ndert. Die Fehler bleiben strukturell vergleichbar, was darauf hindeutet, dass kein ausgepr√§gtes Overfitting vorliegt und zus√§tzliche Regularisierung in dieser Konfiguration keinen erkennbaren Vorteil bringt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60340e31-f12b-479e-8a5f-874841f92191",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_deep_dropout = model_deep_dropout.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd3409-d036-40f7-9a44-688a467b205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_dropout_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/river_cola_black/good/Cam2Side_0000000187.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_model_deep_dropout_gc1,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c74e2-a43f-4fba-b8e5-6a2c95b8406b",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierung der verbliebenen Fehlklassifikation zeigt ein √§hnliches Muster wie beim Modell ohne Dropout. Zwar liegt ein Teil der Aktivierung im Bereich des Verschlusses, jedoch wird auch der Padding-Bereich deutlich mit in die Entscheidungsfindung einbezogen. Der Fokus verteilt sich somit nicht ausschlie√ülich auf die strukturell relevanten Merkmale des Sicherungsrings.\n",
    "\n",
    "Zu beachten ist, dass es sich hierbei nur um ein einzelnes Beispiel handelt und keine generelle Aussage √ºber alle m√∂glichen Fehlklassifikationen erlaubt.\n",
    "\n",
    "Insgesamt zeigt sich, dass die Aktivierung von Dropout mit einer Rate von 0,25 keinen erkennbaren Vorteil hinsichtlich Trainingsstabilit√§t, Generalisierung oder Fehlklassifikationsmuster gebracht hat. Die Testmetriken sowie die Art der Fehlentscheidungen bleiben praktisch unver√§ndert.\n",
    "\n",
    "Aus diesem Grund wird in den folgenden Experimenten auf Dropout verzichtet und das Modell ohne zus√§tzliche Regularisierung im Klassifikationskopf weiterverwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014add26-3bee-421d-b52d-b04bd1b15001",
   "metadata": {},
   "source": [
    "### 4.4.6 Deaktivierung der Batch-Normalisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d813e9-7268-444a-9a58-d965728239bc",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird der Einfluss der Batch-Normalisierung auf das Trainings- und Generalisierungsverhalten des Modells untersucht. Als Ausgangspunkt dient erneut das zuvor identifizierte beste Modell. Architektur, Datenvorverarbeitung und s√§mtliche Hyperparameter bleiben unver√§ndert. Die einzige Modifikation besteht in der Deaktivierung der Batch-Normalisierung innerhalb der Convolution-Bl√∂cke.\n",
    "\n",
    "Batch-Normalisierung dient prim√§r der Stabilisierung der Aktivierungen und kann die Konvergenz w√§hrend des Trainings beschleunigen. Sie wirkt damit weniger als klassische Regularisierung im Sinne von Dropout, sondern vielmehr als Normalisierungsbaustein zur Reduktion interner Kovariatenverschiebung. Insbesondere bei kleineren Batch-Gr√∂√üen kann es jedoch zu schwankenden Statistiksch√§tzungen kommen, was sich potenziell auf die Stabilit√§t des Validierungsverlaufs auswirken kann.\n",
    "\n",
    "Da Architektur, Datenbasis und Trainingsparameter unver√§ndert bleiben, l√§sst sich der Effekt der Batch-Normalisierung in diesem Experiment isoliert bewerten. Ziel ist es zu analysieren, ob der Verzicht auf Batch-Normalisierung in diesem Szenario zu einem stabileren oder instabileren Trainings- und Validierungsverlauf f√ºhrt und welche Auswirkungen sich auf Konvergenz und Modellleistung im Test ergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5d2d6-c117-4141-8bc1-9395b70eb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_no_bn = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),\n",
    "    use_bn=False,\n",
    "    kernel_size=3,\n",
    "    head=\"gap\",\n",
    "    dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3eef70-3eca-4e49-9b6f-700548e2eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_no_bn, hist_deep_no_bn, cm_deep_no_bn, wrong_deep_no_bn = train_model(\n",
    "    model_deep_no_bn,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebd56f-4795-41c4-a936-29fb5ee223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell und Trainingshistorie etc. laden\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep_no_bn.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep_no_bn.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep_no_bn.to(device)\n",
    "model_deep_no_bn.eval()\n",
    "\n",
    "hist_deep_no_bn = torch.load(os.path.join(eval_base_path, \"HIST/hist_deep_no_bn.pth\"),weights_only=False)\n",
    "wrong_deep_no_bn = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep_no_bn.pth\"),weights_only=False)\n",
    "cm_deep_no_bn = np.load(os.path.join(eval_base_path, \"CM/cm_deep_no_bn.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ca5ca-4ea3-4d7a-bb06-c7d6670e6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep_no_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f9af9-e1d3-459b-9be2-15f77aa5b6a1",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf ohne Batch-Normalisierung zeigt eine sehr schnelle Konvergenz. Trainings- und Validierungsaccuracy erreichen bereits nach wenigen Epochen nahezu 100 %. Auch der Loss f√§llt stark ab und bleibt anschlie√üend auf sehr niedrigem Niveau. Der Abstand zwischen Trainings- und Validierungsmetriken ist gering, was zun√§chst auf eine stabile Generalisierung hindeutet.\n",
    "\n",
    "Auff√§llig ist jedoch die sehr steile Anfangskonvergenz sowie die nahezu perfekte √úbereinstimmung beider Kurven √ºber weite Teile des Trainings. Im Vergleich zu den Modellen mit Batch-Normalisierung wirkt der Verlauf weniger ged√§mpft und st√§rker deterministisch.\n",
    "\n",
    "Die False-Positive-Rate zeigt zu Beginn erh√∂hte Schwankungen, stabilisiert sich jedoch im weiteren Verlauf ebenfalls auf sehr niedrigem Niveau.\n",
    "\n",
    "Insgesamt deutet der Trainingsverlauf darauf hin, dass Batch-Normalisierung in diesem Szenario nicht zwingend erforderlich ist, um eine stabile Konvergenz zu erreichen. Eine abschlie√üende Bewertung erfolgt jedoch anhand der Testmetriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b8bc3-e6a7-47cd-80ca-ca3b784e6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep_no_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfd8b9-1f77-4739-bd59-07e517559d50",
   "metadata": {},
   "source": [
    "Die Testkonfusionsmatrix best√§tigt die im Trainingsverlauf beobachtete hohe Leistungsf√§higkeit des Modells ohne Batch-Normalisierung. Mit einer Accuracy von 99,88 % und einer False-Positive-Rate von 0,19 % liegt die Performance auf nahezu identischem Niveau wie bei der Variante mit aktivierter Batch-Normalisierung.\n",
    "\n",
    "Es treten f√ºnf False Positives und ein False Negative auf. Damit ist die Fehlerverteilung weiterhin sehr gering und ausgewogen. Insgesamt zeigt sich, dass die Deaktivierung der Batch-Normalisierung in diesem Szenario keinen signifikanten Leistungsabfall verursacht, jedoch auch keinen erkennbaren Vorteil bringt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f85b27-e1f6-4b83-a6e7-2035e7a7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_deep_no_bn, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd1d72-4ea1-454f-92a9-6f3d293de43d",
   "metadata": {},
   "source": [
    "Die Analyse der Fehlklassifikationen je Verschlussvariante zeigt, dass die Fehler weiterhin stark auf einzelne Varianten konzentriert sind. Die f√ºnf False Positives treten ausschlie√ülich bei river_cola_black auf. Zus√§tzlich ist ein einzelnes False Negative bei topsport_green zu beobachten.\n",
    "\n",
    "Alle √ºbrigen Varianten werden im Testdatensatz fehlerfrei klassifiziert. Das Fehlermuster entspricht damit weitgehend dem zuvor beobachteten Verhalten mit aktivierter Batch-Normalisierung. Es zeigen sich keine neuen systematischen Schw√§chen oder Verschiebungen in der Fehlerverteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b828a98-3874-4539-859f-3454506ec21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep_no_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4065a41-956e-4edd-a9a1-b86b5c475d6f",
   "metadata": {},
   "source": [
    "Die detaillierte Betrachtung der Fehlklassifikationen best√§tigt das zuvor beobachtete Muster. Die False Positives bei river_cola_black treten √ºberwiegend bei Bildern auf, in denen entweder nur der Flaschenhals sichtbar ist oder der relevante Bereich des Sicherungsrings stark √ºberstrahlt beziehungsweise kontrastarm erscheint. In diesen F√§llen sind die charakteristischen Strukturen des Rings nur eingeschr√§nkt erkennbar, was die Entscheidung des Modells erschwert.\n",
    "\n",
    "Zus√§tzlich zeigt sich ein einzelnes False Positive, bei dem der Deckelbereich zwar sichtbar ist, jedoch ebenfalls ung√ºnstige Beleuchtung und geringe Kontraste vorliegen. Auch hier scheint die visuelle Abgrenzung zwischen intaktem und auff√§lligem Bereich erschwert zu sein.\n",
    "\n",
    "Das einzige False Negative bei flirt_orange zeigt eine eher unauff√§llige Deformation. Der Sicherungsring ist nicht klar gebrochen oder deutlich verschoben, sodass die Fehlentscheidung aus visueller Sicht nachvollziehbar erscheint.\n",
    "\n",
    "Insgesamt lassen sich keine neuen systematischen Fehlerquellen erkennen. Die Fehlklassifikationen sind auf wenige, visuell anspruchsvolle Randf√§lle beschr√§nkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9c462-72d5-4598-932f-0a8725db266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_deep_no_bn = model_deep_no_bn.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a74d6-1db1-4981-85d7-e9cfdbb2e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_no_bn_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/river_cola_black/good/Cam2Side_0000000302.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_model_deep_no_bn_gc1,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ceebd-6d46-46d7-a0fa-f04c1e68556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_no_bn_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/flirt_orange/bad/Cam1Side_0000000325.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_model_deep_no_bn_gc2,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1ccea-136b-43a8-82ca-54d8ba57b9ca",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen der Fehlklassifikationen ohne Batch-Normalisierung zeigen ein √§hnliches Muster wie in den vorherigen Experimenten. Beim False Positive wird der Bereich des Sicherungsrings deutlich aktiviert, was darauf hinweist, dass das Modell grunds√§tzlich relevante Strukturen ber√ºcksichtigt. Gleichzeitig ist jedoch auch eine Aktivierung im Bereich des schwarzen Paddings erkennbar, sodass die Entscheidung nicht ausschlie√ülich auf dem inhaltlich relevanten Bildbereich basiert.\n",
    "\n",
    "Beim False Negative liegt der Fokus ebenfalls auf dem Verschlussbereich, insbesondere auf Strukturen des Rings. Zus√§tzlich treten jedoch Aktivierungen im Rand- beziehungsweise Padding-Bereich auf. Dies deutet darauf hin, dass auch ohne Batch-Normalisierung keine vollst√§ndig saubere Trennung zwischen Objekt- und Hintergrundinformationen erfolgt.\n",
    "\n",
    "Insgesamt best√§tigt die Grad-CAM-Analyse, dass die Deaktivierung der Batch-Normalisierung keine qualitative Verbesserung der Entscheidungsfokussierung bewirkt hat. Die genutzten Bildbereiche entsprechen weitgehend den zuvor beobachteten Mustern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2843a7-8c6b-4a98-9e19-7826eda6c4b6",
   "metadata": {},
   "source": [
    "### 4.4.8 Einsatz von Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cf9c9-8955-4d4c-97b0-1e522ec971ec",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird exemplarisch der Einsatz von Data Augmentation untersucht, um die Generalisierungsf√§higkeit des Modells weiter zu st√§rken. Im vorliegenden Anwendungsfall ist Augmentation insbesondere deshalb sinnvoll, weil in realen Produktions- und Aufnahmesituationen Variationen in Beleuchtung, Kontrast sowie leichte Lage und Ausrichtung der Flaschen auftreten k√∂nnen. Durch gezielte, moderate Transformationen kann das Modell robuster gegen√ºber solchen Abweichungen werden und weniger stark von spezifischen Aufnahmebedingungen der Trainingsdaten abh√§ngen.\n",
    "\n",
    "Zur Demonstration wird eine Augmentierungsvariante mit bewusst moderaten Parametern definiert, um realistische Bildver√§nderungen zu simulieren, ohne die relevanten Merkmale des Verschlussbereichs k√ºnstlich zu verf√§lschen. Hierf√ºr wird ein separater Transformationsschritt ausschlie√ülich f√ºr das Training erstellt. Auf Basis dieses Trainings-Transforms werden anschlie√üend eigene DataLoader f√ºr Train, Validation und Test erzeugt, sodass die Augmentation sauber von den √ºbrigen Pipeline-Schritten getrennt bleibt.\n",
    "\n",
    "Das Modell wird danach mit unver√§nderter Architektur und identischen Trainingsparametern erneut trainiert und evaluiert. Dadurch l√§sst sich der Einfluss der Augmentation isoliert betrachten, w√§hrend das Notebook zugleich zeigt, wie sich weitere Augmentierungsvarianten durch Anpassung der Transform-Parameter flexibel erg√§nzen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7c966-b7fd-4156-b352-a326375f16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_augV1 = build_train_transform(\n",
    "    rot_deg=2,\n",
    "    hflip_p=0,\n",
    "    brightness=0.1,\n",
    "    contrast=0.1,\n",
    "    saturation=0.1,\n",
    "    hue=0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf042a1-c1f1-468e-b71e-bf5a19646640",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_augV1, val_loader_augV1, test_loader_augV1 = create_dataloaders(\n",
    "    train_paths=train_paths,\n",
    "    val_paths=val_paths,\n",
    "    test_paths=test_paths,\n",
    "    preprocessed_dir=preprocessed_path,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_train_step2=transform_train_augV1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    shuffle_train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7167c-3255-4ec5-a0ae-a082e84a5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_augV1 = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"gap\",\n",
    "    dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ebbcb-cfd7-4cf1-823e-563ea6679439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_deep_augV1, hist_deep_augV1, cm_deep_augV1, wrong_deep_augV1 = train_model(\n",
    "    model_deep_augV1,\n",
    "    train_loader_augV1,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d055996-c1b3-443c-9c81-682d09489f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell und Trainingshistorie etc. laden\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep_augV1.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep_augV1.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep_augV1.to(device)\n",
    "model_deep_augV1.eval()\n",
    "\n",
    "hist_deep_augV1 = torch.load(os.path.join(eval_base_path, \"HIST/hist_deep_augV1.pth\"),weights_only=False)\n",
    "wrong_deep_augV1 = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep_augV1.pth\"),weights_only=False)\n",
    "cm_deep_augV1 = np.load(os.path.join(eval_base_path, \"CM/cm_deep_augV1.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c784cb-a05f-4acc-8e9e-964391a79e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep_augV1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0a2ba-f987-46ec-837c-e50e839176a5",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf zeigt, dass das Modell auch unter Einsatz von Data Augmentation sehr schnell konvergiert. Die Trainingsgenauigkeit erreicht bereits nach wenigen Epochen nahezu 100 %, und auch die Validierungsgenauigkeit bewegt sich auf einem vergleichbar hohen Niveau.\n",
    "\n",
    "Zwar treten im Validierungs-Loss einzelne deutliche Ausrei√üer auf, insgesamt verschlechtert sich die Modellleistung durch die Augmentation jedoch nicht. Ein systematisches Auseinanderlaufen von Trainings- und Validierungskurven ist weiterhin nicht erkennbar, sodass kein Hinweis auf verst√§rktes Overfitting besteht.\n",
    "\n",
    "Die False-Positive-Rate bleibt √ºber weite Strecken sehr niedrig und zeigt lediglich vereinzelte Peaks. Insgesamt l√§sst sich festhalten, dass die gew√§hlte moderate Augmentation die Stabilit√§t nicht signifikant verbessert, die Modellleistung jedoch auch nicht negativ beeinflusst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cba39-49b7-4c0b-85a2-1fa1a183e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep_augV1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8b4ad-e916-4763-bc72-f4093d39023c",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix im Test zeigt eine Accuracy von 99,92 % bei einer False-Positive-Rate von 0,16 %. Insgesamt werden lediglich vier Good-Flaschen f√§lschlicherweise als Bad klassifiziert, w√§hrend keine Bad-Flasche als Good vorhergesagt wird.\n",
    "\n",
    "Im Vergleich zur Variante ohne Augmentation ergibt sich damit eine minimale Verbesserung der Gesamtgenauigkeit sowie eine leicht reduzierte Anzahl an Fehlklassifikationen. Die Data Augmentation verschlechtert das Modellverhalten somit nicht, sondern f√ºhrt in dieser Konfiguration zu einer geringf√ºgig robusteren Generalisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399fde4-c54f-486e-9728-ba81b9977031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_deep_augV1, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890cea8-bb8c-4d22-add0-3bbccd462b8c",
   "metadata": {},
   "source": [
    "Die Verteilung der Fehlklassifikationen zeigt, dass alle vier False Positives ausschlie√ülich der Variante river_cola_black zuzuordnen sind. F√ºr alle √ºbrigen Verschlussvarianten treten im Test keine Fehlklassifikationen auf.\n",
    "\n",
    "Damit bleibt das Fehlermuster identisch zur vorherigen besten GAP-Variante ohne Augmentation. Die Data Augmentation ver√§ndert also weder die strukturelle Fehlercharakteristik noch verschiebt sie die problematische Variante, reduziert jedoch die Gesamtanzahl der Fehler geringf√ºgig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003bdf6-17df-4487-87c8-8fe25d1b305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep_augV1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eedf33-e449-441a-b37a-0edaf71f95cf",
   "metadata": {},
   "source": [
    "Die Detailanalyse der Fehlklassifikationen best√§tigt das zuvor beobachtete Muster. S√§mtliche False Positives betreffen weiterhin die Variante river_cola_black. Wie bereits in den vorherigen Experimenten treten die Fehlentscheidungen ausschlie√ülich bei Bildern auf, in denen der relevante Bereich nur eingeschr√§nkt sichtbar ist oder starke Reflexionen und Kontrastprobleme vorliegen.\n",
    "\n",
    "False Negatives treten unter Einsatz der Data Augmentation im Test nicht auf.\n",
    "\n",
    "Insgesamt zeigt sich damit, dass die moderate Augmentierung die Fehlerstruktur nicht ver√§ndert, jedoch auch keine zus√§tzlichen systematischen Fehlklassifikationen einf√ºhrt. Das Modell bleibt stabil und beh√§lt sein charakteristisches Fehlermuster bei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f42639-3c8b-4d3e-9098-035f86cd9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_deep_augV1 = model_deep_augV1.features[-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d4d86-43fc-4e10-aaeb-f44952a9cc61",
   "metadata": {},
   "source": [
    "Der Target-Layer f√ºr Grad-CAM wird an dieser Stelle bereits definiert, da im weiteren Verlauf eine vergleichende Analyse einzelner Modellvarianten vorgesehen ist. F√ºr die hier betrachteten Fehlklassifikationen liefert eine zus√§tzliche Grad-CAM-Analyse jedoch keinen substanziellen Mehrwert, da in den betroffenen Bildern der relevante Verschlussbereich  nicht sichtbar ist. Die Fehlentscheidungen sind somit prim√§r auf unzureichende visuelle Information im Eingabebild zur√ºckzuf√ºhren und weniger auf eine systematische Fehlfokussierung des Modells.\n",
    "\n",
    "Insgesamt zeigt die eingesetzte Variante der Data Augmentation einen leicht positiven Einfluss auf die Modellleistung. Die Accuracy steigt geringf√ºgig, die False Positive Rate sinkt weiter, und es treten keine zus√§tzlichen Fehlklassifikationen auf. Die verbleibenden Fehler beschr√§nken sich auf Bilder, in denen der relevante Bildbereich nicht klar erkennbar ist.\n",
    "\n",
    "In einer realen Anwendung w√ºrden an dieser Stelle weitere Augmentierungsstrategien mit unterschiedlichen Parametern systematisch untersucht und miteinander verglichen. Die hier gew√§hlte, moderat konfigurierte Augmentierung liefert jedoch bereits sehr stabile und nahezu optimale Ergebnisse. Aus diesem Grund werden genau diese Augmentierungsparameter im anschlie√üenden Generalisierungstest mittels Leave-One-Variant-Out weiterverwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390c16e-6dc6-467f-971a-ae1f95c036f5",
   "metadata": {},
   "source": [
    "### 4.4.9 CNN von Grund auf - Generalisierungstest mittels Leave-One-Variant-Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5741a54-2bb7-4b44-9155-32077990fd65",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die Generalisierungsf√§higkeit des final ausgew√§hlten Modells mittels eines Leave-One-Variant-Out-Ansatzes untersucht. Dabei wird jeweils eine vollst√§ndige Verschlussvariante vollst√§ndig aus dem Training ausgeschlossen und ausschlie√ülich als Testdatensatz verwendet, w√§hrend das Modell auf allen verbleibenden Varianten trainiert wird.\n",
    "\n",
    "Dieser Test ist insbesondere praxisrelevant, da das Inspektionssystem in realen Anwendungen regelm√§√üig auf neue Verschlussvarianten angepasst werden muss, w√§hrend die Erzeugung und Annotation umfangreicher Trainings- und Testdatens√§tze f√ºr neue Verschl√ºsse mit hohem Aufwand verbunden ist. Der Leave-One-Variant-Out-Ansatz erlaubt daher eine realit√§tsnahe Bewertung, inwieweit das bislang beste Modell in der Lage ist, auf dem Modell bislang unbekannte Verschlussvarianten zu generalisieren.\n",
    "\n",
    "Um ein breiteres und belastbareres Bild der √úbertragbarkeit auf zuvor ungesehene Varianten zu erhalten, werden im Folgenden zwei unterschiedliche Leave-One-Variant-Out-Splits betrachtet und separat ausgewertet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659546df-9d9b-4a4e-a0e3-9185cb0b9723",
   "metadata": {},
   "source": [
    "<u><strong>Variante cc_red</strong></u>  \n",
    "\n",
    "Im Folgenden wird der Leave-One-Variant-Out-Split exemplarisch f√ºr die Variante cc_red umgesetzt. Die zuvor definierte Augmentierungsstrategie bleibt dabei unver√§ndert, sodass die √úbertragbarkeit des bisher besten Modells unter realit√§tsnahen Trainingsbedingungen gepr√ºft wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74def8ee-c79f-4a0c-a508-ed7057bc5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_ccred_aug, val_loader_ccred_aug, test_loader_ccred_aug = create_dataloaders(\n",
    "    train_paths_lovo_ccred,\n",
    "    val_paths_lovo_ccred,\n",
    "    test_paths_lovo_ccred,\n",
    "    preprocessed_dir=preprocessed_path,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_train_step2=transform_train_augV1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc71df-9997-47a8-93a6-1e4b19dfaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_lovo_ccred = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"fc\",\n",
    "    fc_hidden=256,\n",
    "    dropout=0.0,\n",
    "    adaptive_pool_out=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac8fd6-a8d6-4fcc-90cd-4506562d68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_deep_lovo_ccred, hist_deep_lovo_ccred, cm_deep_lovo_ccred, wrong_deep_lovo_ccred = train_model(\n",
    "    model_deep_lovo_ccred,\n",
    "    train_loader_ccred,\n",
    "    val_loader_ccred,\n",
    "    test_loader_ccred,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945dd36-5c6e-48e5-8aaf-92e210b4b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainiertes Modell und Trainingshistorie etc. laden\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep_lovo_ccred.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep_lovo_ccred.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep_lovo_ccred.to(device)\n",
    "model_deep_lovo_ccred.eval()\n",
    "\n",
    "hist_deep_lovo_ccred = torch.load(os.path.join(eval_base_path, \"HIST/hist_deep_lovo_ccred.pth\"),weights_only=False)\n",
    "wrong_deep_lovo_ccred = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep_lovo_ccred.pth\"),weights_only=False)\n",
    "cm_deep_lovo_ccred = np.load(os.path.join(eval_base_path, \"CM/cm_deep_lovo_ccred.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3197f4f-4c3d-40dc-bb9f-17e02745a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep_lovo_ccred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d754c0-ee10-467e-bfa6-dfef405e8f05",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf zeigt auch im Leave-One-Variant-Out-Szenario eine stabile Konvergenz. Trainings- und Validierungsaccuracy n√§hern sich schnell hohen Werten an, und der Loss sinkt kontinuierlich. Einzelne Ausschl√§ge in der False Positive Rate sind sichtbar, stabilisieren sich jedoch im weiteren Verlauf.\n",
    "\n",
    "F√ºr die Bewertung der Generalisierungsf√§higkeit ist dieser Verlauf allerdings nur bedingt aussagekr√§ftig. Entscheidend ist die Performance auf der vollst√§ndig ausgeschlossenen Variante im Testdatensatz, da sich dort zeigt, ob das Modell tats√§chlich auf eine zuvor unbekannte Verschlussvariante √ºbertragen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeff7d6-886c-4455-9edb-7b325182d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep_lovo_ccred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8042439-a7d8-4a67-9ce8-bc98b6e940e8",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix f√ºr die vollst√§ndig ausgeschlossene Variante cc_red zeigt einen klaren, aber erwartbaren Leistungsabfall im Vergleich zu den bisherigen Standard-Splits.\n",
    "\n",
    "Mit einer Accuracy von 97,72 % und einer False Positive Rate von 2,00 % liegt die Performance weiterhin auf einem hohen Niveau, jedoch deutlich unter den nahezu perfekten Ergebnissen, die erzielt wurden, wenn alle Varianten im Training enthalten waren. Konkret treten 42 False Positives (good ‚Üí bad) und 29 False Negatives (bad ‚Üí good) auf. Beide Fehlertypen nehmen also sichtbar zu.\n",
    "\n",
    "Dieser Unterschied verdeutlicht den zentralen Zweck des Leave-One-Variant-Out-Ansatzes: Er pr√ºft nicht die reine Trainingsleistung, sondern die echte √úbertragbarkeit auf eine bislang unbekannte Verschlussvariante. W√§hrend das Modell bei bekannten Varianten nahezu fehlerfrei arbeitet, zeigt sich hier, dass ein Teil der gelernten Merkmale noch variantenspezifisch gepr√§gt ist.\n",
    "\n",
    "Gleichzeitig ist das Ergebnis keineswegs schlecht. Eine Accuracy nahe 98 % bei vollst√§ndig unbekannter Variante zeigt, dass das Modell bereits relevante strukturelle Merkmale des Sicherungsrings und der Verschlussgeometrie gelernt hat, die √ºber einzelne Farb- oder Designunterschiede hinausgehen. Dennoch wird deutlich, dass f√ºr eine noch robustere Generalisierung entweder weitere Varianten im Training oder gezielte Regularisierungs- und Augmentierungsstrategien sinnvoll sein k√∂nnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb8e0d-6742-4b2b-bb4e-7ef4fc79b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep_lovo_ccred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaccab4-642c-4662-8819-fc3fbaf364e2",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen bei der Variante cc_red zeigen ein sehr klares Muster.\n",
    "\n",
    "Sowohl bei den False Positives als auch bei den False Negatives handelt es sich fast ausschlie√ülich um Bilder mit √§hnlicher Perspektive und Beleuchtung. Auff√§llig ist, dass viele Aufnahmen einen sehr dunklen unteren Bildbereich sowie starke Reflexionen auf dem roten Kunststoff aufweisen. Teilweise ist der Sicherungsring nur schwach kontrastiert oder verschmilzt farblich mit dem restlichen Verschluss.\n",
    "\n",
    "Bei den False Positives scheint das Modell einzelne Strukturen oder Schattenbereiche als Defekt zu interpretieren, obwohl die Geometrie korrekt ist. Umgekehrt werden bei den False Negatives offenbar subtile Abweichungen im Ringbereich nicht zuverl√§ssig erkannt, insbesondere wenn sie durch Glanzstellen oder Farbs√§ttigung √ºberlagert werden.\n",
    "\n",
    "Insgesamt best√§tigt die visuelle Analyse, dass der Leistungsabfall im LOvO-Szenario nicht zuf√§llig ist, sondern systematisch mit der spezifischen Farbcharakteristik und Lichtreflexion der Variante cc_red zusammenh√§ngt. Das Modell generalisiert grunds√§tzlich, ist jedoch noch empfindlich gegen√ºber starken farb- und kontrastbedingten Dom√§nenverschiebungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcd5c7-0bf6-4893-8680-1852ebd16281",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_deep_lovo_ccred = model_deep_lovo_ccred.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f7cb5-840c-4f76-b077-27fc44391745",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_lovo_ccred_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/bad/Cam2Top_0000000669.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_model_deep_lovo_ccred_gc1,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41c80c-ec12-4f2d-9cc1-7aabbe12161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_lovo_ccred_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/bad/Cam1Top_0000000700.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_model_deep_lovo_ccred_gc2,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a76e6-6145-4671-b112-5170b5d92772",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_lovo_ccred_gc3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/good/Cam1Side_0000001024.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_model_deep_lovo_ccred_gc3,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a9e53-9b47-4585-ad00-f419ef715f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_lovo_ccred_gc4 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/good/Cam1Side_0000000995.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_model_deep_lovo_ccred_gc4,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8e9ad-d4f9-4dd3-8807-f115f0067ce6",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen best√§tigen die zuvor beobachteten Muster.\n",
    "\n",
    "Bei den False Negatives konzentriert sich das Modell √ºberwiegend auf den mittleren Ringbereich des Verschlusses. Obwohl der relevante Bereich also grunds√§tzlich erfasst wird, scheint die Gewichtung nicht ausreichend, um die feinen strukturellen Abweichungen korrekt als Defekt zu klassifizieren. Teilweise √ºberlagern starke Reflexionen oder homogene Farbfl√§chen die entscheidenden Details.\n",
    "\n",
    "Bei den False Positives ist zus√§tzlich erkennbar, dass neben dem Ringbereich auch der untere Bildrand beziehungsweise der dunkle Hintergrund mit in die Entscheidung einbezogen wird. Das deutet darauf hin, dass das Modell bei dieser bislang unbekannten Variante noch nicht vollst√§ndig zwischen relevanter Geometrie und bildspezifischen Artefakten trennt.\n",
    "\n",
    "Insgesamt zeigen die Grad-CAMs, dass das Modell zwar auf sinnvolle Regionen fokussiert, jedoch bei der Variante cc_red durch die starke Farbcharakteristik und ver√§nderte Lichtverh√§ltnisse in seiner Entscheidungsfindung beeintr√§chtigt wird. Dies unterstreicht die Herausforderung der Generalisierung auf vollst√§ndig ungesehene Varianten.  \n",
    "\n",
    "Abschlie√üend zeigt sich, dass die Generalisierungsleistung auf eine vollst√§ndig unbekannte Variante zwar solide ist, jedoch weiterhin klares Verbesserungspotenzial besteht.\n",
    "\n",
    "Zur weiteren Optimierung k√∂nnten gezieltere Data-Augmentation-Strategien untersucht werden, insbesondere st√§rkere Simulationen von Farb-, Kontrast- und Beleuchtungsvariationen. Gerade bei stark farbdominierten Varianten wie cc_red scheint die Robustheit gegen√ºber Farb- und Lichtunterschieden entscheidend zu sein. Zus√§tzlich w√§re ein Vergleich mit verzerrten Bildern statt Padding sinnvoll, um m√∂gliche Abh√§ngigkeiten vom Randbereich weiter zu reduzieren.\n",
    "\n",
    "Dar√ºber hinaus bietet sich eine systematische Analyse unterschiedlicher Fine-Tuning-Strategien an, etwa abgestuftes Unfreezing mit differenzierten Lernraten oder vollst√§ndiges Fine-Tuning bei reduzierter Lernrate. Auch der Einsatz tieferer Architekturen oder alternativer Regularisierungsstrategien k√∂nnte zeigen, ob komplexere Merkmalsrepr√§sentationen die Variantenunabh√§ngigkeit weiter verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28423b-5a2b-46d2-82b3-c5aeaa3498f1",
   "metadata": {},
   "source": [
    "<u><strong>Variante voesl_zitrone</strong></u>  \n",
    "\n",
    "F√ºr eine zweite, farblich und visuell deutlich abweichende Verschlussvariante wird der Leave-One-Variant-Out-Test analog durchgef√ºhrt. Ziel ist es, die zuvor beobachteten Generalisierungseffekte auf eine weitere unbekannte Variante zu √ºbertragen und die Robustheit des Modells unter identischen Trainings- und Augmentationsparametern zu √ºberpr√ºfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b33ecb-35bc-4125-8280-3be6d1beca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOVO: voesl_zitrone mit Augmentation\n",
    "train_loader_voesl_aug, val_loader_voesl_aug, test_loader_voesl_aug = create_dataloaders(\n",
    "    train_paths_lovo_voesl_zitrone,\n",
    "    val_paths_lovo_voesl_zitrone,\n",
    "    test_paths_lovo_voesl_zitrone,\n",
    "    preprocessed_dir=preprocessed_path,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_train_step2=transform_train_augV1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef9bce-2892-4cf9-afe2-dce2420ca2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_lovo_voesl = BaselineCNN(\n",
    "    channels=(32, 32, 64, 64, 128),\n",
    "    use_bn=True,\n",
    "    kernel_size=3,\n",
    "    head=\"gap\",\n",
    "    dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89792dc0-1047-4979-a0d5-538592570a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_deep_lovo_voesl, hist_deep_lovo_voesl, cm_deep_lovo_voesl, wrong_deep_lovo_voesl = train_model(\n",
    "    model_deep_lovo_voesl,\n",
    "    train_loader_voesl_aug,\n",
    "    val_loader_voesl_aug,\n",
    "    test_loader_voesl_aug,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc4d2d-9084-4390-a2a5-1e4349a58c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/BasicCNN/model_deep_lovo_voesl.pth\"\n",
    "eval_base_path = \"Modellauswertungen/BasicCNN\"\n",
    "\n",
    "model_deep_lovo_voesl.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_deep_lovo_voesl.to(device)\n",
    "model_deep_lovo_voesl.eval()\n",
    "\n",
    "hist_deep_lovo_voesl = torch.load(os.path.join(eval_base_path, \"HIST/hist_deep_lovo_voesl.pth\"),weights_only=False)\n",
    "wrong_deep_lovo_voesl = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_deep_lovo_voesl.pth\"),weights_only=False)\n",
    "cm_deep_lovo_voesl = np.load(os.path.join(eval_base_path, \"CM/cm_deep_lovo_voesl.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74037511-7b2a-470f-81da-3ab442a72ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_deep_lovo_voesl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3043d97-7e57-42e5-8112-2b971ba634bc",
   "metadata": {},
   "source": [
    "Im Trainingsverlauf zeigt sich erneut eine sehr schnelle Konvergenz mit nahezu perfekter Trainingsgenauigkeit und sehr geringem Trainingsverlust. Die Validierungsmetriken schwanken hingegen deutlich st√§rker, insbesondere in den ersten Epochen. Die starken Ausschl√§ge in Validation Loss und False Positive Rate deuten darauf hin, dass die zur√ºckgehaltene Variante strukturell deutlich von den Trainingsvarianten abweicht.\n",
    "\n",
    "Entscheidend ist daher weniger der Trainingsverlauf, sondern die finale Bewertung auf dem vollst√§ndig unbekannten Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f8e41-937b-4549-b500-1fca055ad34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_deep_lovo_voesl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f39a9e-123e-4f5b-ac41-db7993c83cb4",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix zeigt ein deutlich einseitiges Vorhersageverhalten des Modells. S√§mtliche Testbilder werden als bad klassifiziert, unabh√§ngig von der tats√§chlichen Klasse. Dadurch entstehen f√ºr die Klasse good ausschlie√ülich False Positives, w√§hrend f√ºr bad keine True Negatives existieren. Die Accuracy von 56,33 % ergibt sich hier lediglich aus dem Anteil der korrekt als bad erkannten Beispiele und spiegelt keine ausgewogene Klassifikationsleistung wider. Die False Positive Rate von 100 % verdeutlicht, dass das Modell die unbekannte Variante systematisch fehlinterpretiert und keine tragf√§hige Entscheidungsgrenze zwischen den Klassen gelernt hat.\n",
    "\n",
    "Die deutlich schlechtere Generalisierungsleistung bei voesl_zitrone l√§sst sich im Kontext der visuellen Unterschiede der Verschlussvarianten plausibel erkl√§ren.\n",
    "\n",
    "Die nachfolgende Abbildung verdeutlicht, dass sich voesl_zitrone sowohl in der Form als auch in der Farbcharakteristik st√§rker von den √ºbrigen Varianten unterscheidet als beispielsweise cc_red.\n",
    "\n",
    "Im Fall von cc_red bestanden bereits mehrere strukturelle und farbliche √Ñhnlichkeiten zu im Training enthaltenen Varianten. Hinsichtlich der Geometrie weist cc_red eine sehr √§hnliche Verschlussform wie cc_lightgray, fanta_blue oder aquanna_gray auf. Auch bez√ºglich der Farbgebung existierten bereits rote Verschl√ºsse im Trainingsdatensatz, etwa kcl_red oder kcl_pink, sodass sowohl Form- als auch Farbinformationen f√ºr das Modell nicht vollst√§ndig neu waren. Entsprechend konnte zumindest eine gewisse √úbertragungsleistung erzielt werden.\n",
    "\n",
    "Demgegen√ºber weicht voesl_zitrone insbesondere in der Formgestaltung deutlich von den √ºbrigen Verschl√ºssen ab. Die charakteristische Geometrie des Sicherungsrings sowie die Auspr√§gung der Verschlussnase unterscheiden sich sichtbar von den sonst dominierenden Formen. Damit fehlen dem Modell vertraute strukturelle Referenzmuster.\n",
    "\n",
    "Auch farblich ist voesl_zitrone nur eingeschr√§nkt repr√§sentiert. Zwar existiert mit topsport_yellow eine gelbliche Variante, jedoch handelt es sich um einen deutlich anderen Gelbton. Dar√ºber hinaus unterscheiden sich die √ºbrigen Varianten stark in ihrer Farbcharakteristik, sodass auch aus farblicher Sicht nur begrenzte √Ñhnlichkeiten bestehen.\n",
    "\n",
    "Insgesamt zeigt sich somit, dass bei voesl_zitrone sowohl eine geometrische als auch eine farbliche Dom√§nenverschiebung vorliegt, w√§hrend bei cc_red zumindest partielle √úberlappungen in Form und Farbe gegeben waren. Dies erkl√§rt die beobachtete Differenz in der Generalisierungsleistung im Leave-One-Variant-Out-Szenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b018526-039b-42b4-911c-f53ebd3cb49a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Bilder/uebersicht_verschluesse.jpg\" width=\"900\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f1699-41f3-44d8-9069-547896f0da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_deep_lovo_voesl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcd0c0-7af1-4aa9-9adb-40e2b9b3fdc7",
   "metadata": {},
   "source": [
    "Die Detailanalyse der Fehlklassifikationen best√§tigt das Bild aus der Konfusionsmatrix. S√§mtliche Fehlentscheidungen betreffen die Klasse good, die durchg√§ngig als bad klassifiziert wird. False Negatives treten hingegen nicht auf, da das Modell alle Bilder der Variante pauschal als bad einstuft.\n",
    "\n",
    "In den dargestellten Beispielen ist erkennbar, dass die Geometrie des Sicherungsrings und die gesamte Formgebung deutlich von den im Training gesehenen Varianten abweichen. Das Modell scheint diese strukturellen Unterschiede nicht als neue, aber valide Auspr√§gung eines korrekten Verschlusses zu interpretieren, sondern bewertet sie systematisch als Abweichung im Sinne eines Defekts. Die Fehlklassifikationen wirken daher konsistent und nicht zuf√§llig, sondern deuten auf eine fehlende √úbertragbarkeit der gelernten Merkmalsrepr√§sentation auf diese stark abweichende Variante hin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600ffa7-9495-44f7-b623-55dda429851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_deep_lovo_voesl = model_deep_lovo_voesl.features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed573ed-ec7a-4646-981c-6fb5edd2fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_lovo_voesl_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/voesl_zitrone/good/Cam1Top_0000000192.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_model_deep_lovo_voesl_gc1,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fac46-ab88-4463-ba1d-8fc9c822abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_deep_lovo_voesl_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/voesl_zitrone/good/Cam1Side_0000000249.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_model_deep_lovo_voesl_gc2,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369104b0-1280-42ab-b967-729774120e72",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen best√§tigen diesen Eindruck. Das Modell fokussiert sich zwar auf Bereiche des Verschlusses selbst, insbesondere auf Kanten und Strukturen im Bereich des Sicherungsrings, interpretiert diese jedoch offenbar als auff√§llig oder ‚Äûdefektartig‚Äú. Ein konsistenter Fokus auf irrelevante Padding-Bereiche ist hier nicht dominant zu erkennen. Vielmehr scheint das Problem darin zu liegen, dass die gelernten Merkmalsmuster nicht auf die stark abweichende Geometrie dieser Variante √ºbertragbar sind. Die Aktivierungen deuten darauf hin, dass das Modell durchaus strukturelle Details wahrnimmt, diese aber falsch einordnet.\n",
    "\n",
    "Abschlie√üend zeigen die Leave-One-Variant-Out-Experimente deutlich, dass die Generalisierungsf√§higkeit stark von der √Ñhnlichkeit der unbekannten Variante zu den im Training enthaltenen Verschl√ºssen abh√§ngt. Varianten wie cc_red, die in Form und teilweise auch Farbe bereits im Trainingsdatensatz vertreten oder zumindest √§hnlich sind, k√∂nnen solide klassifiziert werden. Weicht eine Variante jedoch sowohl in der Geometrie als auch in der Farbcharakteristik deutlich ab, wie im Fall von voesl_zitrone, bricht die Leistung erheblich ein. Die LOvO-Tests liefern damit einen realit√§tsnahen Einblick in die Robustheit des Modells gegen√ºber neuen Verschlussvarianten und verdeutlichen, dass insbesondere strukturelle Unterschiede eine zentrale Herausforderung f√ºr die √úbertragbarkeit darstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed4e86-5595-4186-b831-b9d00504d75a",
   "metadata": {},
   "source": [
    "### 4.4.10 Zusammenfassung Experimente mit dem CNN von Grund auf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4e60a-7c73-4910-be47-774e2afb5df8",
   "metadata": {},
   "source": [
    "Im Rahmen der Arbeit wurde ein Convolutional Neural Network vollst√§ndig neu aufgebaut und systematisch variiert. Ziel war es, den Einfluss einzelner Architektur- und Trainingsentscheidungen isoliert zu untersuchen und gleichzeitig eine modulare Notebook-Struktur zu schaffen, mit der weitere Experimente einfach erg√§nzt werden k√∂nnen. Ausgehend vom Basis-CNN wurden unterschiedliche Aspekte der Modellierung betrachtet. Dazu geh√∂rten die Bildvorverarbeitung mit Padding im Vergleich zu bewusst verzerrten Bildern, Variationen der Netztiefe, der Vergleich zwischen Fully Connected Head und GAP-Head, die Aktivierung von Dropout, die Deaktivierung von Batch Normalization sowie der Einsatz moderater Data Augmentation. Dar√ºber hinaus wurden Leave-One-Variant-Out-Tests durchgef√ºhrt, um die Generalisierungsf√§higkeit auf vollst√§ndig unbekannte Verschlussvarianten zu untersuchen.\n",
    "\n",
    "Die experimentellen Ergebnisse der untersuchten Architekturvarianten lagen insgesamt sehr dicht beieinander. Das tiefe, schmale Modell mit GAP-Head zeigte in den Tests leicht bessere Ergebnisse, insbesondere im Hinblick auf die False-Positive-Rate, die Unterschiede zu den √ºbrigen Varianten waren jedoch gering. Weder Dropout noch die Deaktivierung von BatchNorm f√ºhrten zu einer klaren Verbesserung oder signifikanten Verschlechterung der Modellleistung. Auch die alternative Vorverarbeitung mit verzerrten Bildern brachte keinen systematischen Vorteil gegen√ºber der Padding-Variante. Innerhalb des gew√§hlten Trainings- und Datensetups haben architektonische Feinvariationen somit nur begrenzten Einfluss auf die Gesamtperformance.\n",
    "\n",
    "Die eingesetzte Data-Augmentation-Strategie zeigte einen leicht positiven Effekt. Fehlklassifikationen traten √ºberwiegend bei Bildern auf, bei denen der relevante Bildbereich nur eingeschr√§nkt sichtbar war. In realen Inspektionssystemen werden solche Situationen h√§ufig durch mehrere hintereinander geschaltete Pr√ºfstationen abgefangen. Das Notebook erm√∂glicht es, weitere Augmentationsvarianten unkompliziert zu erg√§nzen und systematisch zu evaluieren.\n",
    "\n",
    "Die Grad-CAM-Analysen lieferten zus√§tzliche Einblicke in das Entscheidungsverhalten des Modells. In vielen F√§llen fokussierte das Netzwerk korrekt den Bereich des Sicherungsrings. Bei einzelnen Beispielen zeigte sich jedoch auch eine teilweise Fokussierung auf den Padding-Bereich. F√ºr solche F√§lle k√∂nnten weiterf√ºhrende Tests mit alternativer Vorverarbeitung oder gezielten Augmentationsstrategien sinnvoll sein, um den Einfluss des Randbereichs weiter zu reduzieren.\n",
    "\n",
    "Die deutlichsten Unterschiede zeigten sich in den Leave-One-Variant-Out-Experimenten. W√§hrend die Variante cc_red noch vergleichsweise solide generalisierte, f√ºhrte der Ausschluss von voesl_zitrone zu einer starken Verschlechterung der Performance. Die Analyse der Bildbeispiele verdeutlicht, dass cc_red sowohl in Farbe als auch in Form √Ñhnlichkeiten zu bereits im Training enthaltenen Varianten aufweist. Dagegen weicht voesl_zitrone insbesondere in der Geometrie deutlich von den √ºbrigen Verschl√ºssen ab, w√§hrend auch der Gelbton nur eingeschr√§nkt mit vorhandenen Varianten vergleichbar ist. Die Generalisierungsleistung h√§ngt somit stark von der strukturellen und farblichen N√§he neuer Varianten zu bekannten Trainingsbeispielen ab.\n",
    "\n",
    "Abschlie√üend l√§sst sich festhalten, dass das selbst entwickelte CNN auf bekannten Varianten sehr hohe Klassifikationsleistungen erreicht. Architekturvariationen innerhalb dieser Modellklasse f√ºhren nur zu marginalen Unterschieden. Die zentrale Herausforderung liegt weniger in der Feinabstimmung einzelner Layer oder Regularisierungstechniken, sondern vielmehr in der Robustheit gegen√ºber bislang unbekannten, stark abweichenden Varianten. Das entwickelte Notebook stellt dabei ein flexibles und erweiterbares Werkzeug dar, mit dem neue Varianten, zus√§tzliche Augmentationsstrategien oder alternative Architekturkonfigurationen systematisch getestet werden k√∂nnen. Es bildet somit eine belastbare Grundlage f√ºr weiterf√ºhrende Untersuchungen zur Variantenunabh√§ngigkeit und industriellen Skalierbarkeit des Ansatzes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024266fb-300e-47c8-8edd-c7b3660ca865",
   "metadata": {},
   "source": [
    "## 4.5 Experimente mit ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434399cf-5e3c-47c8-b48e-256e516cade9",
   "metadata": {},
   "source": [
    "In diesem Kapitel werden Experimente mit einer vortrainierten ResNet-Architektur durchgef√ºhrt, um den Einsatz von Transfer Learning f√ºr die vorliegende Bildklassifikationsaufgabe systematisch zu untersuchen. Ziel ist es, die Leistungsf√§higkeit eines ResNet-18-Modells unter unterschiedlichen Trainingsstrategien zu analysieren und mit den zuvor entwickelten CNN-Architekturen zu vergleichen.\n",
    "\n",
    "Ausgehend von einem Basismodell, bei dem zun√§chst ausschlie√ülich der Klassifikationskopf trainiert wird, wird die Modellkomplexit√§t schrittweise erh√∂ht. Hierzu werden verschiedene Fine-Tuning-Strategien betrachtet, bei denen ausgew√§hlte Teile des ResNet-Backbones wieder trainierbar gemacht werden. Dadurch kann untersucht werden, in welchem Umfang eine Anpassung tieferer Netzwerkschichten an die dom√§nenspezifischen Bilddaten zu einer Verbesserung der Klassifikationsleistung f√ºhrt.\n",
    "\n",
    "Alle Experimente folgen derselben Trainings- und Evaluationslogik wie die zuvor vorgestellten CNN-Modelle, um eine faire Vergleichbarkeit sicherzustellen. Die Bewertung erfolgt anhand der etablierten Metriken, wobei der Schwerpunkt insbesondere auf der False-Positive-Rate liegt, da diese f√ºr den betrachteten industriellen Anwendungsfall von zentraler Bedeutung ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfcb90-0600-4e7e-9ee5-de30fe825233",
   "metadata": {},
   "source": [
    "### 4.5.1 ResNet-Basismodell (nur Kopf trainierbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbaa7e-28c4-4609-b1c0-707567489342",
   "metadata": {},
   "source": [
    "Auf Basis der in Kapitel 4.1.2 implementierten ResNet-Klasse wird zun√§chst ein Basismodell instanziiert, das dem klassischen Transfer-Learning-Ansatz folgt. Dabei wird das vortrainierte ResNet-18-Modell mit ImageNet-Gewichten geladen, w√§hrend der gesamte Backbone eingefroren bleibt. Trainierbar ist ausschlie√ülich der neu definierte lineare Klassifikationskopf, der an die vorliegende bin√§re Klassifikationsaufgabe angepasst wird.\n",
    "\n",
    "Durch die Nutzung vortrainierter Gewichte startet das Training nicht von zuf√§llig initialisierten Parametern, sondern greift auf bereits gelernte generische Bildmerkmale zur√ºck. Im Vergleich zum selbst implementierten Baseline-CNN bietet die ResNet-Architektur zudem eine deutlich h√∂here Modellkapazit√§t sowie eine tiefere Struktur. Die enthaltenen Residual-Verbindungen erm√∂glichen stabile Gradientenfl√ºsse auch √ºber viele Schichten hinweg und reduzieren das Risiko von Vanishing-Gradient-Problemen.\n",
    "\n",
    "Dieses Basismodell dient als Referenz f√ºr die nachfolgenden Fine-Tuning-Experimente, in denen schrittweise weitere Teile des Backbones f√ºr das Training freigegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8fb9cc-bead-4a0f-827f-a78e92f4793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic-Variante: klassisches Transfer Learning (Backbone gefreezed, linearer Kopf)\n",
    "model_rn18_basic = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True,\n",
    "    unfreeze_from=None,\n",
    "    head=\"linear\",\n",
    "    dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a594f6-ef38-4570-b6f6-16e1144b85b4",
   "metadata": {},
   "source": [
    "Im Folgenden wird die Struktur des verwendeten ResNet-18-Modells anhand der Modellzusammenfassung detailliert betrachtet. Die Architektur besteht aus einem initialen Convolution-Block mit anschlie√üender Max-Pooling-Operation sowie vier aufeinanderfolgenden Residual-Stufen, die jeweils aus zwei sogenannten BasicBlocks bestehen. Innerhalb dieser Residual-Bl√∂cke erm√∂glichen Skip-Connections einen direkten Gradientenfluss √ºber mehrere Schichten hinweg und stabilisieren damit das Training tiefer Netzwerke.\n",
    "\n",
    "Insgesamt umfasst das ResNet-18-Modell rund 11,2 Millionen Parameter. Aufgrund des eingefrorenen Backbones sind jedoch lediglich 1.026 Parameter trainierbar, die ausschlie√ülich dem neu definierten linearen Klassifikationskopf entsprechen. Der √ºberwiegende Teil der Modellkapazit√§t wird somit aus den vortrainierten ImageNet-Gewichten √ºbernommen.\n",
    "\n",
    "Im Vergleich dazu besitzt das zuvor entwickelte tiefe Baseline-CNN lediglich etwa 665.000 Parameter, die vollst√§ndig trainierbar sind. Bereits dieser Gr√∂√üenvergleich verdeutlicht die deutlich h√∂here Modellkapazit√§t und strukturelle Komplexit√§t der ResNet-Architektur. W√§hrend beim Baseline-Modell s√§mtliche Merkmalsextraktion von Grund auf gelernt werden muss, greift das ResNet auf eine umfangreiche, vortrainierte Repr√§sentation zur√ºck, die nun lediglich durch einen angepassten Klassifikationskopf erg√§nzt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e082f-2dde-4576-96e4-3ee9e5a33fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_basic = model_rn18_basic.to(device)\n",
    "\n",
    "summary(\n",
    "    model_rn18_basic,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46d37b-5abc-4a29-bdcf-b8fd26d18d9c",
   "metadata": {},
   "source": [
    "Der folgende Code trainiert das zuvor instanziierte ResNet-Basismodell unter Verwendung derselben Trainings- und Evaluationspipeline wie beim Baseline-CNN, wodurch eine direkte Vergleichbarkeit der Ergebnisse sichergestellt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a636706-83e4-42ed-8e7a-a5c147d948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_rn18_basic, hist_rn18_basic, cm_rn18_basic, wrong_rn18_basic = train_model(\n",
    "    model_rn18_basic,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52645bd5-1ccf-4e59-a34a-23aea20e619c",
   "metadata": {},
   "source": [
    "In der nachfolgenden Codezelle werden das trainierte Modell sowie die zugeh√∂rigen Evaluationsartefakte (Trainingshistorie, Fehlklassifikationen und Konfusionsmatrix) aus dem Speicher geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53af4a6-552e-401b-ac90-a830067a78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_basic.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_basic.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_basic.to(device)\n",
    "model_rn18_basic.eval()\n",
    "\n",
    "hist_rn18_basic = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_basic.pth\"),weights_only=False)\n",
    "wrong_rn18_basic = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_basic.pth\"),weights_only=False)\n",
    "cm_rn18_basic = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_basic.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065412f9-8ca8-4ac7-8303-0288c147b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87df041-e425-40fc-990d-8de53100cd26",
   "metadata": {},
   "source": [
    "Die Trainingshistorie zeigt eine schnelle Konvergenz des Modells innerhalb der ersten Epochen. Sowohl Trainings- als auch Validierungsaccuracy steigen z√ºgig an und stabilisieren sich ab etwa Epoche 10 auf einem Niveau von rund 90‚Äì92 %. Auff√§llig ist, dass die Validierungsaccuracy √ºber weite Strecken leicht √ºber der Trainingsaccuracy liegt, was auf eine gute Generalisierungsf√§higkeit des eingefrorenen Backbones hindeutet.\n",
    "\n",
    "Der Trainings- und Validierungsloss sinken kontinuierlich und verlaufen insgesamt stabil, ohne deutliche Anzeichen von Overfitting. Gegen Ende des Trainings ist lediglich eine leichte Schwankung im Validierungsloss erkennbar, die sich jedoch nicht konsistent fortsetzt.\n",
    "\n",
    "Die False-Positive-Rate bewegt sich im Gro√üteil des Trainings im Bereich zwischen etwa 3 % und 7 %, zeigt jedoch einzelne Ausrei√üer in sp√§teren Epochen. Diese Schwankungen verdeutlichen, dass trotz insgesamt stabiler Accuracy die Klassifikationsgrenze in Bezug auf Fehlalarme nicht vollst√§ndig konstant ist, was f√ºr die sp√§tere Modellselektion relevant ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96ae23-fd62-4aad-9492-a17b5da6d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82441a5b-b527-49f0-bbf7-86a13ee632fd",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix best√§tigt die im Trainingsverlauf beobachtete stabile Modellleistung. Insgesamt wird eine Accuracy von 92,62 % erreicht. Von 2.579 tats√§chlich guten Verschl√ºssen werden 2.477 korrekt als ‚Äûgood‚Äú klassifiziert, w√§hrend 102 F√§lle f√§lschlicherweise als ‚Äûbad‚Äú eingestuft werden. Dies entspricht einer False-Positive-Rate von 3,96 %, die im industriellen Anwendungskontext von besonderer Bedeutung ist, da unn√∂tige Ausschleusungen korrekter Produkte direkte Auswirkungen auf nachgelagerte Prozesse haben k√∂nnen.\n",
    "\n",
    "Die tats√§chlich fehlerhaften Verschl√ºsse werden in 1.994 von 2.248 F√§llen korrekt erkannt. In 254 F√§llen erfolgt jedoch eine f√§lschliche Klassifikation als ‚Äûgood‚Äú (False Negatives). Damit treten Fehlklassifikationen fehlerhafter Verschl√ºsse h√§ufiger auf als f√§lschliche Zur√ºckweisungen guter Verschl√ºsse.\n",
    "\n",
    "Insgesamt zeigt das ResNet-Basismodell bereits ohne Fine-Tuning des Backbones eine robuste Klassifikationsleistung und eine vergleichsweise niedrige False-Positive-Rate. Dies deutet darauf hin, dass die vortrainierten Merkmalsrepr√§sentationen des ResNet-18 bereits gut auf die vorliegende Bilddom√§ne √ºbertragbar sind.  \n",
    "\n",
    "Trotz der insgesamt stabilen Klassifikationsleistung bleibt das ResNet-Basismodell im direkten Vergleich hinter den zuvor entwickelten Baseline-CNN-Architekturen zur√ºck. W√§hrend das selbst implementierte tiefe CNN eine h√∂here Gesamtaccuracy sowie eine geringere False-Positive-Rate erzielte, erreicht das hier betrachtete Modell trotz deutlich gr√∂√üerer Gesamtparameterzahl nicht die gleiche Leistungsf√§higkeit.\n",
    "\n",
    "Ein wesentlicher Grund hierf√ºr liegt in der Trainingsstrategie: Im vorliegenden Experiment wurden lediglich 1.026 Parameter des linearen Klassifikationskopfes trainiert, w√§hrend der gesamte Backbone eingefroren blieb. Die Merkmalsrepr√§sentation basiert somit vollst√§ndig auf den vortrainierten ImageNet-Gewichten. Da sich die ImageNet-Dom√§ne erheblich von der hier betrachteten industriellen Bilddom√§ne unterscheidet, sind die extrahierten Features nur eingeschr√§nkt auf die spezifischen Eigenschaften der Verschlussbilder abgestimmt.\n",
    "\n",
    "Das Ergebnis ist daher zwar insgesamt solide, bleibt jedoch deutlich unter der Leistung des vollst√§ndig trainierten, dom√§nenspezifisch optimierten Baseline-CNNs. Dies deutet darauf hin, dass eine reine Nutzung vortrainierter Features ohne weitere Anpassung an die Zielaufgabe nicht ausreicht, um die bestm√∂gliche Klassifikationsleistung zu erzielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218c188-3c3a-4e68-a761-cc5566dd45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_rn18_basic, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e3065-cad0-4a63-a235-f7f5482dc6d7",
   "metadata": {},
   "source": [
    "Die Auswertung der Fehlklassifikationen je Verschlussvariante zeigt eine deutliche Streuung zwischen den einzelnen Varianten. In den meisten F√§llen √ºberwiegen die False Negatives (Bad ‚Üí Good), das Modell tendiert also eher dazu, fehlerhafte Verschl√ºsse als ‚Äûgood‚Äú zu klassifizieren als umgekehrt korrekte Produkte f√§lschlicherweise zur√ºckzuweisen.\n",
    "\n",
    "Einzelne Varianten weisen deutlich erh√∂hte Fehlklassifikationszahlen auf, was auf eine unzureichende Anpassung der vortrainierten ImageNet-Features an die spezifische industrielle Bilddom√§ne hindeutet. Die heterogene Fehlerverteilung legt nahe, dass eine weitere Anpassung des Backbones durch Fine-Tuning erforderlich sein k√∂nnte, um variantenspezifische Merkmale besser zu erfassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9bf72-08a2-44ca-9875-653989c5cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b2e47-67f1-4809-baa5-7500dbdb2e78",
   "metadata": {},
   "source": [
    "Die visuelle Analyse der False Positives zeigt, dass es sich durchweg um tats√§chlich fehlerfreie Verschl√ºsse handelt, die vom Modell f√§lschlicherweise als ‚Äûbad‚Äú klassifiziert wurden. Auff√§llig ist erneut das h√§ufige Auftreten der Variante river_cola_black, die bereits beim Baseline-CNN erh√∂hte Fehlklassifikationen zeigte. In einzelnen Beispielen ist zus√§tzlich der Flaschenhals deutlich sichtbar, was m√∂glicherweise die Merkmalsextraktion beeinflusst. Insgesamt zeigt sich jedoch keine klare strukturelle Gemeinsamkeit: Die Verschl√ºsse variieren hinsichtlich Farbe, Beleuchtung und Sichtbarkeit der Verschlussnase. Die meisten Bilder sind gut ausgeleuchtet und weisen einen klar erkennbaren Kontrast auf.\n",
    "\n",
    "Auch bei den False Negatives ergibt sich kein eindeutiges Muster. Die fehlerhaften Verschl√ºsse umfassen ein breites Spektrum unterschiedlicher Varianten und Defekttypen. Zu beobachten sind unter anderem gerissene Sicherungsringe, untergeschobene Ringe, gr√∂√üere Spaltma√üe zwischen Ring und Deckel sowie deformierte Strukturen. Die Fehlklassifikationen betreffen somit unterschiedliche Fehlerauspr√§gungen und Farben, ohne dass ein einzelner dominanter Defekttyp erkennbar ist.\n",
    "\n",
    "Insgesamt deutet diese heterogene Fehlerverteilung darauf hin, dass das eingefrorene ResNet-Backbone keine klar systematische Fehlinterpretation eines bestimmten Bildmerkmals aufweist, sondern vielmehr Schwierigkeiten bei der konsistenten Erfassung dom√§nenspezifischer Geometrie- und Strukturmerkmale hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee3d81-3b68-4ce5-8d91-d319b5d70ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_basic = model_rn18_basic.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fab4eb-43c5-43a2-8ff3-1b475b359a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_basic_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/kcl_pink/bad/Cam1Side_0000000085.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_basic,\n",
    "    path_model_rn18_basic_gc1,\n",
    "    target_layer=target_layer_model_rn18_basic,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10663b-72cb-4e63-9414-010b85e2690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_basic_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/kcl_orange/good/Cam2Side_0000000106.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_basic,\n",
    "    path_model_rn18_basic_gc2,\n",
    "    target_layer=target_layer_model_rn18_basic,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c5c1f-af9b-41b5-ac72-feaf809a8a6e",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen verdeutlichen, dass das Modell seine Entscheidung nicht konsistent auf die tats√§chlich relevanten Defektbereiche st√ºtzt. Im ersten Beispiel (False Negative) wird der untergeschobene Sicherungsring nicht als dominantes Entscheidungsmerkmal hervorgehoben. Stattdessen konzentriert sich die Aktivierung teilweise auf andere Bildbereiche. Auff√§llig ist zudem, dass in beiden Beispielen der schwarze Padding-Bereich in die Entscheidungsfindung einbezogen wird ‚Äì wenn auch in unterschiedlicher Intensit√§t.\n",
    "\n",
    "Diese Beobachtungen best√§tigen, dass die eingefrorenen, auf ImageNet vortrainierten Merkmalsrepr√§sentationen nicht optimal auf die spezifischen geometrischen Strukturen der Verschlussringe abgestimmt sind. Um die Aufmerksamkeit st√§rker auf die funktional relevanten Bereiche zu lenken, ist eine weitere Anpassung des Backbones erforderlich. Im n√§chsten Schritt werden daher zus√§tzliche Netzwerkschichten f√ºr das Training freigegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca3efd-0082-4771-8f0e-853aa65212f8",
   "metadata": {},
   "source": [
    "### 4.5.2 Fine-Tuning: Unfreeze layer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b85f57-3920-4d01-bc24-144ccf44fd53",
   "metadata": {},
   "source": [
    "Aufbauend auf dem zuvor untersuchten Basismodell wird im n√§chsten Schritt ein partielles Fine-Tuning durchgef√ºhrt. Hierbei bleibt der Gro√üteil des ResNet-Backbones weiterhin eingefroren, jedoch wird die letzte Residual-Stufe (layer4) f√ºr das Training freigegeben. Dadurch k√∂nnen die h√∂herstufigen, semantisch komplexeren Merkmalsrepr√§sentationen an die spezifische industrielle Bilddom√§ne angepasst werden, w√§hrend die grundlegenden, allgemeineren Bildfeatures der fr√ºheren Schichten erhalten bleiben.  \n",
    "\n",
    "Ziel dieses Experiments ist es zu untersuchen, ob eine gezielte Anpassung der tiefsten Netzwerkschichten zu einer verbesserten Klassifikationsleistung f√ºhrt und insbesondere die zuvor beobachteten Fehlklassifikationen reduziert werden k√∂nnen, ohne das Modell vollst√§ndig neu zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcac1cd-6eaf-4ef7-bd03-847712662df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_u4 = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True,\n",
    "    unfreeze_from=\"layer4\",\n",
    "    head=\"linear\",\n",
    "    dropout=0.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa383bd2-c534-454a-95b1-a4544a62e870",
   "metadata": {},
   "source": [
    "Im Folgenden wird die Modellstruktur nach dem partiellen Fine-Tuning betrachtet. Durch das Freigeben von layer4 sind nun neben dem linearen Klassifikationskopf auch die Parameter der letzten Residual-Stufe trainierbar. Dadurch erh√∂ht sich die Anzahl der lernbaren Parameter signifikant, w√§hrend die fr√ºheren, allgemeineren Merkmalsstufen weiterhin eingefroren bleiben.  \n",
    "\n",
    "W√§hrend im Basismodell lediglich 1.026 Parameter trainiert wurden, sind nun insgesamt 8.394.754 Parameter lernbar. Damit wird ein erheblicher Teil der hochstufigen Merkmalsrepr√§sentationen an die spezifische Bilddom√§ne angepasst, ohne jedoch das gesamte Netzwerk vollst√§ndig neu zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560da0f-bcb1-4dbe-8545-f6dc9ceab18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_u4 = model_rn18_u4.to(device)\n",
    "\n",
    "summary(\n",
    "    model_rn18_u4,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea0a32-f126-4bc5-b2ca-98fba5147fbd",
   "metadata": {},
   "source": [
    "Der folgende Code trainiert das ResNet-Modell mit freigegebenem layer4 unter identischen Trainings- und Evaluationsbedingungen wie die vorherigen Modelle, um eine konsistente Vergleichbarkeit der Ergebnisse sicherzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10009d3f-c605-48c1-af3d-336cefd5204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trainieren\n",
    "model_rn18_u4, hist_rn18_u4, cm_rn18_u4, wrong_rn18_u4 = train_model(\n",
    "    model_rn18_u4,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9f5f1-f64c-4e45-b2a3-f37a4ba528eb",
   "metadata": {},
   "source": [
    "In der nachfolgenden Codezelle werden das trainierte Fine-Tuning-Modell sowie die zugeh√∂rigen Evaluationsartefakte aus dem Speicher geladen, um die Ergebnisse analysieren zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d22d3a-490a-48bd-8398-33981456bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_u4.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_u4.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_u4.to(device)\n",
    "model_rn18_u4.eval()\n",
    "\n",
    "hist_rn18_u4 = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_u4.pth\"),weights_only=False)\n",
    "wrong_rn18_u4 = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_u4.pth\"),weights_only=False)\n",
    "cm_rn18_u4 = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_u4.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12f57c-18ef-45a0-a690-8342af496a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_u4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4221d-758a-4957-b582-827b1e3f2e71",
   "metadata": {},
   "source": [
    "Im Vergleich zum zuvor betrachteten Basismodell zeigt das Fine-Tuning mit freigegebenem layer4 eine deutliche Leistungssteigerung. Sowohl Trainings- als auch Validierungsaccuracy steigen bereits nach wenigen Epochen auf nahezu 99,5‚Äì100 % an und bleiben √ºber den gesamten Trainingsverlauf hinweg stabil. Parallel dazu sinkt der Trainings- und Validierungsloss deutlich unter die Werte des Basismodells.\n",
    "\n",
    "Auch die False-Positive-Rate reduziert sich signifikant und bewegt sich √ºber weite Strecken nahe null. Insgesamt deutet der Trainingsverlauf darauf hin, dass die gezielte Anpassung der hochstufigen Merkmalsrepr√§sentationen eine wesentlich bessere Abstimmung auf die spezifische Bilddom√§ne erm√∂glicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b9689-9d0e-4a68-82f0-3a50e88751ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_u4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d0830-8984-4d6f-b12f-d2de8a17cf0f",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix best√§tigt die deutliche Leistungssteigerung durch das Fine-Tuning. Mit einer Accuracy von 99,81 % wird nahezu eine fehlerfreie Klassifikation erreicht. Von 2.579 tats√§chlich guten Verschl√ºssen werden 2.573 korrekt erkannt, lediglich 6 F√§lle werden f√§lschlicherweise als ‚Äûbad‚Äú klassifiziert. Dies entspricht einer False-Positive-Rate von nur 0,23 %.\n",
    "\n",
    "Auch die Anzahl der False Negatives reduziert sich drastisch: Nur 3 fehlerhafte Verschl√ºsse werden als ‚Äûgood‚Äú eingestuft, w√§hrend 2.245 korrekt erkannt werden. Im Vergleich zum Basismodell stellt dies eine massive Reduktion beider Fehlertypen dar.\n",
    "\n",
    "Die Ergebnisse zeigen deutlich, dass die gezielte Anpassung der hochstufigen Merkmalsrepr√§sentationen eine nahezu vollst√§ndige Dom√§nenanpassung erm√∂glicht und die zuvor beobachteten Schw√§chen des eingefrorenen Backbones weitgehend eliminiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521d4b3-fd42-4033-8684-3a040c426fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_rn18_u4, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c577c5-d6f3-4422-8650-387132ac3d7f",
   "metadata": {},
   "source": [
    "Die Analyse der Fehlklassifikationen je Verschlussvariante zeigt eine drastische Reduktion der Fehler gegen√ºber dem Basismodell. F√ºr die meisten Varianten treten keine Fehlklassifikationen mehr auf. Die verbleibenden Fehler konzentrieren sich auf sehr wenige Einzelf√§lle, wobei insbesondere river_cola_black noch mehrere False Positives aufweist.\n",
    "\n",
    "False Negatives treten nur noch vereinzelt und ohne erkennbares systematisches Muster auf. Insgesamt zeigt sich eine nahezu vollst√§ndige Stabilisierung der Klassifikationsleistung √ºber alle Varianten hinweg.\n",
    "\n",
    "Im Vergleich zum eingefrorenen Backbone wird deutlich, dass das partielle Fine-Tuning von layer4 die variantenspezifischen Schw√§chen weitgehend eliminiert und eine konsistente Dom√§nenanpassung erm√∂glicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f91644-3cbb-4638-a271-ee0ac1a173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_u4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3224a7-aea2-477a-b71a-dc49be350818",
   "metadata": {},
   "source": [
    "Die detaillierte Analyse der verbleibenden Fehlklassifikationen zeigt ein deutlich klareres Muster als im Basismodell. S√§mtliche False Positives betreffen die Variante river_cola_black. In vier der sechs F√§lle ist prim√§r der Flaschenhals sichtbar, w√§hrend der eigentliche Verschluss mit Sicherungsring nicht erscheint. In den beiden √ºbrigen F√§llen sind Verschluss und Sicherungsring zwar erkennbar, jedoch ist der Kontrast vergleichsweise schwach. Die Fehlklassifikationen lassen sich somit zumindest teilweise durch eine eingeschr√§nkte Sichtbarkeit des relevanten Bereichs erkl√§ren.\n",
    "\n",
    "Auch bei den False Negatives zeigt sich eine nachvollziehbare Struktur. Im Fall von kcl_pink ist der Sicherungsring nicht sichtbar, sodass der entscheidungsrelevante Bereich fehlt. In den beiden weiteren Beispielen sind die Defekte nur subtil ausgepr√§gt und visuell wenig offensichtlich. Eine klare, dominante Fehlinterpretation eines bestimmten Defekttyps ist nicht erkennbar.\n",
    "\n",
    "Insgesamt best√§tigt die Analyse die deutlich verbesserte Modellleistung im Vergleich zum Basismodell. Die verbleibenden Fehlklassifikationen sind √ºberwiegend auf eingeschr√§nkte Sichtbarkeit oder sehr subtile Defektauspr√§gungen zur√ºckzuf√ºhren und nicht mehr auf systematische Schw√§chen der Merkmalsextraktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14904207-63a7-4e92-ba45-09fe53ee701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_u4 = model_rn18_u4.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c0b52-be8c-4851-b8e3-db8f4c20cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_u4_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/river_cola_black/good/Cam2Side_0000000086.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_u4,\n",
    "    path_model_rn18_u4_gc1,\n",
    "    target_layer=target_layer_model_rn18_u4,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d095cf6-c412-4344-a77f-31e7f806c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_u4_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/topsport_green/bad/Cam2Top_0000000058.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_u4,\n",
    "    path_model_rn18_u4_gc2,\n",
    "    target_layer=target_layer_model_rn18_u4,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019d8c6-f306-4c24-bf01-3fad635b7227",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen zeigen, dass sich die Aufmerksamkeit des Modells nun deutlich st√§rker auf den Bereich des Verschlussrings konzentriert. Im Fall des False Positives bei river_cola_black liegt die Aktivierung im zentralen Ringbereich, obwohl kein tats√§chlicher Defekt vorliegt. Die Fehlentscheidung scheint hier weniger auf irrelevanten Bildanteilen zu beruhen, sondern vielmehr auf einer fehlerhaften Interpretation lokaler Strukturen.\n",
    "\n",
    "Auch beim False Negative ist die Aktivierung klar im Bereich des Sicherungsrings lokalisiert. Das Modell betrachtet somit grunds√§tzlich den relevanten Bildausschnitt, interpretiert die dort vorhandene Struktur jedoch als unkritisch. Im Gegensatz zum Basismodell wird der Padding-Bereich nicht mehr dominant einbezogen.\n",
    "\n",
    "Insgesamt deutet dies darauf hin, dass die hochstufigen Merkmalsrepr√§sentationen bereits erfolgreich an die Dom√§ne angepasst wurden. Um zu untersuchen, ob eine weitere Verbesserung durch eine noch st√§rkere Dom√§nenanpassung m√∂glich ist, wird im n√§chsten Schritt ein weitergehendes Unfreezing des Backbones betrachtet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db32be-5c50-4007-be3d-b8263b1eb349",
   "metadata": {},
   "source": [
    "### 4.5.3 Unfreeze layer3 und layer4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077717a-e984-46fc-8774-a1fe0be82558",
   "metadata": {},
   "source": [
    "Im n√§chsten Schritt wird das Fine-Tuning weiter ausgeweitet, indem zus√§tzlich zu layer4 nun auch layer3 f√ºr das Training freigegeben wird. Dadurch k√∂nnen nicht nur die h√∂chsten, sondern auch mittlere Repr√§sentationsebenen des Netzwerks an die spezifische Bilddom√§ne angepasst werden. Ziel dieses Experiments ist es zu untersuchen, ob eine noch st√§rkere Dom√§nenanpassung zu einer weiteren Reduktion der verbleibenden Fehlklassifikationen f√ºhrt oder ob bereits mit dem partiellen Fine-Tuning von layer4 ein Leistungsplateau erreicht wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c47f66-4293-4e8d-876f-c1d60aec3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning: layer3 + layer4 + Kopf trainierbar\n",
    "model_rn18_u3 = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=True,\n",
    "    unfreeze_from=\"layer3\",\n",
    "    head=\"linear\",\n",
    "    dropout=0.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5e9d3-0f1a-4ba4-b7fd-90324a067bff",
   "metadata": {},
   "source": [
    "Im Folgenden wird die Modellstruktur nach dem erweiterten Fine-Tuning betrachtet. Durch das zus√§tzliche Freigeben von layer3 erh√∂ht sich die Anzahl der trainierbaren Parameter nochmals deutlich, da nun sowohl mittlere als auch hochstufige Merkmalsrepr√§sentationen an die Ziel-Dom√§ne angepasst werden.\n",
    "\n",
    "W√§hrend beim vorherigen Modell mit freigegebenem layer4 rund 8,39 Millionen Parameter trainierbar waren, sind es nun 10.494.466 Parameter. Damit wird ein Gro√üteil des Backbones aktiv optimiert, w√§hrend lediglich die fr√ºhesten, generischen Feature-Extraktionsstufen eingefroren bleiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb6dbf-b0ec-4756-ad27-345174116cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_u3 = model_rn18_u3.to(device)\n",
    "\n",
    "summary(\n",
    "    model_rn18_u3,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f86c79-fbea-4fa3-96eb-3d782284ad8c",
   "metadata": {},
   "source": [
    "Der folgende Code trainiert das ResNet-Modell mit freigegebenem layer3 und layer4 unter identischen Trainingsbedingungen wie die vorherigen Varianten, um die Auswirkungen des erweiterten Fine-Tunings direkt vergleichen zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a3d3c-60e2-4992-bc9f-b4613d895f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_rn18_u3, hist_rn18_u3, cm_rn18_u3, wrong_rn18_u3 = train_model(\n",
    "    model_rn18_u3,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab63c2b-0007-41d1-995c-687c6c4bef77",
   "metadata": {},
   "source": [
    "In der anschlie√üenden Codezelle werden das trainierte Modell sowie die zugeh√∂rigen Evaluationsartefakte geladen, um die Ergebnisse des erweiterten Fine-Tunings systematisch auszuwerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87c586-00f4-446f-bea7-965b2bf7fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_u3.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_u3.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_u3.to(device)\n",
    "model_rn18_u3.eval()\n",
    "\n",
    "hist_rn18_u3 = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_u3.pth\"),weights_only=False)\n",
    "wrong_rn18_u3 = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_u3.pth\"),weights_only=False)\n",
    "cm_rn18_u3 = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_u3.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84b0e6-18c6-4b91-a659-624b4e87e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_u3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d7d65-e5e3-4b95-afa3-eb9c2f815d49",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf zeigt insgesamt eine sehr hohe und stabile Modellleistung. Trainings- und Validierungsaccuracy erreichen erneut nahezu 100 % und verlaufen √ºber weite Strecken sehr dicht beieinander. Der Loss sinkt schnell ab und verbleibt auf einem niedrigen Niveau, wobei nur geringe Schwankungen in der Validierung erkennbar sind.\n",
    "\n",
    "Im Vergleich zum Modell mit ausschlie√ülich freigegebenem layer4 ist jedoch keine signifikante weitere Leistungssteigerung erkennbar. Auch die False-Positive-Rate bewegt sich weiterhin nahe null, ohne eine deutliche zus√§tzliche Verbesserung zu zeigen.\n",
    "\n",
    "Dies deutet darauf hin, dass bereits das Fine-Tuning von layer4 eine nahezu vollst√§ndige Dom√§nenanpassung erm√∂glicht hat und das zus√§tzliche Freigeben von layer3 nur einen marginalen Einfluss auf die Gesamtperformance besitzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48611b-6b70-4aa0-be55-5ef6d5fd2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_u3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbd98b-e2b2-4bb5-aeeb-e0b904a87e62",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix best√§tigt die sehr hohe Gesamtleistung des Modells. Mit einer Accuracy von 99,85 % wird eine minimal h√∂here Genauigkeit als beim Modell mit ausschlie√ülich freigegebenem layer4 erreicht. Von 2.579 tats√§chlich guten Verschl√ºssen werden 2.574 korrekt klassifiziert, lediglich 5 F√§lle werden f√§lschlicherweise als ‚Äûbad‚Äú eingestuft (FPR = 0,19 %).\n",
    "\n",
    "Auch die Anzahl der False Negatives reduziert sich weiter auf nur 2 F√§lle. Im direkten Vergleich zeigt sich jedoch, dass die Verbesserung gegen√ºber dem vorherigen Fine-Tuning-Schritt nur marginal ausf√§llt. Dies deutet darauf hin, dass bereits das Freigeben von layer4 den Gro√üteil der notwendigen Dom√§nenanpassung erm√∂glicht hat und das zus√§tzliche Unfreezing von layer3 lediglich einen sehr geringen zus√§tzlichen Nutzen bringt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af80d8a-ac7d-4738-9e31-ac35bc53cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_rn18_u3, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b3597-7a9c-498c-99a8-664f1b4ee422",
   "metadata": {},
   "source": [
    "Die Verteilung der Fehlklassifikationen unterscheidet sich nur geringf√ºgig von der vorherigen Variante mit ausschlie√ülich freigegebenem layer4. Die False Positives treten weiterhin ausschlie√ülich bei river_cola_black auf, w√§hrend die False Negatives vereinzelt bei einzelnen anderen Varianten erscheinen.\n",
    "\n",
    "Insgesamt ist die Fehleranzahl minimal reduziert, jedoch bewegt sich die Leistung auf einem sehr √§hnlichen Niveau wie beim vorherigen Fine-Tuning-Schritt. Das zus√§tzliche Unfreezing von layer3 f√ºhrt somit zu keiner grundlegenden Leistungssteigerung, sondern lediglich zu einer leichten Stabilisierung bzw. marginalen Verbesserung einzelner Kennzahlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3653463-1d01-4191-ad2d-e75f0d5ee0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_u3 = model_rn18_u3.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddafb5f-a360-4d57-8b81-6e0a875639ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_u3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a38302-3188-45c6-9183-1cdc136e1fef",
   "metadata": {},
   "source": [
    "Die detaillierte Betrachtung der Fehlklassifikationen zeigt ein sehr √§hnliches Muster wie in der vorherigen Ausbaustufe.\n",
    "\n",
    "Die False Positives treten weiterhin ausschlie√ülich bei river_cola_black auf. In mehreren F√§llen ist der Flaschenhals st√§rker sichtbar als der eigentliche Verschluss mit Sicherungsring, sodass der relevante Bereich nur eingeschr√§nkt erkennbar ist. In anderen Beispielen ist der Kontrast insgesamt gering, was die klare Abgrenzung des Sicherungsrings erschwert.\n",
    "\n",
    "Die False Negatives verteilen sich auf einzelne Varianten. Bei cc_lightgray fehlt der Sicherungsring vollst√§ndig, was vom Modell nicht korrekt als Defekt erkannt wurde. Im Fall von flirt_orange ist der Defekt visuell nur schwach ausgepr√§gt, sodass die fehlerhafte Einstufung als ‚Äûgood‚Äú nachvollziehbar erscheint.\n",
    "\n",
    "Damit best√§tigt sich, dass das Modell auf einem sehr hohen Leistungsniveau arbeitet und die verbleibenden Fehler vor allem in visuell anspruchsvollen Randf√§llen auftreten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858a1c2-066a-4b02-b221-779a9e67ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_u3 = model_rn18_u3.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0c7c7-597c-4be6-859c-7f757c742140",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_u3_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/river_cola_black/good/Cam2Side_0000000086.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_u3,\n",
    "    path_model_rn18_u3_gc1,\n",
    "    target_layer=target_layer_model_rn18_u3,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec8041-6820-4c09-ba04-4d8911f546d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_u3_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_lightgray/bad/Cam1Side_0000000229.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_u3,\n",
    "    path_model_rn18_u3_gc2,\n",
    "    target_layer=target_layer_model_rn18_u3,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4725203-bc70-4938-be21-7de4ed2b4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_u3_gc3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/flirt_orange/bad/Cam1Side_0000000325.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_u3,\n",
    "    path_model_rn18_u3_gc3,\n",
    "    target_layer=target_layer_model_rn18_u3,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa9c49-4e26-4a74-bab9-a87cc7597949",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen zeigen, dass das Modell seine Entscheidung √ºberwiegend auf den relevanten Verschlussbereich st√ºtzt. Beim True-Negative-Beispiel (river_cola_black) liegt der Fokus klar auf der Verschlussgeometrie, insbesondere im Bereich der ‚ÄûNase‚Äú bzw. der ringnahen Struktur.\n",
    "\n",
    "Bei den False-Negatives (cc_lightgray, flirt_orange) wird ebenfalls der Verschlussbereich hervorgehoben, jedoch ohne dass der fehlende bzw. deformierte Sicherungsring als eindeutig diskriminierendes Merkmal genutzt wird. Damit best√§tigt sich, dass die verbleibenden Fehler vor allem bei visuell schwierigen Randf√§llen auftreten und weniger durch eine falsche Fokussierung auf irrelevante Bildbereiche erkl√§rt werden k√∂nnen.\n",
    "\n",
    "Im n√§chsten Schritt wird daher ein vollst√§ndiges Fine-Tuning durchgef√ºhrt, bei dem alle Schichten des ResNet-Backbones trainierbar sind, um auch die fr√ºheren Merkmalsextraktionsstufen st√§rker an die spezifische Bilddom√§ne anzupassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e27a1-c021-4d36-a693-325f6097597d",
   "metadata": {},
   "source": [
    "### 4.5.4 Vollst√§ndiges Fine-Tuning (alle Schichten trainierbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83b30a-f0ad-4d17-8938-bf99e6c3c0cb",
   "metadata": {},
   "source": [
    "Im letzten Schritt wird ein vollst√§ndiges Fine-Tuning durchgef√ºhrt, bei dem s√§mtliche Schichten des ResNet-Backbones trainierbar sind. Ziel ist es, die vortrainierten ImageNet-Merkmale vollst√§ndig an die spezifische Bilddom√§ne der Tethered-Caps anzupassen und zu untersuchen, ob sich dadurch eine weitere Leistungssteigerung erzielen l√§sst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cc94e-b926-4a98-b6b3-40523fa069d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vollst√§ndiges Fine-Tuning: alles trainierbar\n",
    "model_rn18_full = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False,   # Backbone nicht einfrieren\n",
    "    unfreeze_from=\"layer1\",  # wird effektiv ignoriert/ist hier nur explizit\n",
    "    head=\"linear\",\n",
    "    dropout=0.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97677a-2407-479f-8de0-94f7df2182a6",
   "metadata": {},
   "source": [
    "Die folgende Modell√ºbersicht zeigt, dass nun s√§mtliche Parameter des ResNet-18 trainierbar sind, sodass das Netzwerk vollst√§ndig an die Zielaufgabe angepasst werden kann. Insgesamt werden damit 11.177.538 Parameter optimiert, was die maximale Anpassungsf√§higkeit, aber auch das h√∂chste Overfitting-Risiko innerhalb der getesteten Varianten bedeutet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f82270-a32e-45d3-8a02-6798b886b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_full = model_rn18_full.to(device)\n",
    "\n",
    "summary(\n",
    "    model_rn18_full,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b84fa-b964-4b7a-947a-1f8e8f980b3e",
   "metadata": {},
   "source": [
    "Im letzten Schritt wird das vollst√§ndig freigegebene ResNet mit reduzierter Lernrate trainiert, um die vortrainierten Gewichte vorsichtig an die spezifische Bilddom√§ne anzupassen und zu starke Gewichtsanpassungen zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75c8ea-d175-4fba-8c27-153b762be054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model_rn18_full, hist_rn18_full, cm_rn18_full, wrong_rn18_full = train_model(\n",
    "    model_rn18_full,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-4, # reduzierte Lernrate, da vollst√§ndiges Fine-Tuning aller vortrainierten Schichten\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3c1b6-9021-431e-9f1f-284c1dede758",
   "metadata": {},
   "source": [
    "Zur konsistenten Auswertung werden anschlie√üend das gespeicherte Modell sowie Trainingshistorie, Fehlklassifikationen und Konfusionsmatrix erneut geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c68bf-6163-4ea1-aaba-6afe800d18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_full.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_full.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_full.to(device)\n",
    "model_rn18_full.eval()\n",
    "\n",
    "hist_rn18_full = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_full.pth\"),weights_only=False)\n",
    "wrong_rn18_full = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_full.pth\"),weights_only=False)\n",
    "cm_rn18_full = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_full.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8c697-9f43-4320-92a1-b140d32f7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94480de-0331-4dad-8e0a-767f168e89ae",
   "metadata": {},
   "source": [
    "Das vollst√§ndige Fine-Tuning f√ºhrt zu einem sehr schnellen Konvergenzverhalten. Trainings- und Validierungsgenauigkeit stabilisieren sich bereits nach wenigen Epochen nahe 100 %, w√§hrend der Loss kontinuierlich auf sehr niedrige Werte f√§llt. Die False-Positive-Rate bleibt √ºber weite Teile des Trainings bei 0 % und zeigt nur vereinzelte, sehr geringe Ausschl√§ge, was auf eine insgesamt stabile und robuste Anpassung aller Schichten hinweist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d2469-cb4f-4708-91e0-bd540572f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092bef3e-7b0d-48d3-8794-eb13725614e4",
   "metadata": {},
   "source": [
    "Das vollst√§ndige Fine-Tuning erzielt die bislang beste Gesamtleistung. Mit einer Accuracy von 99,92 % und einer FPR von 0,16 % wird die Zahl der False Positives weiter reduziert. Besonders hervorzuheben ist, dass keine einzige fehlerhafte Flasche (bad) mehr als gut klassifiziert wird (0 False Negatives). Damit zeigt das Modell eine nahezu perfekte Trennung beider Klassen im Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdd213-4f16-4f2d-9b5a-2e9d1d1e2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_rn18_full, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b4b2f-6865-4ed2-add5-a69f088125fa",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen sind nun nahezu vollst√§ndig eliminiert. Es treten ausschlie√ülich wenige False Positives bei river_cola_black auf, w√§hrend keine False Negatives mehr auftreten. Damit zeigt das vollst√§ndig feinabgestimmte Modell nicht nur eine sehr hohe Gesamtgenauigkeit, sondern auch eine stabile Performance √ºber nahezu alle Verschlussvarianten hinweg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822f78a-4c25-405b-92f1-21c9b5e2ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0309e1-6e72-4647-818b-4cad68d0d215",
   "metadata": {},
   "source": [
    "Die verbleibenden Fehlklassifikationen treten ausschlie√ülich bei Bildern auf, in denen √ºberwiegend der Flaschenhals sichtbar ist und der relevante Bereich des Sicherungsrings gar nicht im Bild enthalten ist. In diesen F√§llen fehlen dem Modell die entscheidenden visuellen Merkmale zur Beurteilung des Manipulationszustands, sodass die Fehlentscheidung nachvollziehbar erscheint.\n",
    "\n",
    "Damit h√§ngen die letzten Fehler weniger mit einer falschen Merkmalsinterpretation zusammen, sondern vielmehr mit einer eingeschr√§nkten Bildinformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214924ca-177e-43e2-9dcd-24fbbe10dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_full = model_rn18_full.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16517c-3e96-4a25-9050-eef2a8692b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/river_cola_black/good/Cam1Side_0000000343.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full,\n",
    "    path_model_rn18_full_gc1,\n",
    "    target_layer=target_layer_model_rn18_full,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029f28d-089d-4180-9aff-f5e210f5e8d7",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierung f√ºr ein korrekt klassifiziertes Beispiel (True Negative) zeigt, dass das Modell seine Entscheidung √ºberwiegend auf den Bereich des Sicherungsrings konzentriert. Die Aktivierungen liegen entlang der relevanten Ringstruktur und nicht im Hintergrund oder im schwarzen Paddingbereich.\n",
    "Dies deutet darauf hin, dass das vollst√§ndige Fine-Tuning eine noch st√§rkere Fokussierung auf die tats√§chlich entscheidungsrelevanten Merkmale erm√∂glicht.  \n",
    "\n",
    "Im Vergleich zu den zuvor teilweise eingefrorenen Varianten wirkt die Aktivierung homogener und deckt den funktional relevanten Bereich konsistenter ab, was mit der verbesserten Fehlerrate korrespondiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff82303-8c25-4a65-af9b-2e6262f576df",
   "metadata": {},
   "source": [
    "W√§hrend das Backbone nun vollst√§ndig trainierbar ist, bleibt der Kopf bislang vergleichsweise einfach. Durch die Erweiterung um eine zus√§tzliche versteckte Schicht wird gepr√ºft, ob eine h√∂here Modellkapazit√§t im Klassifikationskopf die Entscheidungsgrenze weiter verfeinern kann, selbst wenn die bisherigen Ergebnisse bereits nahe am Optimum liegen.\n",
    "\n",
    "Ziel ist hier weniger eine deutliche Leistungssteigerung, sondern die systematische √úberpr√ºfung, ob das lineare Mapping bereits ausreichend ist oder ob noch geringf√ºgige Verbesserungen, beispielsweise bei einzelnen Fehlklassifikationen, m√∂glich sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16934c8-6e0b-4091-9e14-c0e4ec4dd779",
   "metadata": {},
   "source": [
    "### 4.5.5 MLP-Head (Kapazit√§t im Kopf erh√∂hen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1b8bb-7469-47cd-ab8a-4bcead8bf8d8",
   "metadata": {},
   "source": [
    "Nachdem im vorherigen Abschnitt das vollst√§ndige Fine-Tuning des Backbones betrachtet wurde, wird nun der Fokus auf den Klassifikationskopf gelegt. Ziel ist es zu untersuchen, ob eine Erh√∂hung der Kapazit√§t im Kopf durch ein MLP mit versteckter Schicht zus√§tzliche Leistungsreserven erschlie√üen kann, obwohl das Gesamtsystem bereits sehr hohe Genauigkeiten erreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdf7f0-e7f0-4e72-88f4-c0beb918b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_best_mlp = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False,\n",
    "    unfreeze_from=\"layer1\",  \n",
    "    head=\"mlp\",\n",
    "    fc_hidden=256,\n",
    "    dropout=0.1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a662d68-d6dc-4239-a202-9aa509286de8",
   "metadata": {},
   "source": [
    "Die folgende tabellarische Darstellung zeigt die angepasste Architektur des Modells mit dem erweiterten MLP-Kopf.\n",
    "\n",
    "Im Vergleich zur linearen Variante besteht der Klassifikationskopf nun aus zwei voll verbundenen Schichten mit einer dazwischenliegenden ReLU-Aktivierung sowie Dropout. Konkret wird der 512-dimensionale Featurevektor zun√§chst auf 256 Neuronen projiziert und anschlie√üend auf die zwei Zielklassen abgebildet.\n",
    "\n",
    "Dadurch erh√∂ht sich die Anzahl der trainierbaren Parameter leicht, w√§hrend das Backbone unver√§ndert bleibt. Der Kopf erh√§lt somit zus√§tzliche Modellkapazit√§t, um nichtlineare Entscheidungsgrenzen im Merkmalsraum zu modellieren, anstatt lediglich eine lineare Trennung vorzunehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533a748-4b79-4f0a-86ab-35ba25449efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_best_mlp = model_rn18_best_mlp.to(device)\n",
    "\n",
    "summary(\n",
    "    model_rn18_best_mlp,\n",
    "    input_size=(32, 3, 256, 256),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbb230-a1fb-443f-9088-4f77b37b6e5a",
   "metadata": {},
   "source": [
    "Im Folgenden wird das Modell mit erweitertem MLP-Kopf unter identischen Trainingsbedingungen erneut trainiert, um den Einfluss der erh√∂hten Kapazit√§t im Klassifikationskopf zu evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7bcfa-cc8a-4614-89a4-751fe1ae481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_best_mlp, hist_rn18_best_mlp, cm_rn18_best_mlp, wrong_rn18_best_mlp = train_model(\n",
    "    model_rn18_best_mlp,\n",
    "    train_loader_std,\n",
    "    val_loader_std,\n",
    "    test_loader_std,\n",
    "    num_epochs=30,\n",
    "    lr=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1e579-8a70-4ecd-8f5d-dda2d45a667e",
   "metadata": {},
   "source": [
    "Anschlie√üend werden das gespeicherte Modell sowie die zugeh√∂rigen Trainingshistorien, Fehlklassifikationen und die Konfusionsmatrix zur weiteren Analyse geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348999d-381a-49c5-9198-830e3cad3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_best_mlp.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_best_mlp.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_best_mlp.to(device)\n",
    "model_rn18_best_mlp.eval()\n",
    "\n",
    "hist_rn18_best_mlp = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_best_mlp.pth\"),weights_only=False)\n",
    "wrong_rn18_best_mlp = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_best_mlp.pth\"),weights_only=False)\n",
    "cm_rn18_best_mlp = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_best_mlp.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1140533-bc5b-4f4a-8dbf-6a9737dea701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de0d32-329f-4b0e-9e4c-e5cefa026806",
   "metadata": {},
   "source": [
    "Im Trainingsverlauf zeigt sich insgesamt eine sehr stabile Performance mit nahezu perfekter Trainings- und Validierungsgenauigkeit √ºber die meisten Epochen hinweg. Der kurzzeitige Einbruch der Validierungs-Accuracy sowie der korrespondierende Peak im Validierungs-Loss gegen Ende des Trainings deuten auf eine instabile Einzel-Epoche hin, wirken sich jedoch nicht nachhaltig auf die Gesamtleistung aus. Auch die False-Positive-Rate bleibt √ºber weite Strecken bei 0 %, was auf eine insgesamt sehr robuste Entscheidungsgrenze schlie√üen l√§sst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b45fd-4eb4-426c-9c75-1de5c868b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e2022-1a5c-4640-85d9-1e3f30d85d50",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix zeigt mit 99,90 % Accuracy und einer FPR von 0,16 % eine weiterhin sehr hohe Gesamtleistung. Gegen√ºber dem vollst√§ndig feinjustierten linearen Kopf ergibt sich jedoch keine relevante Verbesserung; es verbleiben vier False Positives und ein False Negative. Die zus√§tzliche Kapazit√§t im MLP-Head f√ºhrt somit zu keiner messbaren Leistungssteigerung, was darauf hindeutet, dass das lineare Mapping f√ºr diese Aufgabe bereits ausreichend ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87593ccd-7b19-4b97-acd4-e786beaf6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassifications_by_variant(wrong_rn18_best_mlp, variant_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4bfce1-108d-456a-aea4-2be266523a34",
   "metadata": {},
   "source": [
    "Die Fehlklassifikationen konzentrieren sich weiterhin auf einzelne Varianten. Vier False Positives treten bei river_cola_black auf, w√§hrend ein False Negative bei flirt_orange verbleibt. Eine systematische Schw√§che √ºber mehrere Varianten hinweg ist nicht erkennbar; die Fehler wirken eher vereinzelte Ausrei√üer als strukturelles Generalisierungsproblem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f566570-2d25-4acd-80a2-ddffadc2186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9076852-20a6-4703-8884-9818c6fb9805",
   "metadata": {},
   "source": [
    "Die vier False Positives entsprechen erneut Bildern, auf denen ausschlie√ülich der Flaschenhals sichtbar ist, w√§hrend der relevante Bereich des Sicherungsrings nicht enthalten ist. Die Fehlentscheidung ist hier daher weniger auf eine falsche Merkmalsinterpretation des Modells zur√ºckzuf√ºhren als auf fehlende visuelle Information im entscheidenden Bereich.\n",
    "\n",
    "Das einzelne False Negative bei flirt_orange zeigt hingegen einen nur subtil ausgepr√§gten Defekt, der visuell nicht eindeutig erkennbar ist. Der Fehler wirkt daher eher grenzwertig als systematisch und deutet nicht auf eine klare strukturelle Schw√§che des Modells hin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb805b-407a-498e-b84d-302d70121db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_best_mlp = model_rn18_best_mlp.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0453d1-b4e2-4dd8-aa77-9f7d65450176",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_best_mlp_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/flirt_orange/bad/Cam1Side_0000000325.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_best_mlp,\n",
    "    path_model_rn18_best_mlp_gc1,\n",
    "    target_layer=target_layer_model_rn18_best_mlp,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b36bfa-095f-4a08-9252-f4b004184c60",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierung des False Negatives zeigt, dass das Modell den linken Bereich des Verschlusses besonders stark gewichtet. Der tats√§chliche Defekt ist jedoch visuell nur schwach ausgepr√§gt und hebt sich kaum vom regul√§ren Strukturmuster ab. Das Modell fokussiert damit auf einen plausiblen, strukturell relevanten Bereich, interpretiert die dortigen Merkmale jedoch als ‚Äûgood‚Äú. Die Fehlklassifikation wirkt daher nicht zuf√§llig, sondern als Grenzfall zwischen regul√§rer Struktur und tats√§chlichem Defekt. Im folgenden Kapitel wird erneut ein Leave-One-Variant-Out-Experiment f√ºr cc_red durchgef√ºhrt. Ziel ist es, wie beim Basis-CNN die Generalisierungsf√§higkeit auf eine unbekannte Variante zu √ºberpr√ºfen, wobei als Ausgangspunkt wieder das Modell mit linearem Kopf verwendet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639512ea-13d8-4f89-8dff-0909fe86969d",
   "metadata": {},
   "source": [
    "### 4.5.6 ResNet - Generalisierungstest mittels Leave-One-Variant-Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608dc6d-0092-45a8-858f-03d69ff28314",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die Generalisierungsf√§higkeit des ResNet-Modells mithilfe eines Leave-One-Variant-Out-Experiments untersucht. Analog zum Vorgehen im Baseline-CNN-Kapitel wird jeweils eine vollst√§ndige Verschlussvariante aus dem Training ausgeschlossen und ausschlie√ülich f√ºr den Test verwendet.\n",
    "\n",
    "Zur besseren Vergleichbarkeit orientiert sich die Durchf√ºhrung methodisch an der Baseline-Ausarbeitung. Es werden dieselben Varianten betrachtet, n√§mlich cc_red und voesl_zitrone, sodass die Ergebnisse architektur√ºbergreifend gegen√ºbergestellt werden k√∂nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33417e5d-9359-461a-895b-4582e747ee5d",
   "metadata": {},
   "source": [
    "<u><strong>Variante cc_red</strong></u>  \n",
    "\n",
    "F√ºr die Variante cc_red wird im ersten Schritt das ResNet-Modell erneut vollst√§ndig freigegeben und mit linearem Kopf instanziiert. Anschlie√üend erfolgt das Training ohne Einbeziehung dieser Variante, sodass sie ausschlie√ülich zur Bewertung der Generalisierungsf√§higkeit dient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cff34b-c50a-41a4-b7b5-b6d880324aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vollst√§ndiges Fine-Tuning: alles trainierbar\n",
    "model_rn18_full_lovo_cc_red = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False,   # Backbone nicht einfrieren\n",
    "    unfreeze_from=\"layer1\",  # wird effektiv ignoriert/ist hier nur explizit\n",
    "    head=\"linear\",\n",
    "    dropout=0.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16071187-a24f-4548-9d7d-0c075b6e7f99",
   "metadata": {},
   "source": [
    "Im n√§chsten Schritt wird das Modell im Leave-One-Variant-Out-Setup f√ºr die Variante cc_red trainiert, wobei diese Variante w√§hrend des Trainings vollst√§ndig ausgeschlossen bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480b4fa-6de2-4b49-b3c2-77a1e607dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_full_lovo_cc_red, hist_rn18_full_lovo_cc_red, cm_rn18_full_lovo_cc_red, wrong_rn18_full_lovo_cc_red = train_model(\n",
    "    model_rn18_full_lovo_cc_red,\n",
    "    train_loader_ccred,\n",
    "    val_loader_ccred,\n",
    "    test_loader_ccred,\n",
    "    num_epochs=30,\n",
    "    lr=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d7f0b-3094-4c18-9e7c-cceec10e77cc",
   "metadata": {},
   "source": [
    "Anschlie√üend werden das gespeicherte Modell sowie die zugeh√∂rigen Trainingsverl√§ufe, Fehlklassifikationen und die Konfusionsmatrix zur Auswertung wieder geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff97c73-a0d2-4fb2-aaf9-275d44f03e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_full_lovo_cc_red.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_full_lovo_cc_red.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_full_lovo_cc_red.to(device)\n",
    "model_rn18_full_lovo_cc_red.eval()\n",
    "\n",
    "hist_rn18_full_lovo_cc_red = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_full_lovo_cc_red.pth\"),weights_only=False)\n",
    "wrong_rn18_full_lovo_cc_red = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_full_lovo_cc_red.pth\"),weights_only=False)\n",
    "cm_rn18_full_lovo_cc_red = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_full_lovo_cc_red.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3e264-d5a4-4dd9-949f-57842e3e5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_full_lovo_cc_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a72b2-def1-42df-a7d5-42e0a72fb0fd",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf des vollst√§ndig feinjustierten ResNet-18 zeigt eine sehr schnelle Konvergenz innerhalb der ersten Epochen. Bereits nach wenigen Trainingsdurchl√§ufen stabilisieren sich sowohl Trainings- als auch Validierungsaccuracy auf einem Niveau nahe 100 %. Parallel dazu sinkt der Trainingsverlust kontinuierlich und verbleibt dauerhaft auf sehr niedrigem Niveau.\n",
    "\n",
    "Die Validierungsmetriken verlaufen weitgehend synchron zu den Trainingswerten. Zwischen beiden Kurven ist √ºber den gesamten Trainingsprozess hinweg kein signifikanter Abstand erkennbar, was auf eine gute Generalisierungsf√§higkeit ohne ausgepr√§gtes Overfitting hindeutet. Einzelne leichte Schwankungen in der Validierungs-FPR sind sichtbar, bewegen sich jedoch durchgehend in einem sehr niedrigen Bereich.\n",
    "\n",
    "Im Vergleich zum Leave-One-Variant-Out-Experiment des Baseline-CNN zeigt sich ein insgesamt stabilerer und schnellerer Konvergenzverlauf. W√§hrend beim Baseline-Modell in fr√ºhen Epochen teils st√§rkere Schwankungen insbesondere in der False-Positive-Rate beobachtet wurden, stabilisiert sich das ResNet-Modell fr√ºher und auf niedrigerem Niveau. Dies deutet auf eine robustere Merkmalsrepr√§sentation durch das vortrainierte Backbone hin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0a607-2464-40c4-9705-7e0e8ee92f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_full_lovo_cc_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2dd66-17ca-4070-8c57-05e0e383815a",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix des ResNet-Modells im Leave-One-Variant-Out-Szenario f√ºr die Variante cc_red zeigt eine Accuracy von 98,59 % bei einer False-Positive-Rate von lediglich 0,33 %. Insgesamt wurden nur 7 tats√§chlich gute Verschl√ºsse f√§lschlicherweise als fehlerhaft klassifiziert.\n",
    "\n",
    "Im direkten Vergleich zum Baseline-CNN ergibt sich eine deutliche Reduktion der False Positives (42 ‚Üí 7). Dies entspricht einer Verringerung um rund 83 %. Da False Positives in einem industriellen Anwendungsszenario unn√∂tigen Ausschuss und damit direkte Kosten verursachen, stellt diese Verbesserung einen signifikanten praktischen Vorteil dar.\n",
    "\n",
    "Demgegen√ºber ist beim ResNet ein leichter Anstieg der False Negatives (29 ‚Üí 37) zu beobachten. Das Modell √ºbersieht somit geringf√ºgig mehr tats√§chlich fehlerhafte Verschl√ºsse. Insgesamt √ºberwiegt jedoch der Vorteil der stark reduzierten False-Positive-Rate, sodass das ResNet-Modell im LOVO-Szenario f√ºr cc_red als leistungsf√§higer eingestuft werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfa785-50e2-4757-b7cc-ebeae01c61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_full_lovo_cc_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66bf7e-97dc-4cb8-89d2-874e45038c79",
   "metadata": {},
   "source": [
    "Eine detaillierte Betrachtung der Fehlklassifikationen zeigt klare strukturelle Gemeinsamkeiten innerhalb beider Fehlertypen.\n",
    "\n",
    "Bei den sieben f√§lschlicherweise als fehlerhaft klassifizierten guten Verschl√ºssen ist die charakteristische Verschlussnase in allen F√§llen deutlich sichtbar. Mit Ausnahme eines einzelnen Beispiels weisen die Bilder zudem eine sehr √§hnliche Verschlussposition und Beleuchtungssituation auf. Die Bildkomposition wirkt insgesamt homogen. Das letzte Beispiel hebt sich durch eine leicht abweichende Positionierung beziehungsweise Perspektive ab.\n",
    "\n",
    "Die als gut klassifizierten fehlerhaften Verschl√ºsse zeigen hingegen ein konsistentes Muster. In allen dargestellten F√§llen handelt es sich um deformierte Verschl√ºsse. Andere Fehlertypen treten innerhalb dieser Gruppe nicht auf. Auff√§llig ist au√üerdem, dass die Nase in diesen Beispielen nicht sichtbar ist. Die Beleuchtung variiert zwischen helleren und dunkleren Aufnahmen, ein systematischer Zusammenhang mit der Helligkeit ist jedoch nicht erkennbar. Das dominierende gemeinsame Merkmal ist die Deformation als Fehlertyp.\n",
    "\n",
    "Insgesamt deutet diese Analyse darauf hin, dass deformierte Strukturen ohne klar sichtbare Referenzmerkmale teilweise nicht zuverl√§ssig erkannt werden, w√§hrend die sichtbare Nase in bestimmten Konstellationen eine starke Rolle in der Entscheidungsfindung zu spielen scheint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbf273-9b4a-4e30-97f6-23ce38a2ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_full_lovo_cc_red = model_rn18_full_lovo_cc_red.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf45171-e4b6-42f8-9f79-b097af744e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_cc_red_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/good/Cam1Side_0000001009.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_cc_red,\n",
    "    path_model_rn18_full_lovo_cc_red_gc1,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_cc_red,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39baf06-c8a8-4059-ae42-a882c5cd6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_cc_red_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/good/Cam2Side_0000000029.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_cc_red,\n",
    "    path_model_rn18_full_lovo_cc_red_gc2,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_cc_red,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b70d0-ccb7-4545-84b6-8d79b7003823",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_cc_red_gc3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/bad/Cam2Side_0000000640.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_cc_red,\n",
    "    path_model_rn18_full_lovo_cc_red_gc3,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_cc_red,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979758f8-bfd1-44dd-86df-b57121fe3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_cc_red_gc4 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/bad/Cam2Top_0000000646.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_cc_red,\n",
    "    path_model_rn18_full_lovo_cc_red_gc4,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_cc_red,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4afca-c168-4fe3-b78a-04d47f11482e",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen zeigen, dass das ResNet-Modell seine Entscheidungsfindung auf die relevanten Bildbereiche konzentriert. Sowohl bei korrekt als auch bei falsch klassifizierten Beispielen liegt die Aktivierung √ºberwiegend im Bereich des Verschlussrings und der strukturellen Merkmale des Tethered-Caps-Mechanismus. Hintergrundbereiche sowie schwarze Randfl√§chen werden nicht als dominante Entscheidungsgrundlage genutzt.\n",
    "\n",
    "Bei den False-Positive-Beispielen konzentriert sich die Aktivierung insbesondere auf die Region der sichtbaren Nase sowie angrenzende Ringstrukturen. Dies best√§tigt die zuvor beobachtete Annahme, dass dieses Merkmal eine starke Rolle in der Modellentscheidung spielt. Bei den False-Negative-Beispielen liegt die Aktivierung im Bereich der deformierten Ringstruktur. Auch hier werden somit die relevanten geometrischen Ver√§nderungen betrachtet, wenngleich diese nicht in allen F√§llen zur korrekten Klassifikation f√ºhren.\n",
    "\n",
    "Insgesamt zeigt die Grad-CAM-Analyse, dass das Modell keine irrelevanten Artefakte oder Randbereiche als prim√§re Entscheidungsbasis nutzt, sondern strukturell sinnvolle Bildregionen fokussiert.  \n",
    "\n",
    "Im Leave-One-Variant-Out-Szenario f√ºr die Variante cc_red zeigt das vollst√§ndig feinjustierte ResNet-18 eine sehr hohe Generalisierungsf√§higkeit. Trotz vollst√§ndigem Ausschluss dieser Variante aus dem Training erreicht das Modell eine hohe Gesamtgenauigkeit bei gleichzeitig deutlich reduzierter False-Positive-Rate im Vergleich zum Baseline-CNN.\n",
    "\n",
    "Die Fehleranalyse verdeutlicht, dass die verbleibenden Fehlklassifikationen strukturell konsistent sind und nicht zuf√§llig verteilt auftreten. Die Grad-CAM-Visualisierungen best√§tigen zudem, dass das Modell relevante geometrische Merkmale des Verschlusses ber√ºcksichtigt und seine Entscheidungen auf plausiblen Bildregionen basieren.\n",
    "\n",
    "Zusammenfassend kann das ResNet-Modell im LOVO-Szenario f√ºr cc_red als robust und varianten√ºbergreifend generalisierungsf√§hig bewertet werden. Die Kombination aus hoher Accuracy, niedriger False-Positive-Rate und nachvollziehbarer Entscheidungsstruktur spricht f√ºr eine stabile Merkmalsrepr√§sentation auch bei zuvor nicht gesehenen Verschlussvarianten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efdb5e-c76b-4622-a9b2-46e6a3199e58",
   "metadata": {},
   "source": [
    "<u><strong>Variante voesl_zitrone</strong></u>  \n",
    "\n",
    "F√ºr die Variante voesl_zitrone wird analog zur vorherigen Untersuchung erneut ein Leave-One-Variant-Out-Experiment durchgef√ºhrt. Ziel ist es, die Generalisierungsf√§higkeit des vollst√§ndig feinjustierten ResNet auch auf diese bislang ungesehene Verschlussvariante zu √ºberpr√ºfen. Die Modellarchitektur entspricht dabei dem zuvor verwendeten ResNet mit linearem Kopf, sodass die Ergebnisse direkt vergleichbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bae6da-4eae-4008-8ed2-500a48bd9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vollst√§ndiges Fine-Tuning: alles trainierbar\n",
    "model_rn18_full_lovo_voesl_zitrone = ResNetClassifier(\n",
    "    num_classes=2,\n",
    "    arch=\"resnet18\",\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False,   # Backbone nicht einfrieren\n",
    "    unfreeze_from=\"layer1\",  # wird effektiv ignoriert/ist hier nur explizit\n",
    "    head=\"linear\",\n",
    "    dropout=0.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e4925-a58c-4c7d-97cb-9ce557f753f8",
   "metadata": {},
   "source": [
    "Im folgenden Schritt wird das ResNet im Leave-One-Variant-Out-Setup f√ºr die Variante voesl_zitrone trainiert, wobei diese Variante im Training vollst√§ndig ausgeschlossen bleibt und ausschlie√ülich zur Evaluation dient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7c79a-2652-4d22-b80a-2e5bd66ed55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rn18_full_lovo_voesl_zitrone, hist_rn18_full_lovo_voesl_zitrone, cm_rn18_full_lovo_voesl_zitrone, wrong_rn18_full_lovo_voesl_zitrone = train_model(\n",
    "    model_rn18_full_lovo_voesl_zitrone,\n",
    "    train_loader_voesl,\n",
    "    val_loader_voesl,\n",
    "    test_loader_voesl,\n",
    "    num_epochs=30,\n",
    "    lr=1e-4, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1c1e7-6609-4f9a-a150-d81064008944",
   "metadata": {},
   "source": [
    "Anschlie√üend werden das gespeicherte Bestmodell sowie Trainingshistorie, Fehlklassifikationen und Konfusionsmatrix geladen, um die Generalisierungsleistung auf die zuvor ungesehene Variante detailliert auszuwerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8aa575-78e7-41f7-bf1f-4e2b0f7197a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten des bereits trainierten Modells abrufen\n",
    "# Basis-Pfade\n",
    "model_path = \"Modelle/ResNet/model_rn18_full_lovo_voesl_zitrone.pth\"\n",
    "eval_base_path = \"Modellauswertungen/ResNet\"\n",
    "\n",
    "model_rn18_full_lovo_voesl_zitrone.load_state_dict(\n",
    "    torch.load(model_path, map_location=device)\n",
    ")\n",
    "model_rn18_full_lovo_voesl_zitrone.to(device)\n",
    "model_rn18_full_lovo_voesl_zitrone.eval()\n",
    "\n",
    "hist_rn18_full_lovo_voesl_zitrone = torch.load(os.path.join(eval_base_path, \"HIST/hist_rn18_full_lovo_voesl_zitrone.pth\"),weights_only=False)\n",
    "wrong_rn18_full_lovo_voesl_zitrone = torch.load(os.path.join(eval_base_path, \"WRONG/wrong_rn18_full_lovo_voesl_zitrone.pth\"),weights_only=False)\n",
    "cm_rn18_full_lovo_voesl_zitrone = np.load(os.path.join(eval_base_path, \"CM/cm_rn18_full_lovo_voesl_zitrone.npy\"))\n",
    "\n",
    "print(\"Modell + Hist + Wrong erfolgreich geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367318f-4fc6-4230-84d2-e8dc05dd78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hist_rn18_full_lovo_voesl_zitrone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2328577-4be5-457d-8048-cc3513a38571",
   "metadata": {},
   "source": [
    "Der Trainingsverlauf des ResNet-Modells im Leave-One-Variant-Out-Szenario f√ºr die Variante voesl_zitrone zeigt insgesamt eine stabile Konvergenz. Die Trainingsaccuracy steigt bereits in den ersten Epochen auf ein Niveau nahe 100 % und verbleibt dort √ºber den weiteren Verlauf. Auch die Validierungsaccuracy stabilisiert sich nach kurzer Zeit auf einem hohen Niveau.\n",
    "\n",
    "Vereinzelt treten einzelne Ausrei√üer im Validierungsverlust sowie in der False-Positive-Rate auf. Diese sind jedoch zeitlich begrenzt und f√ºhren nicht zu einem nachhaltigen Leistungsabfall. Insgesamt verlaufen Trainings- und Validierungskurven weitgehend synchron, was auf eine robuste Anpassung des vollst√§ndig feinjustierten Backbones hindeutet.\n",
    "\n",
    "Im direkten Vergleich zum Baseline-CNN zeigt sich ein deutlich stabilerer Trainingsprozess. W√§hrend das Baseline-Modell im LOVO-Szenario f√ºr voesl_zitrone starke Schwankungen in Validierungsaccuracy, Verlust und False-Positive-Rate aufweist, verl√§uft das Training des ResNet weitgehend konsistent. Insbesondere die extremen Ausschl√§ge der False-Positive-Rate, die beim Baseline-CNN in einzelnen Epochen beobachtet wurden, treten beim ResNet nicht in vergleichbarer Form auf.\n",
    "\n",
    "Diese Beobachtung deutet darauf hin, dass das vortrainierte und vollst√§ndig feinjustierte ResNet-Modell eine deutlich robustere Merkmalsrepr√§sentation lernt, die weniger sensitiv gegen√ºber der strukturell abweichenden Variante voesl_zitrone ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a603e0a-603c-464a-9d76-d615f5828a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_rn18_full_lovo_voesl_zitrone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161ac60-cd59-4e27-928b-3148c78437bd",
   "metadata": {},
   "source": [
    "Die Konfusionsmatrix des ResNet-Modells im Leave-One-Variant-Out-Szenario f√ºr die Variante voesl_zitrone zeigt eine Accuracy von 71,27 % bei einer False-Positive-Rate von 65,77 %. Von 1084 tats√§chlich guten Verschl√ºssen werden 713 f√§lschlicherweise als fehlerhaft klassifiziert. Gleichzeitig werden alle 1398 tats√§chlich fehlerhaften Verschl√ºsse korrekt erkannt.\n",
    "\n",
    "Im Vergleich zum Baseline-CNN stellt dies dennoch eine deutliche Verbesserung dar. W√§hrend das Baseline-Modell im LOVO-Szenario f√ºr voesl_zitrone alle Bilder unabh√§ngig von ihrer tats√§chlichen Klasse als ‚Äûbad‚Äú klassifizierte und somit eine False-Positive-Rate von 100 % aufwies, reduziert das ResNet diesen Effekt signifikant. Dennoch bleibt die Fehlklassifikationsrate im Bereich der guten Verschl√ºsse hoch.\n",
    "\n",
    "Auff√§llig ist, dass keine False Negatives auftreten. Das Modell klassifiziert somit keinen tats√§chlich fehlerhaften Verschluss als ‚Äûgood‚Äú. Die Generalisierungsproblematik √§u√üert sich ausschlie√ülich in einer starken √úberklassifikation als ‚Äûbad‚Äú.\n",
    "\n",
    "Die im Vergleich zu cc_red deutlich schlechtere Leistung l√§sst sich plausibel durch die strukturelle Andersartigkeit der Variante voesl_zitrone erkl√§ren. Diese unterscheidet sich sowohl in der Farbgebung als auch in der geometrischen Auspr√§gung des Verschlusses von den √ºbrigen Varianten. Da diese Merkmalskombination im Training vollst√§ndig fehlte, interpretiert das Modell gro√üe Teile dieser bislang unbekannten Struktur offenbar als anomal oder fehlerhaft.\n",
    "\n",
    "Zusammenfassend zeigt das ResNet im LOVO-Szenario f√ºr voesl_zitrone eine verbesserte, jedoch weiterhin eingeschr√§nkte Generalisierungsf√§higkeit gegen√ºber stark abweichenden Verschlussvarianten. Im direkten Vergleich zu cc_red wird deutlich, dass die Generalisierungsleistung ma√ügeblich vom strukturellen Abstand der ausgeschlossenen Variante zum Trainingsdatensatz abh√§ngt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d6453-820e-41d2-9bee-c54fc3e312d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassification_examples(wrong_rn18_full_lovo_voesl_zitrone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61355381-20b5-4c0e-8ac9-65d585daca1c",
   "metadata": {},
   "source": [
    "Die Betrachtung der f√§lschlicherweise als ‚Äûbad‚Äú klassifizierten guten Verschl√ºsse zeigt ein deutlich homogeneres Bild als im Fall von cc_red. Die Beleuchtung ist in nahezu allen dargestellten Beispielen sehr √§hnlich, sodass kein offensichtlicher Zusammenhang zwischen Helligkeit oder Kontrast und der Fehlklassifikation erkennbar ist.\n",
    "\n",
    "Auch hinsichtlich der geometrischen Merkmale ergibt sich kein klar trennendes Muster. Sowohl Bilder mit sichtbarer Nase als auch ohne deutlich erkennbare Nase werden f√§lschlicherweise als ‚Äûbad‚Äú klassifiziert. Eine eindeutige strukturelle Besonderheit, die ausschlie√ülich in den Fehlbeispielen auftritt, l√§sst sich visuell nicht identifizieren.\n",
    "\n",
    "Auff√§llig ist vielmehr, dass die gesamte Variante eine deutlich abweichende Farbgebung und Ringstruktur aufweist. Die gelb-gr√ºne Farbcharakteristik sowie die spezifische geometrische Auspr√§gung des Rings unterscheiden sich sichtbar von den im Training verwendeten Varianten. Da diese Kombination im Trainingsdatensatz vollst√§ndig fehlte, scheint das Modell gro√üe Teile dieser bislang unbekannten Erscheinungsform als anomal zu interpretieren.\n",
    "\n",
    "Im Gegensatz zu cc_red liegt das Problem somit weniger in der Fehlinterpretation einzelner Merkmale, sondern eher in einer globalen Abweichung der gesamten Variante vom gelernten Merkmalsraum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095b495-cef2-44d4-a31a-905e43313539",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_model_rn18_full_lovo_voesl_zitrone = model_rn18_full_lovo_voesl_zitrone.model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449edd89-6749-4e67-ba39-fc63fe436c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_voesl_zitrone_gc1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/voesl_zitrone/good/Cam1Side_0000000211.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_voesl_zitrone,\n",
    "    path_model_rn18_full_lovo_voesl_zitrone_gc1,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_voesl_zitrone,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0e0a9-0736-4322-8a80-892da84cce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_voesl_zitrone_gc2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/voesl_zitrone/good/Cam2Top_0000000234.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_voesl_zitrone,\n",
    "    path_model_rn18_full_lovo_voesl_zitrone_gc2,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_voesl_zitrone,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d51f00-52f7-40bc-8f3c-c2b68c37d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_rn18_full_lovo_voesl_zitrone_gc3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/voesl_zitrone/good/Cam1Top_0000000043.jpg\"\n",
    "\n",
    "show_gradcam_from_path(\n",
    "    model_rn18_full_lovo_voesl_zitrone,\n",
    "    path_model_rn18_full_lovo_voesl_zitrone_gc3,\n",
    "    target_layer=target_layer_model_rn18_full_lovo_voesl_zitrone,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c3d32-ce5d-495b-ac6c-50d942ceb7ce",
   "metadata": {},
   "source": [
    "Die Grad-CAM-Visualisierungen der False-Positive-Beispiele zeigen, dass das Modell seine Aktivierungsschwerpunkte √ºberwiegend im oberen Bereich des Verschlussrings setzt. Besonders stark aktiviert werden die vertikalen Rippenstrukturen sowie √úbergangsbereiche zwischen Ring und Deckel.\n",
    "\n",
    "Auff√§llig ist, dass die Aktivierung nicht punktuell auf eine klar identifizierbare Deformation oder lokale Unregelm√§√üigkeit gerichtet ist, sondern gr√∂√üere zusammenh√§ngende Bereiche der Ringstruktur umfasst. Dies deutet darauf hin, dass das Modell die gesamte strukturelle Auspr√§gung dieser Variante als anomal interpretiert, anstatt einzelne konkrete Defektmerkmale zu identifizieren.\n",
    "\n",
    "Im Gegensatz zur Variante cc_red liegt hier kein isoliertes Fehlfokus-Muster auf einem spezifischen Detail wie der Nase vor. Vielmehr scheint die globale Form- und Strukturabweichung des Rings die Entscheidungsfindung zu dominieren. Die charakteristische Rippengeometrie der Variante voesl_zitrone wird offenbar nicht als normale Variantenpr√§gung, sondern als fehlerhaftes Muster bewertet.\n",
    "\n",
    "Die Grad-CAM-Analyse best√§tigt damit die zuvor aus der Konfusionsmatrix abgeleitete Beobachtung: Die Fehlklassifikation beruht weniger auf einer falschen Interpretation einzelner lokaler Merkmale, sondern auf einer generellen Verschiebung im gelernten Merkmalsraum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e82d4-7979-4684-bfbc-96e1d4d97f8a",
   "metadata": {},
   "source": [
    "### 4.5.7 Zusammenfassung Experimente mit ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709daa3-63f6-4f14-b7d7-4a2a432ced06",
   "metadata": {},
   "source": [
    "Im zweiten Teil der Untersuchung wurde ein vortrainiertes ResNet-18 im Sinne des Transfer Learnings eingesetzt, um die Leistungsf√§higkeit im Vergleich zum zuvor entwickelten Basic CNN systematisch zu analysieren. Ziel war es, zu untersuchen, inwiefern sich durch die Nutzung eines vortrainierten Backbones und durch schrittweises Unfreezen einzelner Layer die Klassifikationsleistung verbessern l√§sst.\n",
    "\n",
    "Zun√§chst wurde ausschlie√ülich der Klassifikationskopf trainiert. Anschlie√üend wurden sukzessive weitere Layer des Backbones freigegeben, bis schlie√ülich das gesamte Netzwerk feinjustiert wurde. Dabei zeigte sich eindeutig, dass mit zunehmender Dom√§nenanpassung eine deutliche Leistungssteigerung erzielt werden konnte. Das vollst√§ndig feinjustierte Modell erreichte sehr hohe Accuracy-Werte bei gleichzeitig niedriger False-Positive-Rate. Bereits ohne tiefgreifende Architektur√§nderungen konnten somit exzellente Ergebnisse erzielt werden.\n",
    "\n",
    "Erg√§nzend wurde ein alternativer MLP-Head getestet. Der Schwerpunkt lag jedoch klar auf dem Transfer-Learning-Ansatz und der Analyse des stufenweisen Unfreezens.\n",
    "\n",
    "Abseits der Leave-One-Variant-Out-Experimente zeigte das beste ResNet-Modell eine sehr stabile und nahezu perfekte Klassifikationsleistung. Die wenigen Fehlklassifikationen waren √ºberwiegend auf ung√ºnstige Bildausschnitte zur√ºckzuf√ºhren, nicht auf strukturelle Fehlinterpretationen des Verschlussmechanismus.\n",
    "\n",
    "Im Vergleich zum Basic CNN erwies sich das ResNet als deutlich robuster und trainingsstabiler. W√§hrend das Basic CNN teils starke Schwankungen im Validierungsverlauf zeigte, konvergierte das ResNet konsistenter und mit besserer Gesamtleistung.\n",
    "\n",
    "Die Leave-One-Variant-Out-Experimente verdeutlichen, dass die Generalisierungsf√§higkeit stark vom strukturellen Abstand der ausgeschlossenen Variante abh√§ngt. F√ºr cc_red konnte eine nahezu vollst√§ndige Generalisierung erreicht werden. F√ºr die strukturell und farblich deutlich abweichende Variante voesl_zitrone blieb die Leistung eingeschr√§nkt, lag jedoch klar √ºber der des Basic CNN. Die Grad-CAM-Analysen zeigen, dass relevante Bildbereiche ber√ºcksichtigt werden, die Fehlentscheidungen jedoch auf eine Repr√§sentationsgrenze bei starkem Domain Shift zur√ºckzuf√ºhren sind.\n",
    "\n",
    "Insgesamt best√§tigt der ResNet-Ansatz, dass Transfer Learning mit schrittweisem Fine-Tuning eine deutlich leistungsf√§higere und robustere L√∂sung darstellt als das Training eines CNN von Grund auf. Gleichzeitig zeigen die Ergebnisse, dass die Generalisierung auf stark abweichende Varianten weiterhin eine Herausforderung bleibt.\n",
    "\n",
    "Das modular aufgebaute Notebook erm√∂glicht jedoch problemlos weiterf√ºhrende Experimente, etwa mit erweiterten Data-Augmentation-Strategien, alternativen Backbones, unterschiedlichen Klassifikationsk√∂pfen, Regularisierungsverfahren oder variierenden Vorverarbeitungsans√§tzen wie verzerrtem Resize statt Padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d07ead-0df7-4cbf-ada5-fe377a888f95",
   "metadata": {},
   "source": [
    "## 4.6 Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2d94c-1b40-4c49-87f1-675b9e1675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gradcam_bad1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/fanta_blue/bad/Cam1Side_0000000150.jpg\"\n",
    "path_gradcam_bad2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/fanta_orange/bad/Cam1Side_0000000151.jpg\"\n",
    "path_gradcam_bad3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/kcl_red/bad/Cam2Top_0000000216.jpg\"\n",
    "path_gradcam_bad4 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/sprite_green/bad/Cam1Top_0000000005.jpg\"\n",
    "path_gradcam_bad5 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/topsport_yellow/bad/Cam2Side_0000000060.jpg\"\n",
    "path_gradcam_bad6 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/voesl_zitrone/bad/Cam2Top_0000000119.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f02794-6563-4d4c-a34f-8d6d913401fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gradcam_good1 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/aquapanna_grey/good/Cam1Side_0000000057.jpg\"\n",
    "path_gradcam_good2 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/topsport_green/good/Cam1Side_0000000039.jpg\"\n",
    "path_gradcam_good3 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/cc_red/good/Cam1Side_0000000178.jpg\"\n",
    "path_gradcam_good4 = \"/home/archive/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures/fanta_blue/good/Cam1Side_0000000046.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3724c-03bb-41ff-8431-3628b6c99f4a",
   "metadata": {},
   "source": [
    "### Bad1 - Fanta Blue - Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee2bdf-2e60-4f0b-a427-b965a4b750f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024ac20-58fd-4876-b4d9-06775eaf167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4413eed-41bf-43db-98b0-14c9b32e1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b8cc8-67e1-4123-95d2-c35eac45b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696fb72-1d48-482c-9204-0047ac11ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f56a7a-656b-45bf-8ffd-6c1b5b0d7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7c237-59ce-4484-b978-2bb83568211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c12e4-c9dc-4afc-9592-19212f4d6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a7f80-7b06-4617-8c79-c3df7001616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_bad1,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef590e1-b6eb-42d1-b45c-8a2b5aef107c",
   "metadata": {},
   "source": [
    "### Bad2 - Fanta Orange - Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470504a-034b-4bb7-be40-f4f6b074c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9fb76-246a-4c79-8683-32cf6f3c3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5b836-36ff-4b31-9387-93db67b9e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06147d50-6f76-4ad7-bffc-d837717338bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3635a-7ecd-4620-ac32-97b2394e8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d2ac8-dc42-48c1-a1af-51f23d58b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030ce1f-da48-402e-89cf-bad92e4d2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ff2af-25c8-4b4a-835a-73cacea738f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4f6b5-0ede-44c8-82f5-e9664bf6f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_bad2,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5e118-f3fd-4bb9-9ac0-27fac34e6739",
   "metadata": {},
   "source": [
    "### Bad3 - kcl_red - Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb592c-2c1d-4371-b4b1-8a86541627b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08c7c3-cf0c-494a-8ec6-0f736176c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50801137-1e77-48ad-8a0d-e06c1c153849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84dc7e-d933-4029-8605-e78e1eb207a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169972a-7100-4127-ba8a-8291ac1ba93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73b0fb-1780-469c-9478-bde66d8a186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1252c2b-70be-43b3-acbe-8c9891605323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c1775-a87d-4572-8586-40d32405cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b610749-97c1-45c3-b409-6ebf59abf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_bad3,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f75ce-0545-4baf-9a0d-38b7e592cf5b",
   "metadata": {},
   "source": [
    "### Bad4 - sprite_green - Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afee82-2524-4578-93e8-525728719a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0a06e-b38d-4df5-95de-a27a8e9f9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a0224-db3d-479a-b130-6207afa6375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53053769-5db1-4eff-a7f5-a6aa5d902735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b7494-3301-4af5-b7f9-43ad879bcead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96772671-77ff-4e63-b768-8b53ab5d6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c7e49-d550-4f55-a758-7f1487b86235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c32bf-7354-443d-8394-b83686d0341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e45ca4-e039-4ff6-8ca0-36c6ed0f372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_bad4,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e1cb3-2d3b-4dcb-a436-99efe87b101c",
   "metadata": {},
   "source": [
    "### Bad5 - topsport_yellow - Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c802523-5044-47c1-b96e-2978942e5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a75b4-3a27-4e8d-8251-a58643c996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d6bd3-0f08-47dc-9986-df4eb0de0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61171ace-eead-4744-aabd-5b6be5cc44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404794b-46e6-4936-8c6e-03677cdd3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63fde4-e56c-4cc6-a33e-3e7c2eb4d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567acab-a9ee-400e-bb9d-77fb7923a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196d2a8-682e-4bc1-acda-d8aefba1ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2057004-fe09-4b00-9bc6-513f1f6ad299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_bad5,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f1ee1-0a94-4a02-986c-ee1e5f8867db",
   "metadata": {},
   "source": [
    "### Bad6 - voesl_zitrone - Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb7174-fc57-4fa9-9a65-f8eaac6b122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e6fd9-2e49-446d-8935-496aff2169ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857f320-2173-4e59-bd60-05a27cf502a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d657bbc-3b1f-487a-b902-035656044940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ebb38-d257-4267-a507-32e1e594be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb44fc5-f1d1-4257-a3f0-289bd8278f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1974dc-f94d-4d67-9d8a-3763d30f6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d53d55-2576-4d9e-b25e-56f03abd65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baece04d-446c-413c-a462-fda4f7d67ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_bad6,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760842e-8a40-472f-8668-5c682e6e5e68",
   "metadata": {},
   "source": [
    "### Good1 - aquana_grey - Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfc3f2-4d89-48d5-969f-6344f0cf38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eef133-415e-49f0-a286-f0d674f71000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58fba1-5823-4a62-a169-579c924d9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefe Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34e626-6b73-44db-bd02-de672ba8e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde39a6-53c0-47ca-a0db-f191c5c79304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e1ec2-a8bb-49fc-a701-5c0729d6e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1d738-6512-4e4b-be1d-1352cab390c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aecd1b-6d4c-4d88-b878-acd08f7ed09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bef956-fa2e-4a41-9f2d-aedea21bfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_good1,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858524bb-b942-4b28-9c4b-9ae4c4cb5c0a",
   "metadata": {},
   "source": [
    "### Good2 - topsport_green - Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72ed76-d95e-429e-9d4d-d54c4610df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9f4d7-7199-4908-adb0-b299d781f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecf4db-8694-45a7-8b44-89a5d7fc87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741c53f-60ba-4393-9f03-05694844680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f945dea-41e0-49a0-b43e-21579b1a052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9432c8d-3969-427b-b950-47a4ccf9a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f11de-e255-4019-a008-86396046e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e801e48-c128-4a17-bc79-3c0c3f67bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b603aa-fed3-4b6a-8f98-09e92d45ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_good2,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a71244-4589-44fb-aae7-2048dc317288",
   "metadata": {},
   "source": [
    "### Good3 - cc_red - Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32360b-37d7-4f29-9131-97f8f0451067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604d886-1d40-4f18-897e-b5a1e5f48854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21180af1-efcb-449e-822a-b7b654025e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b955d5-1361-4c9f-a641-c305016e43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502301fc-f514-45e3-aa67-2a4837752cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572676c0-62b5-4c8b-8966-ea56351af790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7f9e1-145e-4c11-b408-28868d7718a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1de01-472d-4de1-a428-5db742f326bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a5378-a511-47c5-9ccc-7df2c36af902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_good3,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d6b25-058e-46a3-97c8-5ba516b62471",
   "metadata": {},
   "source": [
    "### Good4 - fanta_blue - Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d16e08-ef7d-414f-b1fd-b531cbdb74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell\n",
    "show_gradcam_from_path(\n",
    "    model_basis,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_basis,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13725e22-3197-429c-96d9-1e7d29cf9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - mit verzerrten Bildern\n",
    "show_gradcam_from_path(\n",
    "    model_basis_distort,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_basis_distort,\n",
    "    transform_step1=transform_step1_distort,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f8e1f-d596-45db-a790-5bfe8a6cfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenes CNN - Basismodell - tiefes Modell\n",
    "show_gradcam_from_path(\n",
    "    model_deep,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_model_deep,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cfe09-8d24-4c3c-a307-acadfa7b23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - einfacher Kopf\n",
    "show_gradcam_from_path(\n",
    "    model_deep_gap,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_deep_gap,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ba08e-7da0-42be-a59f-522da00d9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - mit Dropout\n",
    "show_gradcam_from_path(\n",
    "    model_deep_dropout,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_deep_dropout,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee23d90-6a2f-4d90-a1de-50d3b4f42ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - ohne Batchnorm\n",
    "show_gradcam_from_path(\n",
    "    model_deep_no_bn,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_model_deep_no_bn,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e4266-c357-40a5-8c8e-9f82a3463de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiefes Modell - Data Augmentation\n",
    "show_gradcam_from_path(\n",
    "    model_deep_augV1,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_model_deep_augV1,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfeacc-b0b4-41f2-9fe6-d1302029ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - cc_red\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_ccred,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_model_deep_lovo_ccred,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7671e-ab39-4ac6-8c12-4b7a0f0649e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lovo - Voessl\n",
    "show_gradcam_from_path(\n",
    "    model_deep_lovo_voesl,\n",
    "    path_gradcam_good4,\n",
    "    target_layer=target_layer_model_deep_lovo_voesl,\n",
    "    transform_step1=transform_step1,\n",
    "    transform_test_step2=transform_test_step2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d674c8-2490-40eb-83c8-fd8971292a59",
   "metadata": {},
   "source": [
    "## 4.7 Inferenzzeit und Praxistauglichkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974521c4-7734-44f6-882d-156b3e915e33",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die Inferenzzeit der trainierten Modelle untersucht, um deren Praxistauglichkeit f√ºr einen sp√§teren Inline-Einsatz zu bewerten. Das Zielsystem ist auf eine Verarbeitung von 72 000 Beh√§ltern pro Stunde ausgelegt. Da pro Beh√§lter vier Bildaufnahmen erfasst und klassifiziert werden, ergibt sich eine Anforderung von 288 000 Bildern pro Stunde, entsprechend 80 Bildern pro Sekunde.\n",
    "\n",
    "Im Fokus steht daher die reine Vorw√§rtslaufzeit der Modelle unter realistischen Randbedingungen, insbesondere im Hinblick auf parallele Bildverarbeitung. Ziel ist es zu beurteilen, ob die Modelle diese zeitlichen Anforderungen erf√ºllen und welche Architekturvarianten grunds√§tzlich f√ºr den produktiven Einsatz geeignet sind.  \n",
    "\n",
    "Zur Messung der Inferenzzeit werden zwei Hilfsfunktionen implementiert. Die erste Funktion dient der robusten und reproduzierbaren Bestimmung der reinen Vorw√§rtslaufzeit eines bereits trainierten Modells auf Basis eines DataLoaders. Dabei werden Warm-up-Durchl√§ufe ber√ºcksichtigt, um Initialisierungseffekte der Hardware zu minimieren.\n",
    "\n",
    "Erg√§nzend erm√∂glicht eine zweite Funktion die gezielte Anpassung der Batchgr√∂√üe, um praxisnahe Szenarien, insbesondere die parallele Verarbeitung von vier Bildern pro Beh√§lter, abzubilden. Auf dieser Basis kann die effektive Inferenzgeschwindigkeit ausgew√§hlter Modellvarianten systematisch ermittelt und mit den definierten Echtzeitanforderungen verglichen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980baac9-efe9-49a6-a4c3-ab5005af5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device=None,\n",
    "    num_warmup=20,\n",
    "    num_batches=100,\n",
    "    use_amp=False,\n",
    "):\n",
    "    # Inferenz-Zeitmessung f√ºr ein bereits trainiertes Modell\n",
    "    # misst nur den Forward-Pass (kein Backprop, kein Optimizer)\n",
    "    # Ergebnis: mittlere Batch-Zeit + FPS (Bilder pro Sekunde)\n",
    "\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    # Modell auf Device schieben und in Eval-Mode setzen\n",
    "    # Eval ist wichtig: Dropout aus, BatchNorm nutzt Running Stats\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # hier sammeln wir die gemessenen Zeiten pro Batch\n",
    "    times = []\n",
    "    total_images = 0  # f√ºr FPS √ºber alle gemessenen Bilder\n",
    "\n",
    "    # AMP macht nur auf CUDA wirklich Sinn\n",
    "    amp_enabled = bool(use_amp and str(device).startswith(\"cuda\"))\n",
    "\n",
    "    # je nach Dataset liefert der DataLoader z.B.\n",
    "    # (x, y) oder (x, y, path) -> wir brauchen nur x\n",
    "    def get_x(batch):\n",
    "        # batch kann tuple/list sein -> erstes Element ist immer x\n",
    "        if isinstance(batch, (tuple, list)):\n",
    "            return batch[0]\n",
    "        return batch\n",
    "\n",
    "    # Warmup:\n",
    "    # Ziel: GPU/Kernel-Cache \"anwerfen\", ggf. cuDNN-Autotuning stabilisieren,\n",
    "    # damit die eigentliche Messung nicht durch die ersten Batches verf√§lscht wird\n",
    "    it = iter(dataloader)\n",
    "    with torch.inference_mode():  # schneller als no_grad, keine Gradienten-Overheads\n",
    "        for _ in range(num_warmup):\n",
    "            # wenn der Loader zu Ende ist, einfach wieder von vorne anfangen\n",
    "            try:\n",
    "                batch = next(it)\n",
    "            except StopIteration:\n",
    "                it = iter(dataloader)\n",
    "                batch = next(it)\n",
    "\n",
    "            # Input auf Device (non_blocking kann helfen, wenn pin_memory=True ist)\n",
    "            x = get_x(batch).to(device, non_blocking=True)\n",
    "\n",
    "            # CUDA ist asynchron -> ohne synchronize misst perf_counter nur das \"losschicken\" der Arbeit\n",
    "            if str(device).startswith(\"cuda\"):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            # Forward-Pass\n",
    "            if amp_enabled:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    _ = model(x)\n",
    "            else:\n",
    "                _ = model(x)\n",
    "\n",
    "            # nochmal synchronisieren, damit wirklich fertig gerechnet wurde\n",
    "            if str(device).startswith(\"cuda\"):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "    # Messung:\n",
    "    # wir messen pro Batch die reine Forward-Zeit (ohne Backprop, ohne Optimizer)\n",
    "    it = iter(dataloader)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(num_batches):\n",
    "            # wieder robust gegen StopIteration\n",
    "            try:\n",
    "                batch = next(it)\n",
    "            except StopIteration:\n",
    "                it = iter(dataloader)\n",
    "                batch = next(it)\n",
    "\n",
    "            # Input holen + auf Device schieben\n",
    "            x = get_x(batch).to(device, non_blocking=True)\n",
    "            bs = x.size(0)  # Batchsize dieses konkreten Batches (kann beim letzten Batch kleiner sein)\n",
    "\n",
    "            if str(device).startswith(\"cuda\"):\n",
    "                torch.cuda.synchronize()\n",
    "            t0 = time.perf_counter()  # Startzeit\n",
    "\n",
    "            # Forward (optional mit AMP)\n",
    "            if amp_enabled:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    _ = model(x)\n",
    "            else:\n",
    "                _ = model(x)\n",
    "\n",
    "            if str(device).startswith(\"cuda\"):\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()  # Endzeit\n",
    "\n",
    "            # Zeit speichern + Bildanzahl hochz√§hlen\n",
    "            dt = t1 - t0\n",
    "            times.append(dt)\n",
    "            total_images += bs\n",
    "\n",
    "    # Zeiten als numpy array f√ºr Statistik\n",
    "    times = np.array(times, dtype=np.float64)\n",
    "\n",
    "    # mittlere Batch-Zeit (Sekunden)\n",
    "    mean_batch_time = times.mean()\n",
    "\n",
    "    # 95%-Perzentil Batch-Zeit (zeigt \"schlechte\" / langsamere Batches)\n",
    "    p95_batch_time = np.percentile(times, 95)\n",
    "\n",
    "    # FPS lieber aus allen gemessenen Bildern berechnen:\n",
    "    # robust auch wenn der letzte Batch kleiner ist oder Batchsize schwankt\n",
    "    mean_fps = total_images / times.sum()\n",
    "\n",
    "    # optional: k√∂nnte man aus p95_batch_time absch√§tzen,\n",
    "    # aber wenn Batchsize variiert, wird das schnell unsauber\n",
    "    p95_fps = None\n",
    "\n",
    "    # alles als Dictionary zur√ºckgeben, damit du es easy printen/loggen kannst\n",
    "    return {\n",
    "        \"device\": str(device),\n",
    "        \"batch_size\": dataloader.batch_size,\n",
    "        \"num_batches_measured\": int(len(times)),\n",
    "        \"mean_batch_time_s\": float(mean_batch_time),\n",
    "        \"p95_batch_time_s\": float(p95_batch_time),\n",
    "        \"mean_fps\": float(mean_fps),\n",
    "        \"p95_fps\": p95_fps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003181f-4a77-4c3c-ba8d-e917715f5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_bs4 = DataLoader(\n",
    "    test_loader_std.dataset,   # wir verwenden exakt den gleichen Test-Datensatz wie beim normalen Testlauf, nur mit anderer Batchgr√∂√üe\n",
    "    batch_size=4,              # immer 4 Bilder gleichzeitig\n",
    "    shuffle=False,             \n",
    "    num_workers=0,             \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa0e07-226c-4575-b5d6-4b621cb701c7",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN (model_basis)</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a229b50-3b97-4932-9508-ec31c73842a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_basis,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb7a4e-a24b-44a0-817e-6992a1e468d9",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN mit verzerrten Bildern(model_basis_distort)</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78835559-abb9-4f36-aece-e0474cb9c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_basis_distort,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398025c-598b-4e37-92b9-ea2829129c62",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - Deep</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb596f-610e-4282-b7f5-d2a2b7820939",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51e1b3-d78e-43c5-b144-a70531b46bfa",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - Deep mit einfachem Kopf</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755694b-bf9b-4ee3-9b1c-812061fe07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep_gap,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c235118-92ab-43d9-9c90-b7fe79c5ac70",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - Deep mit Dropout</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074cae2-425c-4cd6-b209-e4c08dd8e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep_dropout,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22830689-57ce-4673-948b-b7a2bac32e40",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - ohne Batchnorm</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a82b0-da19-4108-b423-3cc99a070de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep_no_bn,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe11cfd-e095-42b6-819d-cb7d6d2ea844",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - mit Data Augmentation</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b2a4a-e815-4bfa-b2f1-1a8116249761",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep_augV1,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d82008-da31-4442-ab17-8f559f81afa7",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - LOVO - CC_red</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6169e-86e8-4769-8ec6-e8a7814053de",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep_lovo_ccred,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09212d-ed19-4e23-8ef5-5a4e82584329",
   "metadata": {},
   "source": [
    "<u><strong>Inferenzzeit f√ºr das Basis-CNN - Lovo- voesl_zitrone</strong></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4e67e-715d-49bf-ae89-b1f2d19973ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_bs4 = measure_inference(\n",
    "    model_deep_lovo_voesl,\n",
    "    test_loader_bs4,\n",
    "    device=device,\n",
    "    num_warmup=50,\n",
    "    num_batches=200,\n",
    ")\n",
    "\n",
    "print(speed_bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6044c0f-6b5a-4bb3-9e15-1b04ef922588",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 5. Un√ºberwachter Ansatz mittels Encoder-Netzwerken\n",
    "\n",
    "In den vorangegangenen Abschnitten wurde die Inspektion von Tethered Caps als √ºberwachtes Klassifikationsproblem betrachtet. In der industriellen Praxis ist jedoch die Verf√ºgbarkeit repr√§sentativer Fehlerdaten oft eingeschr√§nkt: Defekte treten selten auf, variieren stark und k√∂nnen sich im Zeitverlauf √§ndern. Vor diesem Hintergrund wird in diesem Kapitel ein un√ºberwachter Ansatz vorgestellt, bei dem ein Encoder-Netzwerk (Autoencoder) den Normalzustand anhand fehlerfreier (Good-)Bilder lernt. Abweichungen von diesem Normalmodell werden anschlie√üend √ºber einen Rekonstruktionsfehler als Anomalien erkannt.\n",
    "\n",
    "Der Fokus liegt dabei explizit auf der Qualit√§tsanforderung, dass Good-Bilder nicht f√§lschlich als Bad bewertet werden d√ºrfen. Entsprechend wird die Entscheidungsgrenze (Threshold) so kalibriert, dass die False-Positive-Rate (FPR) < 0,1 % bleibt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca8631-f8bf-4094-9c92-63149b629d9e",
   "metadata": {},
   "source": [
    "## 5.1 Setup & Datencheck\n",
    "\n",
    "Aufbauend auf den in Kapitel 3 beschriebenen Verfahren zum Datenimport und zur Datenvorbereitung werden in diesem Abschnitt die f√ºr den un√ºberwachten Ansatz relevanten Rahmenbedingungen definiert. Neben dem technischen Setup zur Sicherstellung reproduzierbarer Experimente liegt der Fokus insbesondere auf der √úberpr√ºfung der Datenqualit√§t, da fehlerhafte oder inkonsistente Good-Daten unmittelbar zu einer erh√∂hten False-Positive-Rate f√ºhren k√∂nnen.\n",
    "\n",
    "Zun√§chst werden die verwendeten Bibliotheken eingebunden und Zufalls-Seeds festgelegt, um eine konsistente Initialisierung des Trainingsprozesses zu gew√§hrleisten. Anschlie√üend werden die Bilddaten geladen und hinsichtlich ihrer Verteilung (Good/Bad) sowie grundlegender Eigenschaften wie Aufl√∂sung, Farbraum und Dateiformat analysiert.\n",
    "\n",
    "Dar√ºber hinaus erfolgt eine visuelle Stichprobenpr√ºfung der Good- und Bad-Bilder, um offensichtliche Fehlannotationen, Ausrei√üer oder systematische Unterschiede zu identifizieren. Ein besonderer Fokus liegt auf der Erkennung von Duplikaten oder nahezu identischen Bildern, da diese zu Data Leakage zwischen Trainings- und Evaluationsdaten f√ºhren und die Aussagekraft der Ergebnisse verf√§lschen w√ºrden.\n",
    "\n",
    "Das Ergebnis dieses Abschnitts ist eine validierte und dokumentierte Datenbasis, die als Ausgangspunkt f√ºr den un√ºberwachten Trainingsprozess des Autoencoders dient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd3ae5-4e33-4904-bf42-eb2c8bb2a183",
   "metadata": {},
   "source": [
    "### 5.1.1 Setup & Reproduzierbarkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f60fd3-334c-47ba-886a-bec96ac6c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71233d-4be2-4c31-8fab-49bb76c28c92",
   "metadata": {},
   "source": [
    "### 5.1.2 Daten laden (aus Kapitel 3 √ºbernommen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283881e-9ba8-47dc-afca-b906fd36163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.expanduser(\"~/TetheredCap/Tamper_Evident_Seal_Inspection_Ring_Only_Lab_Pictures\")\n",
    "\n",
    "# Unterst√ºtzte Bild-Endungen (bei Bedarf erweitern)\n",
    "EXTS = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\")\n",
    "\n",
    "def collect_images(base_dir: str, class_name: str):\n",
    "    \"\"\"\n",
    "    Sammelt rekursiv Bilder unter base_dir/*/<class_name>/*.<ext>\n",
    "    Gibt: list(paths), list(groups)\n",
    "    group = der Unterordner direkt unter base_dir (z.B. Messreihe/Charge)\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for ext in EXTS:\n",
    "        paths.extend(glob(os.path.join(base_dir, \"*\", class_name, ext)))\n",
    "    paths = sorted(paths)\n",
    "\n",
    "    # group extrahieren: .../Lab_Pictures/<group>/<class>/image.jpg\n",
    "    groups = [p.split(os.sep)[-3] for p in paths]  # -3 = <group>\n",
    "    return paths, groups\n",
    "\n",
    "good_images, good_groups = collect_images(BASE_DIR, \"good\")\n",
    "bad_images, bad_groups   = collect_images(BASE_DIR, \"bad\")  # falls vorhanden\n",
    "\n",
    "print(f\"GOOD images: {len(good_images)}\")\n",
    "print(f\"BAD  images: {len(bad_images)}\")\n",
    "\n",
    "# √úberblick Gruppen (wichtig f√ºr Leakage/Group-Split sp√§ter)\n",
    "print(\"\\nGOOD Gruppen (Top 10):\")\n",
    "for g, c in Counter(good_groups).most_common(10):\n",
    "    print(f\"  {g}: {c}\")\n",
    "\n",
    "if len(bad_images) > 0:\n",
    "    print(\"\\nBAD Gruppen (Top 10):\")\n",
    "    for g, c in Counter(bad_groups).most_common(10):\n",
    "        print(f\"  {g}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f82a43-5f58-4668-a105-6b474482ae74",
   "metadata": {},
   "source": [
    "### 5.1.3 Visueller Datencheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09dc9a-f25f-4934-ae24-33340fd4bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rgb(path: str):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Kann Bild nicht lesen: {path}\")\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def show_samples(image_paths, title, n=8, seed=SEED):\n",
    "    if len(image_paths) == 0:\n",
    "        print(f\"[WARN] Keine Bilder f√ºr: {title}\")\n",
    "        return\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(n, len(image_paths))\n",
    "    picks = rng.choice(image_paths, size=n, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(2.2*n, 2.6))\n",
    "    for i, p in enumerate(picks, start=1):\n",
    "        img = read_rgb(p)\n",
    "        plt.subplot(1, n, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(os.path.basename(p), fontsize=8)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "show_samples(good_images, \"GOOD ‚Äì Stichproben\", n=8)\n",
    "show_samples(bad_images,  \"BAD ‚Äì Stichproben\",  n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c380ed-6acf-4ccf-a408-c5cf871a455e",
   "metadata": {},
   "source": [
    "### 5.1.4 Basis-Checks: Aufl√∂sung, Kan√§le, defekte Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015d7a7-4fc5-468f-befa-be0f48831bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_image_stats(image_paths, max_check=5000):\n",
    "    \"\"\"\n",
    "    Pr√ºft bis zu max_check Bilder: (H,W), Kan√§le, Lesbarkeit.\n",
    "    \"\"\"\n",
    "    if len(image_paths) == 0:\n",
    "        return {}\n",
    "\n",
    "    check_paths = image_paths[:min(max_check, len(image_paths))]\n",
    "    shapes = []\n",
    "    unreadable = []\n",
    "\n",
    "    for p in check_paths:\n",
    "        img = cv2.imread(p, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            unreadable.append(p)\n",
    "            continue\n",
    "        # img shape: (H,W) oder (H,W,C)\n",
    "        if img.ndim == 2:\n",
    "            h, w = img.shape\n",
    "            c = 1\n",
    "        else:\n",
    "            h, w, c = img.shape\n",
    "        shapes.append((h, w, c))\n",
    "\n",
    "    shapes = np.array(shapes) if len(shapes) else np.empty((0,3), dtype=int)\n",
    "    out = {\n",
    "        \"checked\": len(check_paths),\n",
    "        \"ok\": len(shapes),\n",
    "        \"unreadable\": unreadable,\n",
    "    }\n",
    "    if len(shapes):\n",
    "        out.update({\n",
    "            \"h_min\": int(shapes[:,0].min()),\n",
    "            \"h_max\": int(shapes[:,0].max()),\n",
    "            \"w_min\": int(shapes[:,1].min()),\n",
    "            \"w_max\": int(shapes[:,1].max()),\n",
    "            \"channels\": dict(Counter(shapes[:,2].tolist()))\n",
    "        })\n",
    "    return out\n",
    "    \n",
    "\n",
    "good_stats = quick_image_stats(good_images, max_check=3000)\n",
    "print(\"\\nGOOD stats:\", good_stats)\n",
    "\n",
    "if len(bad_images) > 0:\n",
    "    bad_stats = quick_image_stats(bad_images, max_check=3000)\n",
    "    print(\"\\nBAD stats:\", bad_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba25508-1949-4196-a873-383ab8cee177",
   "metadata": {},
   "source": [
    "### 5.1.5 Duplikate / Near-Duplikate (empfohlen)\n",
    "* \"Exact duplicates\": gleicher Dateihash (schnell)\n",
    "* \"Near duplicates\": perceptual hash (pHash, etwas langsamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ba001-04df-4cdd-a3db-fc3eb422baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def file_md5(path, chunk_size=1024*1024):\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def find_exact_duplicates(paths, max_files=None):\n",
    "    \"\"\"\n",
    "    Findet exakte Duplikate √ºber MD5-Hash.\n",
    "    R√ºckgabe: dict(hash -> [paths]) nur mit Duplikaten (len>1)\n",
    "    \"\"\"\n",
    "    if max_files is not None:\n",
    "        paths = paths[:max_files]\n",
    "    buckets = defaultdict(list)\n",
    "    for p in tqdm(paths, desc=\"MD5 hashing\"):\n",
    "        try:\n",
    "            buckets[file_md5(p)].append(p)\n",
    "        except Exception:\n",
    "            buckets[\"__ERROR__\"].append(p)\n",
    "    dups = {k:v for k,v in buckets.items() if len(v) > 1 and k != \"__ERROR__\"}\n",
    "    return dups\n",
    "\n",
    "# OPTIONAL: tqdm import (falls nicht vorhanden oben)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Exakte Duplikate (kann bei sehr vielen Bildern dauern; ggf. max_files setzen)\n",
    "exact_dups_good = find_exact_duplicates(good_images, max_files=None)\n",
    "print(f\"\\nExakte Duplikat-Gruppen (GOOD): {len(exact_dups_good)}\")\n",
    "if len(exact_dups_good) > 0:\n",
    "    # zeige 1-2 Gruppen\n",
    "    for i, (h, ps) in enumerate(exact_dups_good.items()):\n",
    "        print(f\"  Gruppe {i+1} ({len(ps)} Dateien):\")\n",
    "        for p in ps[:5]:\n",
    "            print(\"   \", p)\n",
    "        if i >= 1:\n",
    "            break\n",
    "\n",
    "\n",
    "# Near-Duplikate via perceptual hash (pHash)\n",
    "def phash(image_rgb, hash_size=8):\n",
    "    \"\"\"\n",
    "    Einfacher pHash:\n",
    "    1) Graustufen\n",
    "    2) Resize (32x32)\n",
    "    3) DCT\n",
    "    4) Median threshold\n",
    "    R√ºckgabe: 64-bit als np.uint64\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(gray, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "    dct = cv2.dct(np.float32(img))\n",
    "    dct_low = dct[:hash_size, :hash_size]\n",
    "    med = np.median(dct_low[1:, 1:])  # ohne DC-Anteil\n",
    "    bits = (dct_low > med).astype(np.uint8).flatten()\n",
    "    # pack bits -> uint64\n",
    "    out = np.uint64(0)\n",
    "    for b in bits[:64]:\n",
    "        out = (out << np.uint64(1)) | np.uint64(int(b))\n",
    "    return out\n",
    "\n",
    "def hamming_u64(a, b):\n",
    "    return int((a ^ b).bit_count())\n",
    "\n",
    "def find_near_duplicates(paths, max_files=1000, dist_threshold=6):\n",
    "    \"\"\"\n",
    "    Heuristik: berechnet pHash f√ºr max_files und sucht Paare mit Hamming-Distanz <= dist_threshold.\n",
    "    Bei sehr vielen Bildern: max_files klein starten (z.B. 2000) oder gruppenweise pr√ºfen.\n",
    "    \"\"\"\n",
    "    paths = paths[:min(max_files, len(paths))]\n",
    "    hashes = []\n",
    "    ok_paths = []\n",
    "    for p in tqdm(paths, desc=\"pHash\"):\n",
    "        try:\n",
    "            img = read_rgb(p)\n",
    "            hashes.append(phash(img))\n",
    "            ok_paths.append(p)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    pairs = []\n",
    "    for i in tqdm(range(len(hashes)), desc=\"Near-dup scan\"):\n",
    "        for j in range(i+1, len(hashes)):\n",
    "            if hamming_u64(hashes[i], hashes[j]) <= dist_threshold:\n",
    "                pairs.append((ok_paths[i], ok_paths[j]))\n",
    "    return pairs\n",
    "\n",
    "# OPTIONAL (kann quadratisch teuer sein): near duplicates nur als Stichprobe\n",
    "near_dups_good = find_near_duplicates(good_images, max_files=800, dist_threshold=6)\n",
    "print(f\"\\nNear-Duplikat-Paare (GOOD, Stichprobe): {len(near_dups_good)}\")\n",
    "if len(near_dups_good) > 0:\n",
    "    print(\"Beispielpaar:\")\n",
    "    print(\" \", near_dups_good[0][0])\n",
    "    print(\" \", near_dups_good[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d07c86-21ea-4d28-87ff-d8be6bf6edb2",
   "metadata": {},
   "source": [
    "## 5.2 Split & Preprocessing\n",
    "\n",
    "Der un√ºberwachte Anomalieansatz folgt einer One-Class-Logik:\n",
    "\n",
    "Training: ausschlie√ülich Good (Normalzustand)\n",
    "\n",
    "Validierung/Test: Good + Bad (Bewertung der Trennf√§higkeit)\n",
    "\n",
    "Damit das Modell sp√§ter robust arbeitet, wird ein konsistentes Preprocessing festgelegt (Resize, Normalisierung, ggf. Farbraum). Optional kann eine ROI-Strategie (Region of Interest) eingesetzt werden, um Hintergrundartefakte zu reduzieren und die Bewertung st√§rker auf die Verschlusskappe selbst zu fokussieren. Zudem ist sicherzustellen, dass Splits nicht zuf√§llig Bilder aus derselben Serie/Charge auf Train und Test verteilen (Leakage), sofern solche Abh√§ngigkeiten existieren.\n",
    "\n",
    "Ergebnis dieses Abschnitts: definierte Splits (Train/Val/Test) und ein reproduzierbarer Preprocessing-Pipeline-Stand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca078c-9bbf-4cd3-81a3-2ecc1912dafe",
   "metadata": {},
   "source": [
    "## 5.3 Modell & Training\n",
    "\n",
    "Als Modell wird ein Convolutional Autoencoder verwendet, bestehend aus einem Encoder zur Merkmalsextraktion und einem Decoder zur Rekonstruktion des Eingabebildes. Das Training erfolgt ausschlie√ülich auf Good-Daten und optimiert die Rekonstruktionsqualit√§t (z. B. L1/MSE, optional erg√§nzt durch strukturelle √Ñhnlichkeitsma√üe).\n",
    "\n",
    "W√§hrend des Trainings ist neben der Loss-Entwicklung insbesondere eine qualitative Kontrolle zentral: F√ºr ausgew√§hlte Beispiele werden Original und Rekonstruktion gegen√ºbergestellt, um zu pr√ºfen, ob das Modell den Normalzustand stabil rekonstruiert und nicht bereits im Training unerw√ºnschte Artefakte einf√ºhrt.\n",
    "\n",
    "Ergebnis dieses Abschnitts: trainiertes Autoencoder-Modell, das den Normalzustand der Good-Verschlusskappen hinreichend rekonstruiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172056c6-0e84-405c-ab2a-41a5b9a53e91",
   "metadata": {},
   "source": [
    "## 5.4 Scoring & Threshold\n",
    "\n",
    "Die Entscheidung ‚ÄûGood vs. Bad‚Äú erfolgt nicht direkt als Klassifikation, sondern √ºber einen Anomalie-Score, typischerweise der Rekonstruktionsfehler pro Bild. Intuitiv gilt: Je st√§rker ein Bild vom gelernten Normalzustand abweicht, desto schlechter kann es rekonstruiert werden und desto h√∂her f√§llt der Score aus.\n",
    "\n",
    "Die kritische Komponente ist die Threshold-Kalibrierung. Da Good-Bilder nicht als Bad eingestuft werden sollen, wird der Threshold auf Good-Validierungsdaten so gew√§hlt, dass h√∂chstens 0,1 % der Good-Bilder oberhalb liegen (z. B. √ºber das 99,9%-Quantil der Good-Score-Verteilung). Dadurch wird die FPR formal kontrolliert; Bad-Erkennung (Recall) wird im Anschluss als Trade-off bewertet.\n",
    "\n",
    "Ergebnis dieses Abschnitts: fest definierter Threshold zur Einhaltung der Ziel-FPR (< 0,1%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7543c05-19bf-4dc7-894b-257f06d24a8e",
   "metadata": {},
   "source": [
    "## 5.5 Evaluation & Inference\n",
    "\n",
    "Die Performance wird zun√§chst auf dem Validierungsset und abschlie√üend auf dem Testset bewertet. Neben klassischen Kennzahlen (Confusion Matrix, Precision/Recall) ist hier vor allem die False-Positive-Rate (Good ‚Üí Bad) die entscheidende Zielgr√∂√üe. Erg√§nzend wird die False-Negative-Rate (Bad ‚Üí Good) betrachtet, um den Preis der konservativen Threshold-Wahl zu quantifizieren.\n",
    "\n",
    "F√ºr die Anwendung wird eine Inference-Pipeline definiert: Eingabebild ‚Üí Preprocessing ‚Üí Rekonstruktion ‚Üí Score ‚Üí Vergleich mit Threshold ‚Üí Ausgabe (Good/Bad + Score). Optional kann ein Fehlerbild/Heatmap bereitgestellt werden, um Abweichungsregionen zu visualisieren und die Nachvollziehbarkeit f√ºr Prozessbeteiligte zu erh√∂hen.\n",
    "\n",
    "Ergebnis dieses Abschnitts: belastbare Bewertung der Zielerreichung und eine einsatzf√§hige Vorhersagefunktion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4fd5f-b674-40ab-9e5e-2144f25c690b",
   "metadata": {},
   "source": [
    "## 5.6 Fazit & Ausblick\n",
    "\n",
    "Abschlie√üend wird bewertet, ob die Zielanforderung FPR < 0,1 % erreicht wurde und unter welchen Randbedingungen (Datenumfang, Varianz in Beleuchtung/Positionierung, ROI-Qualit√§t) diese Aussage g√ºltig ist. Besonders wichtig ist eine strukturierte Analyse von False Positives, da diese unmittelbar der zentralen Anforderung widersprechen und h√§ufig auf systematische Ursachen wie Reflexionen, Hintergrundwechsel oder Grenzf√§lle in der Good-Klasse zur√ºckzuf√ºhren sind.\n",
    "\n",
    "Als Ausblick werden konkrete Verbesserungspfade aufgezeigt, etwa eine robustere ROI-Extraktion, stabilere Aufnahmebedingungen sowie der Vergleich mit leistungsf√§higeren AD-Verfahren (z. B. PatchCore/PaDiM) und ein Drift-Monitoring √ºber Score-Verteilungen im Produktionsbetrieb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ce865-28fc-4b6f-9512-acbeaf0ebb45",
   "metadata": {},
   "source": [
    "# Auf neue Flaschen Aufzulegen. Der Code soll dann auf neue Flaschen anwendbar sein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d0da2-3bdd-4591-8964-8c0d1b4a38c5",
   "metadata": {},
   "source": [
    "# 6. Ergebnisse und Bewertung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548a3a4-1c64-42e1-9e9b-f0a1f47ee7b0",
   "metadata": {},
   "source": [
    "# 7. Zusammenfassung und Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cf457-3bfa-409b-a641-456c0a651a57",
   "metadata": {},
   "source": [
    "* Ausblick: Training auf neue Flaschen / Verschl√ºsse\n",
    "* Die Analyse der Fehlklassifikationen zeigt, dass falsche Vorhersagen h√§ufig dann auftreten, wenn der Bildausschnitt nicht den eigentlichen Verschluss inklusive Sicherungsring erfasst, sondern stattdessen √ºberwiegend den Flaschenhals oder andere irrelevante Bildbereiche zeigt. In solchen F√§llen fehlen dem Modell die f√ºr die Klassifikation entscheidenden visuellen Informationen, was zu unsicheren oder fehlerhaften Entscheidungen f√ºhren kann. Dieses Problem verdeutlicht zugleich den hohen Aufwand bei der Erstellung qualitativ hochwertiger Trainingsdaten im industriellen Kontext, da die manuelle Erzeugung und √úberpr√ºfung geeigneter Labels zeit- und kostenintensiv ist. Als m√∂gliche Verbesserung bietet sich daher ein vorgelagerter Pr√ºf- oder Filtermechanismus an, der ungeeignete Bildausschnitte automatisch erkennt und von der weiteren Verarbeitung ausschlie√üt. Bilder, die den relevanten Inspektionsbereich nicht ausreichend abbilden, w√ºrden in diesem Fall bereits vor der eigentlichen Klassifikation verworfen und nicht in das Trainings- oder Testverfahren einbezogen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd37ca5-7ab0-4a94-9896-5a7ba79e93ee",
   "metadata": {},
   "source": [
    "# 8. Quellenverzeichnis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1880-fffd-479c-b163-335613ef3c16",
   "metadata": {},
   "source": [
    "[1] KHS GmbH, *‚ÄûInnocheck TSI ‚Äì Sicherungsringinspektion‚Äú*, interne Pr√§sentation, Dortmund, 2025.\n",
    "\n",
    "[2] Verbraucherzentrale, *‚ÄûTethered Caps: Warum geht der Verschluss an der Flasche nicht mehr ab?‚Äú*, Online-Artikel, verf√ºgbar unter:  \n",
    "https://www.verbraucherzentrale.de/wissen/umwelt-haushalt/nachhaltigkeit/tethered-caps-warum-geht-der-verschluss-an-der-flasche-nicht-mehr-ab-78264  \n",
    "[Zugriff am 07.12.2025].\n",
    "\n",
    "[3] KHS GmbH, *‚ÄûEU-Richtlinie Tethered Caps: KHS ber√§t ganzheitlich zu Ma√ünahmen und Anpassungen in der Linie‚Äú*, Online-Pressemitteilung, verf√ºgbar unter:  \n",
    "https://www.khs.com/unternehmen/aktuelles/pressemitteilungen/detail/tethered-caps  \n",
    "[Zugriff am 07.12.2025].\n",
    "\n",
    "[4] Bruce, P., Bruce, A., Gedeck, P., *Praktische Statistik f√ºr Data Scientists ‚Äì 50+ essenzielle Konzepte mit R und Python*, 1. Aufl., dpunkt.verlag, Heidelberg, 2021.\n",
    "\n",
    "[5] G√©ron, A., *Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und TensorFlow ‚Äì Konzepte, Tools und Techniken f√ºr intelligente Systeme*, 3. Aufl., O‚ÄôReilly, 2023.  \n",
    "Deutsche √úbersetzung von Kristian Rother und Thomas Demmig.\n",
    "\n",
    "[6] Mitchell, L., Yogesh, S. K., Subramanian, V., *Deep Learning with PyTorch 1.x ‚Äì Implement deep learning techniques and neural network architecture variants using Python*, 2nd ed., Packt Publishing, Birmingham, 2019.  \n",
    "\n",
    "[7] Goetze, S., Machine Learning (AKI), Foliensatz 7 ‚Äì Deep Learning, \n",
    "Vorlesung im Sommersemester 2025, Fachhochschule S√ºdwestfalen, Dortmund, 31.05.2025.  \n",
    "\n",
    "[8] Dragon1, ‚ÄûConvolutional Neural Networks (CNN) ‚Äì Definition and Architecture‚Äú, Online-Grafik, verf√ºgbar unter:\n",
    "https://www.dragon1.com/terms/convolutional-neural-networks-definition\n",
    "[Zugriff am 07.02.2026]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b3b5a-e3e6-4700-a344-29905cf37b5d",
   "metadata": {},
   "source": [
    "# 9. Eigenst√§ndigkeitserkl√§rung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea9f61-b181-453f-b7f6-748cc27e81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Notebook einlesen\n",
    "with open(\"DL_TetheredCaps.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Wortanzahl in Markdown-Zellen berechnen\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').strip().split())\n",
    "\n",
    "print(f\"Wortanzahl in Markdown-Zellen: {word_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
